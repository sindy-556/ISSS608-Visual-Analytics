[
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1.html",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "",
    "text": "This dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n\nIn this exercise we will be:\n\nExamining Heart Attack Occurrence: Analyze the distribution and determinants of heart attack events across the dataset.\nConducting Demographic Analysis: Investigate how age, gender, and region contribute to heart attack risk, distinguishing between younger and older cohorts.\nExploring Health Metrics: Visualize relationships between clinical indicators (e.g., BMI, blood pressure, cholesterol) and heart attack occurrence.\nAssessing Lifestyle Factors: Evaluate the impact of lifestyle variables such as smoking history, physical activity, diet quality, alcohol consumption, and stress levels on heart health."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1.html#overview",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "",
    "text": "This dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n\nIn this exercise we will be:\n\nExamining Heart Attack Occurrence: Analyze the distribution and determinants of heart attack events across the dataset.\nConducting Demographic Analysis: Investigate how age, gender, and region contribute to heart attack risk, distinguishing between younger and older cohorts.\nExploring Health Metrics: Visualize relationships between clinical indicators (e.g., BMI, blood pressure, cholesterol) and heart attack occurrence.\nAssessing Lifestyle Factors: Evaluate the impact of lifestyle variables such as smoking history, physical activity, diet quality, alcohol consumption, and stress levels on heart health."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex1.html#getting-started",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nWe load the following R packages using the pacman::p_load() function:\n\ntidyverse: Core collection of R packages for data wrangling and visualization (e.g., dplyr, ggplot2)\n\nSmartEDA: For the ExpData() function used in exploratory data analysis\n\neasystats: Specifically for check_collinearity() to diagnose multicollinearity issues\n\nreshape2: Provides the melt() function for reshaping data from wide to long format\n\ncaret: Functions for data partitioning (createDataPartition) and model training workflows\n\nyardstick: Offers conf_mat() and other classification metrics\n\npROC: For ROC curves and AUC calculations (roc, auc)\n\nGGally: For the ggpairs() function to create pairwise scatterplot matrices\n\nggmosaic: To create mosaic plots via geom_mosaic()\n\npatchwork: For arranging multiple ggplot figures into a composite layout\n\nxgboost: Gradient boosting library for classification and regression tasks\n\n\npacman::p_load(tidyverse, SmartEDA, easystats, reshape2, caret, yardstick, pROC, GGally, ggmosaic, patchwork, xgboost)\n\nThis dataset contains information about heart attack occurrences in Japan, focusing on various demographic and health-related factors.\n\n\nImport data\n\nheart_data &lt;- read_csv(\"./data/japan_heart_attack_dataset.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#data-pre-processing",
    "href": "Take-home_Ex/Take-home_Ex1.html#data-pre-processing",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Data pre-processing",
    "text": "Data pre-processing\n\nGlimpse of data\nUsing the glimpse() function, we see that the dataset consists of 30,000 rows and 32 columns. The output displays the column names, their data types, and the first few entries for each variable. Additionally, there are 15 extra columns (Extra_Column_1 to Extra_Column_15) which are not clearly defined.\n\nglimpse(heart_data)\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\nThe following provides an overview of the Japan Heart Attack dataset using the ExpData() function, summarizing both overall and variable-level details.\n\nOverall data summaryVariable level summary\n\n\n\nsummary1 &lt;- heart_data %&gt;%\n  ExpData(type = 1)\n\n# Display the summary (further customization possible)\nsummary1\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)     30000\n2                              No. of variables (ncol)        32\n3                    No. of numeric/interger variables        22\n4                              No. of factor variables         0\n5                                No. of text variables        10\n6                             No. of logical variables         0\n7                          No. of identifier variables        20\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         0\n10               %. of variables having complete cases 100% (32)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\n\nsummary2 &lt;- heart_data %&gt;%\n  ExpData(type = 2)\n\n# Display the summary (further customization possible)\nsummary2\n\n   Index           Variable_Name Variable_Type Sample_n Missing_Count\n1      1                     Age       numeric    30000             0\n2      2                  Gender     character    30000             0\n3      3                  Region     character    30000             0\n4      4         Smoking_History     character    30000             0\n5      5        Diabetes_History     character    30000             0\n6      6    Hypertension_History     character    30000             0\n7      7       Cholesterol_Level       numeric    30000             0\n8      8       Physical_Activity     character    30000             0\n9      9            Diet_Quality     character    30000             0\n10    10     Alcohol_Consumption     character    30000             0\n11    11           Stress_Levels       numeric    30000             0\n12    12                     BMI       numeric    30000             0\n13    13              Heart_Rate       numeric    30000             0\n14    14             Systolic_BP       numeric    30000             0\n15    15            Diastolic_BP       numeric    30000             0\n16    16          Family_History     character    30000             0\n17    17 Heart_Attack_Occurrence     character    30000             0\n18    18          Extra_Column_1       numeric    30000             0\n19    19          Extra_Column_2       numeric    30000             0\n20    20          Extra_Column_3       numeric    30000             0\n21    21          Extra_Column_4       numeric    30000             0\n22    22          Extra_Column_5       numeric    30000             0\n23    23          Extra_Column_6       numeric    30000             0\n24    24          Extra_Column_7       numeric    30000             0\n25    25          Extra_Column_8       numeric    30000             0\n26    26          Extra_Column_9       numeric    30000             0\n27    27         Extra_Column_10       numeric    30000             0\n28    28         Extra_Column_11       numeric    30000             0\n29    29         Extra_Column_12       numeric    30000             0\n30    30         Extra_Column_13       numeric    30000             0\n31    31         Extra_Column_14       numeric    30000             0\n32    32         Extra_Column_15       numeric    30000             0\n   Per_of_Missing No_of_distinct_values\n1               0                    62\n2               0                     2\n3               0                     2\n4               0                     2\n5               0                     2\n6               0                     2\n7               0                 30000\n8               0                     3\n9               0                     3\n10              0                     4\n11              0                 29613\n12              0                 30000\n13              0                 30000\n14              0                 30000\n15              0                 30000\n16              0                     2\n17              0                     2\n18              0                 30000\n19              0                 30000\n20              0                 30000\n21              0                 30000\n22              0                 30000\n23              0                 30000\n24              0                 30000\n25              0                 30000\n26              0                 30000\n27              0                 30000\n28              0                 30000\n29              0                 30000\n30              0                 30000\n31              0                 30000\n32              0                 30000\n\n\n\n\n\n\n\nConvert categorical variables to factors\nFrom the overview above, we see that the dataset contains no missing values, and the categorical variables have a maximum of 4 unique values. Converting these variables into factors ensures they are correctly treated as categorical data during analysis and visualization.\n\n# Convert selected categorical variables into factors\nheart_data &lt;- heart_data %&gt;%\n  mutate(\n    Gender = as.factor(Gender),\n    Region = as.factor(Region),\n    Smoking_History = as.factor(Smoking_History),\n    Diabetes_History = as.factor(Diabetes_History),\n    Hypertension_History = as.factor(Hypertension_History),\n    Physical_Activity = as.factor(Physical_Activity),\n    Diet_Quality = as.factor(Diet_Quality),\n    Alcohol_Consumption = as.factor(Alcohol_Consumption),\n    Family_History = as.factor(Family_History),\n    Heart_Attack_Occurrence = as.factor(Heart_Attack_Occurrence)\n  )\n\n\n\nDrop extra columns\n\n# Select only the Extra_Columns and the outcome variable\nextra_data &lt;- heart_data %&gt;%\n  select(starts_with(\"Extra_Column_\"), Heart_Attack_Occurrence)\n\n# Reshape to long format\nextra_data_long &lt;- melt(extra_data, id.vars = \"Heart_Attack_Occurrence\")\n\n# Create boxplots comparing each Extra_Column by Heart_Attack_Occurrence\nggplot(extra_data_long, aes(x = Heart_Attack_Occurrence, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = \"free\") +\n  labs(\n    title = \"Distribution of Extra Columns by Heart Attack Occurrence\",\n    x = \"Heart Attack Occurrence\",\n    y = \"Value\"\n  )\n\n\n\n\n\n\n\n\nSince these variables do not appear to vary by heart attack status, they are unlikely to provide useful information for any downstream analysis (e.g., modeling, hypothesis testing). Dropping them will simplify the dataset and help focus on variables that do relate to heart attack risk.\nWe can drop them with the following code:\n\nheart_data &lt;- heart_data %&gt;%\n  select(-starts_with(\"Extra_Column_\"))\n\n\n\nCleaned dataset\n\nglimpse(heart_data)\n\nRows: 30,000\nColumns: 17\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;fct&gt; Male, Male, Male, Female, Female, Female, Male…\n$ Region                  &lt;fct&gt; Urban, Urban, Rural, Urban, Rural, Rural, Urba…\n$ Smoking_History         &lt;fct&gt; Yes, No, Yes, No, No, No, No, Yes, No, No, No,…\n$ Diabetes_History        &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, No, N…\n$ Hypertension_History    &lt;fct&gt; No, No, No, No, No, No, Yes, No, Yes, No, Yes,…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;fct&gt; Moderate, Low, Low, Moderate, High, Low, High,…\n$ Diet_Quality            &lt;fct&gt; Poor, Good, Average, Good, Good, Good, Poor, P…\n$ Alcohol_Consumption     &lt;fct&gt; Low, Low, Moderate, High, High, High, High, No…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, Yes,…\n$ Heart_Attack_Occurrence &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, No, N…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#exploratory-visuals",
    "href": "Take-home_Ex/Take-home_Ex1.html#exploratory-visuals",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Exploratory visuals",
    "text": "Exploratory visuals\n\nCreate new variables\nWe create a new variable, Age_Group, classifying individuals as “Over50” or “50OrBelow” to compare younger vs. older individuals.\n\nheart_data_eda &lt;- heart_data %&gt;%\n  mutate(Age_Group = ifelse(Age &gt; 50, \"Over50\", \"50OrBelow\") %&gt;% as.factor())\n\nWe create AgeGender by combining the Age_Group and gender. We also combine smoking status and physical activity into SmokeAct and reorder alcohol consumption levels.\n\n# Demographic variables\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    AgeGender = case_when(\n      Age_Group == \"Over50\" & Gender == \"Male\"   ~ \"Over 50 Male\",\n      Age_Group == \"Over50\" & Gender == \"Female\" ~ \"Over 50 Female\",\n      Age_Group == \"50OrBelow\" & Gender == \"Male\"   ~ \"≤50 Male\",\n      Age_Group == \"50OrBelow\" & Gender == \"Female\" ~ \"≤50 Female\"\n    ) %&gt;% factor(levels = c(\"≤50 Female\",\"≤50 Male\",\"Over 50 Female\",\"Over 50 Male\"))\n  )\n\n# Lifestyle variables\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    SmokeAct = case_when(\n      Smoking_History == \"Yes\" & Physical_Activity == \"Low\"      ~ \"Smoker, PA:Low\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"Moderate\" ~ \"Smoker, PA:Mod\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"High\"     ~ \"Smoker, PA:High\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Low\"      ~ \"Non-Smoker, PA:Low\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Moderate\" ~ \"Non-Smoker, PA:Mod\",\n      Smoking_History == \"No\"  & Physical_Activity == \"High\"     ~ \"Non-Smoker, PA:High\"\n    ) %&gt;% \n    # Order them in a sensible sequence:\n    factor(levels = c(\"Non-Smoker, PA:Low\",\"Non-Smoker, PA:Mod\",\"Non-Smoker, PA:High\",\n                      \"Smoker, PA:Low\",\"Smoker, PA:Mod\",\"Smoker, PA:High\"))\n  )\n\n\n\nMosaic Plot: Demographic Analysis\nWe plot a mosaic where AgeGender is on the x-axis, color indicates heart attack occurrence, and each facet represents a different region.\n\np_demo &lt;- ggplot(heart_data_eda) +\n  geom_mosaic(\n    aes(x = product(AgeGender),\n        fill = Heart_Attack_Occurrence,\n        text = paste0(\"Group: \", AgeGender,\n                      \"&lt;br&gt;Region: \", Region,\n                      \"&lt;br&gt;Heart Attack: \", Heart_Attack_Occurrence)\n    ),\n    alpha = 0.9\n  ) +\n  facet_wrap(~ Region) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Demographic Mosaic: Age & Gender by Region vs. Heart Attack\",\n    x     = \"Age & Gender\",\n    y     = \" \",\n    fill  = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\np_demo\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis mosaic plot illustrates heart attack occurrences across different age and gender groups within rural and urban regions. The width of each bar segment corresponds to the relative size of that demographic group, while the height indicates the proportion of individuals who experienced a heart attack.\nOverall, heart attack rates remain relatively consistent between rural and urban areas. However, males tend to have a higher probability of heart attack than females, regardless of age or region.\n\n\n\nMosaic plot: Lifestyle factors\nWe create a mosaic plot with SmokeAct on the x-axis, color by heart attack occurrence, and facet by the four alcohol consumption levels.\n\n# Reorder factor levels for Alcohol_Consumption\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    Alcohol_Consumption = factor(\n      Alcohol_Consumption,\n      levels = c(\"High\", \"Moderate\", \"Low\", \"None\")\n    )\n  )\n\nggplot(heart_data_eda) +\n  geom_mosaic(aes(\n    x    = product(SmokeAct),\n    fill = Heart_Attack_Occurrence\n  ), alpha = 0.9) +\n  facet_wrap(~ Alcohol_Consumption, ncol = 2) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Lifestyle Mosaic: Smoking, Activity, and Alcohol vs. Heart Attack\",\n    subtitle = \"PA = Physical Activity. Each facet represents a different Alcohol Consumption level.\",\n    x = \"Smoking & PA Group\",\n    y = \"\",\n    fill = \"Heart Attack\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 14, hjust=0.5),\n    plot.subtitle = element_text(size = 10, hjust=0.5),\n    strip.text    = element_text(face=\"bold\"),\n    axis.text.x   = element_text(angle=40, hjust=1, size=7),\n    panel.spacing = unit(2, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis mosaic plot explores how smoking, physical activity (PA), and alcohol consumption interact to influence heart attack occurrences. Each facet represents a different alcohol consumption level (High, Moderate, Low, None).\nInterestingly, non-smokers who report no alcohol consumption but high physical activity exhibit one of the highest heart attack rates. Additionally, smokers with moderate physical activity tend to have higher heart attack rates compared to smokers with low or high physical activity.\n\n\n\nPairwise numeric plot (Health metrics)\nThis code uses ggpairs() to create a matrix of pairwise plots for all numeric variables in heart_data. The mapping = aes(color = Heart_Attack_Occurrence) argument adds a color-coded grouping by heart attack status.\n\n# Automatically select all numeric columns from the dataset\nnumeric_cols &lt;- sapply(heart_data, is.numeric)\n\npairwise_plot &lt;- ggpairs(\n  data = heart_data,\n  columns = which(numeric_cols),\n  mapping = aes(color = Heart_Attack_Occurrence),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, size = 0.5)),\n  diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5)),\n  upper = list(continuous = wrap(\"cor\", size = 4))\n) +\n  ggtitle(\"Pairwise Correlations Among All Numeric Metrics\")\n\npairwise_plot\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis grid compares health metrics like BMI, blood pressure, cholesterol, and stress. The diagonal panels show density curves for each variable, revealing, for instance, that Age has a broader distribution compared to the other variables.\nThe upper panels list correlation coefficients and their significance, most of which are near zero (e.g., Corr: 0.025, 0.048), indicating that these variables do not strongly co-vary. In the lower scatter plots, points are colored by heart attack occurrence; no tight clustering suggests no single numeric threshold exclusively separates “Yes” vs. “No.” For instance, Systolic_BP and Diastolic_BP show little correlation as high Systolic_BP often coexists with both high and low Diastolic_BP. Overall, no single numeric factor stands out as a strictly linear driver of heart attack, though there may be subtle nonlinear or interactive effects to explore later."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#train-test-split",
    "href": "Take-home_Ex/Take-home_Ex1.html#train-test-split",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Train test split",
    "text": "Train test split\nBefore building a predictive model, it is best practice to split the data into training and testing sets. The createDataPartition function ensures that the distribution of the target class is approximately the same in both sets. Here, we allocate 80% of the data for training and 20% for testing.\n\nset.seed(123)\n\ntrain_index &lt;- createDataPartition(heart_data$Heart_Attack_Occurrence, p = 0.8, list = FALSE)\n\ntrain_data &lt;- heart_data[train_index, ]\ntest_data  &lt;- heart_data[-train_index, ]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#naive-logistic-regression",
    "href": "Take-home_Ex/Take-home_Ex1.html#naive-logistic-regression",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Naive logistic regression",
    "text": "Naive logistic regression\nHere, we build an initial (“naive”) logistic regression model that includes all available predictors (except the 15 “Extra_Column” variables we dropped). This approach gives us a baseline.\n\nFit the model\nWe will fit a logistic regression using glm().\n\n# Use a standard glm with all predictors\nnaive_glm &lt;- glm(\n  Heart_Attack_Occurrence ~ .,\n  data   = train_data,\n  family = binomial\n)\n\n\n\nUnderstanding the model\nWe use check_collinearity() to see if any variables are highly correlated or cause near‐complete separation. A “good” logistic regression typically avoids extremely high VIFs or indefinite confidence intervals.\n\n# Capture the output\nresult &lt;- check_collinearity(naive_glm)\n\n# Coerce to a data frame\ndf &lt;- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n\n\nCheck for Multicollinearity\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nAge\n1.001325\n1.000000\n1.991615e+01\n1.000662\n0.9986771\n0.0502105\n0.9999999\n\n\nGender\n1.000943\n1.000000\n6.378053e+02\n1.000472\n0.9990575\n0.0015679\n1.0000000\n\n\nRegion\n1.000670\n1.000000\n1.062715e+05\n1.000335\n0.9993302\n0.0000094\n1.0000000\n\n\nSmoking_History\n1.000446\n1.000000\n9.589080e+08\n1.000223\n0.9995547\n0.0000000\n1.0000000\n\n\nDiabetes_History\n1.001228\n1.000000\n3.808479e+01\n1.000614\n0.9987733\n0.0262572\n1.0000000\n\n\nHypertension_History\n1.000834\n1.000000\n3.250209e+03\n1.000417\n0.9991664\n0.0003077\n1.0000000\n\n\nCholesterol_Level\n1.000587\n1.000000\n1.351909e+06\n1.000293\n0.9994134\n0.0000007\n1.0000000\n\n\nPhysical_Activity\n1.001852\n1.000002\n2.745436e+00\n1.000926\n0.9981510\n0.3642408\n0.9999980\n\n\nDiet_Quality\n1.001445\n1.000000\n1.030279e+01\n1.000722\n0.9985567\n0.0970611\n0.9999998\n\n\nAlcohol_Consumption\n1.002116\n1.000005\n1.852209e+00\n1.001057\n0.9978885\n0.5398958\n0.9999947\n\n\nStress_Levels\n1.000919\n1.000000\n8.893873e+02\n1.000459\n0.9990821\n0.0011244\n1.0000000\n\n\nBMI\n1.000849\n1.000000\n2.561245e+03\n1.000424\n0.9991522\n0.0003904\n1.0000000\n\n\nHeart_Rate\n1.000834\n1.000000\n3.252876e+03\n1.000417\n0.9991665\n0.0003074\n1.0000000\n\n\nSystolic_BP\n1.000939\n1.000000\n6.753992e+02\n1.000469\n0.9990618\n0.0014806\n1.0000000\n\n\nDiastolic_BP\n1.000920\n1.000000\n8.726101e+02\n1.000460\n0.9990807\n0.0011460\n1.0000000\n\n\nFamily_History\n1.000826\n1.000000\n3.716482e+03\n1.000413\n0.9991743\n0.0002691\n1.0000000\n\n\n\n\n\n\n\n\nInterpreting the collinearity results\n\nVIF ~1.0 but extremely large upper confidence bounds: This indicates the algorithm is unsure about the exact magnitude of possible collinearity. In simpler terms, the model’s variance–covariance matrix is nearly singular.\nThis often happens when:\n\nQuasi‐complete separation: Certain variables or combinations nearly “perfectly” predict the outcome.\nImbalance in the dataset (many more “No” than “Yes”) plus insufficient signal in some predictors.\nOver‐parametrization: Too many correlated predictors for the sample size.\n\n\n\n\n\nModel performance\n\n# 1) Collinearity plot\ncheck_c &lt;- check_collinearity(naive_glm)\np_collinearity &lt;- plot(check_c) +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\n# 2) Confusion matrix heatmap\npred_prob_naive &lt;- predict(naive_glm, newdata = test_data, type = \"response\")\n\npred_class_naive &lt;- ifelse(pred_prob_naive &gt;= 0.5, \"Yes\", \"No\") %&gt;%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\n\n# Evaluate\nnaive_results &lt;- data.frame(\n  obs   = test_data$Heart_Attack_Occurrence,\n  pred  = pred_class_naive,\n  prob  = pred_prob_naive\n)\n\nnaive_cm &lt;- naive_results %&gt;%\n  conf_mat(obs, pred)\n\np_confmat &lt;- autoplot(naive_cm, type = \"heatmap\") +\n  labs(title = \"Naive Logistic Regression: Confusion Matrix\")\n\n# 3) ROC curve as a ggplot object using ggroc()\nroc_naive &lt;- roc(\n  response  = as.numeric(naive_results$obs),\n  predictor = as.numeric(naive_results$prob)\n)\n\np_roc &lt;- ggroc(roc_naive, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Naïve Logistic Model\") +\n  theme_minimal()\n\n\ncombined_plot &lt;- p_collinearity / (p_confmat + p_roc)\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\nExplanation of the plot\n\nCollinearity Plot:\n\n\n\nThis bar chart displays VIF estimates, where the ideal values fall in the green region.\nIndividual VIF point estimates hover around 1.0, but their upper confidence intervals extend into the red, indicating extremely high values.\nThis suggests that the model’s parameter estimates are unstable due to quasi‐complete separation or an excess of correlated predictors relative to the sample size. In essence, the model cannot reliably discern each variable’s true contribution, leading to artificially low VIF point estimates paired with massive uncertainty bounds.\n\n\nConfusion Matrix:\n\n\n\nThe matrix shows 0 true positives.\nThis is typical when a logistic model either encounters near-complete separation or opts to disregard the minority class in imbalanced datasets.\n\n\nROC Curve:\n\n\n\nThe ROC curve lies near the diagonal reference line, confirming that the model lacks predictive power and is essentially guessing."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#improving-logistic-regression",
    "href": "Take-home_Ex/Take-home_Ex1.html#improving-logistic-regression",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Improving logistic regression",
    "text": "Improving logistic regression\n\nRationale\nOur naïve logistic regression suggested potential issues: - Very low sensitivity (predicting all “No”) - Large variance inflation factor (VIF) intervals\nTherefore, we refine the logistic model by:\n\nUse weighted logistic regression to handle class imbalance,\n\nIncorporate mild non-linear terms for BMI, Systolic_BP, and Diastolic_BP with polynomial expansions,\n\nRemove redundant variables.\n\n\nnew_train &lt;- train_data\nnew_test  &lt;- test_data\n\n\nnew_formula &lt;- as.formula(\n  \"Heart_Attack_Occurrence ~ Age + Gender + Family_History + poly(BMI, 2, raw=TRUE) + \n   Heart_Rate + poly(Systolic_BP, 2, raw=TRUE) + poly(Diastolic_BP, 2, raw=TRUE) + Cholesterol_Level + \n   Diabetes_History + Hypertension_History + Physical_Activity + Smoking_History + \n   Diet_Quality + Alcohol_Consumption + Stress_Levels\"\n)\n\n\nCreating observation weights\nWe create balanced weights to give more importance to the minority class. This ensures misclassifying a minority‐class “Yes” is penalized more strongly than misclassifying a “No.”\n\nn_yes &lt;- sum(new_train$Heart_Attack_Occurrence == \"Yes\")\nn_no  &lt;- sum(new_train$Heart_Attack_Occurrence == \"No\")\nN     &lt;- n_yes + n_no\n\nw_yes &lt;- N / (2 * n_yes)\nw_no  &lt;- N / (2 * n_no)\n\n# Assign weights in the new training dataset\nnew_train$weights_col &lt;- ifelse(\n  new_train$Heart_Attack_Occurrence == \"Yes\",\n  w_yes,\n  w_no\n)\n\n\n\n\nFit weighted logistic model\n\nmodel_glm_weighted &lt;- glm(\n  formula = new_formula,\n  data    = new_train,\n  family  = binomial(link = \"logit\"),\n  weights = weights_col\n)\n\n\n# Capture the output\nresult &lt;- check_collinearity(model_glm_weighted)\n\n# Coerce to a data frame\ndf &lt;- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n\n\nCheck for Multicollinearity\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nAge\n1.004556\n1.000277\n1.074857\n1.002275\n0.9954646\n0.9303566\n0.9997228\n\n\nGender\n1.003347\n1.000075\n1.149809\n1.001672\n0.9966644\n0.8697096\n0.9999252\n\n\nFamily_History\n1.001847\n1.000002\n2.774072\n1.000923\n0.9981564\n0.3604809\n0.9999981\n\n\npoly(BMI, 2, raw = TRUE)\n1.003982\n1.000162\n1.097600\n1.001989\n0.9960340\n0.9110785\n0.9998376\n\n\nHeart_Rate\n1.002266\n1.000008\n1.614256\n1.001132\n0.9977391\n0.6194804\n0.9999916\n\n\npoly(Systolic_BP, 2, raw = TRUE)\n1.005107\n1.000419\n1.062202\n1.002550\n0.9949186\n0.9414402\n0.9995808\n\n\npoly(Diastolic_BP, 2, raw = TRUE)\n1.004244\n1.000211\n1.085491\n1.002120\n0.9957736\n0.9212419\n0.9997893\n\n\nCholesterol_Level\n1.001808\n1.000002\n3.013365\n1.000904\n0.9981953\n0.3318549\n0.9999984\n\n\nDiabetes_History\n1.002857\n1.000033\n1.244352\n1.001427\n0.9971512\n0.8036315\n0.9999666\n\n\nHypertension_History\n1.002289\n1.000009\n1.586196\n1.001144\n0.9977158\n0.6304391\n0.9999911\n\n\nPhysical_Activity\n1.004614\n1.000291\n1.073216\n1.002304\n0.9954073\n0.9317786\n0.9997093\n\n\nSmoking_History\n1.002518\n1.000016\n1.390398\n1.001258\n0.9974880\n0.7192187\n0.9999838\n\n\nDiet_Quality\n1.003581\n1.000102\n1.125179\n1.001789\n0.9964314\n0.8887471\n0.9998975\n\n\nAlcohol_Consumption\n1.007005\n1.001124\n1.043643\n1.003496\n0.9930440\n0.9581821\n0.9988770\n\n\nStress_Levels\n1.002948\n1.000039\n1.219987\n1.001473\n0.9970609\n0.8196808\n0.9999605\n\n\n\n\n\n\n\n\n\nVisualizing the model\n\npred_prob_improved &lt;- predict(model_glm_weighted, newdata = test_data, type = \"response\")\n\n\nggplot(mapping = aes(x = pred_prob_improved)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Distribution of Predicted Probabilities (Naïve Logistic)\",\n       x = \"Predicted Probability of Heart Attack\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nFrom the histogram above, we can see that the model is overpredicting positive cases at 0.50 threshold. As the positive cases is approximately 10% of the dataset, we will take a threshold of 0.55 instead.\n\ncheck_c &lt;- check_collinearity(model_glm_weighted)\np_collinearity &lt;- plot(check_c) +\n  labs(title = \"Collinearity of Weighted Logistic Model\") +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\npred_prob_improved &lt;- predict(model_glm_weighted, newdata = test_data, type = \"response\")\npred_class_improved &lt;- ifelse(pred_prob_improved &gt;= 0.55, \"Yes\", \"No\") %&gt;%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\nimproved_cm &lt;- data.frame(\n  obs  = test_data$Heart_Attack_Occurrence,\n  pred = pred_class_improved\n) %&gt;% conf_mat(obs, pred)\n\np_confmat &lt;- autoplot(improved_cm, type = \"heatmap\") +\n  labs(title = \"Weighted Logistic: Confusion Matrix\")\n\nroc_improved &lt;- roc(\n  response  = as.numeric(test_data$Heart_Attack_Occurrence),\n  predictor = as.numeric(pred_prob_improved)\n)\np_roc &lt;- ggroc(roc_improved, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Weighted Logistic Model\") +\n  theme_minimal()\n\n## Combine the three plots\nlibrary(patchwork)\ncombined_plot &lt;- p_collinearity / (p_confmat + p_roc)\ncombined_plot\n\n\n\n\n\n\n\n\n\nWeighted Logistic Regression Results\nAfter applying class weights and mild polynomial terms to BMI and Blood Pressure, our weighted logistic model shows some improvements compared to the naïve model:\n\nSensitivity Improves: The model now predicts a small number of “Yes” cases rather than labeling everything “No.”\nCollinearity appears more stable\n\nThe VIF plot still shows point estimates around 1.0 for most variables, with moderate spikes in confidence intervals for a few. However, these are less extreme than in the naïve model, suggesting the parameter estimates are more stable overall.\n\nLittle change to overall accuracy (AUC)\n\nWhile the model does somewhat better at identifying positives, the ROC curve remains fairly close to the diagonal, reflecting an AUC only slightly better than 0.5.\nIn other words, the model is still not very accurate overall, indicating that the available predictors may not strongly discriminate between “Yes” and “No”—or that we need further refinements (e.g., more complex interactions, alternative transformations, or additional data)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#fitting-a-xgboost",
    "href": "Take-home_Ex/Take-home_Ex1.html#fitting-a-xgboost",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Fitting a xgboost",
    "text": "Fitting a xgboost\n\nData preparation\nWe first convert the outcome Heart_Attack_Occurrence to a 0/1 numeric variable. Then, we build model matrices using model.matrix() which transforms both categorical and numeric predictors into a suitable format for XGBoost.\n\n# Convert outcome to 0/1\ntrain_data_xgb &lt;- train_data %&gt;%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\nx_train &lt;- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=train_data_xgb)\ny_train &lt;- train_data_xgb$YesNo\n\ntest_data_xgb &lt;- test_data %&gt;%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\nx_test &lt;- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=test_data_xgb)\ny_test &lt;- test_data_xgb$YesNo\n\nn_yes &lt;- sum(y_train == 1)\nn_no  &lt;- sum(y_train == 0)\nscale_pos &lt;- n_no / n_yes\n\n\n\nXGBoost model training\nWe create DMatrix objects for both training and test sets, then specify key hyperparameters like max_depth, eta, and scale_pos_weight. The model is trained with early stopping if the test AUC does not improve after a certain number of rounds.\n\ndtrain &lt;- xgb.DMatrix(data = x_train, label = y_train)\ndtest  &lt;- xgb.DMatrix(data = x_test,  label = y_test)\n\nparam &lt;- list(\n  objective        = \"binary:logistic\",\n  eval_metric      = \"auc\",             # can also track \"error\" or \"logloss\"\n  max_depth        = 10,\n  eta              = 0.2,\n  scale_pos_weight = scale_pos          # imbalance correction\n)\n\n# Train with 100 rounds\nset.seed(123)\nxgb_model &lt;- xgb.train(\n  params   = param,\n  data     = dtrain,\n  nrounds  = 1000,\n  watchlist= list(train=dtrain, test=dtest),\n  early_stopping_rounds = 50,  # optional, for early stop\n  print_every_n          = 10\n)\n\n[1] train-auc:0.703267  test-auc:0.493556 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 50 rounds.\n\n[11]    train-auc:0.893593  test-auc:0.519498 \n[21]    train-auc:0.955144  test-auc:0.513634 \n[31]    train-auc:0.977267  test-auc:0.519925 \n[41]    train-auc:0.985509  test-auc:0.514492 \n[51]    train-auc:0.993414  test-auc:0.505698 \n[61]    train-auc:0.996438  test-auc:0.503955 \n[71]    train-auc:0.998506  test-auc:0.502356 \nStopping. Best iteration:\n[28]    train-auc:0.972693  test-auc:0.525233\n\n\nTraining AUC may reach near 1.0 (overfitting), but test AUC remains around 0.52, implying the model struggles to find a robust pattern. However, this may also indicate a difference in distribution of the training and testing dataset, and that no robust pattern is learnable from these features. There may also be insufficient predictive signal in the data.\n\n\nModel evaluation\n\npred_prob_xgb &lt;- predict(xgb_model, newdata = dtest, iteration_range = xgb_model$best_iteration)\n\npred_class_xgb &lt;- ifelse(pred_prob_xgb &gt;= 0.5, 1, 0)\n\nresults_xgb &lt;- data.frame(\n  obs  = factor(y_test, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  pred = factor(pred_class_xgb, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  prob = pred_prob_xgb\n)\n\nxgb_cm &lt;- conf_mat(results_xgb, truth=obs, estimate=pred)\nxgb_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.763 \n 2 kap                  binary        0.0124\n 3 sens                 binary        0.826 \n 4 spec                 binary        0.191 \n 5 ppv                  binary        0.903 \n 6 npv                  binary        0.107 \n 7 mcc                  binary        0.0131\n 8 j_index              binary        0.0167\n 9 bal_accuracy         binary        0.508 \n10 detection_prevalence binary        0.824 \n11 precision            binary        0.903 \n12 recall               binary        0.826 \n13 f_meas               binary        0.863 \n\n\n\n\nROC curve and confusion matrix\n\np_confmat &lt;- autoplot(xgb_cm, type=\"heatmap\") +\n  labs(title=\"XGBoost Confusion Matrix\", fill=\"Count\")\n\nroc_xgb &lt;- roc(response = as.numeric(results_xgb$obs), predictor = results_xgb$prob)\nauc_val &lt;- auc(roc_xgb)\n\np_roc &lt;- ggroc(roc_xgb, colour=\"#1c61b6\") +\n  labs(\n    title = paste0(\"XGBoost ROC Curve (AUC=\", round(auc_val,3), \")\"),\n    x     = \"1 - Specificity\",\n    y     = \"Sensitivity\"\n  ) +\n  theme_minimal()\n\n\n\ncombined_plot &lt;- p_confmat | p_roc\n\ncombined_plot\n\n\n\n\n\n\n\n\n\nPlot explanation:\n\nConfusion matrix: Shows how many “No” vs. “Yes” cases are classified correctly vs. incorrectly. Despite the class‐imbalance correction, the model still misclassifies most “Yes” events.\nROC curve: The curve hovers close to the diagonal, with an AUC near ~0.52. This is only marginally better than random guessing (AUC=0.5).\n\n\nimportance_xgb &lt;- xgb.importance(model = xgb_model)\nimportance_xgb  # see a data frame of feature importances\n\n                        Feature        Gain       Cover   Frequency\n                         &lt;char&gt;       &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n 1:           Cholesterol_Level 0.145242292 0.155522476 0.136697155\n 2:                 Systolic_BP 0.136727355 0.167277235 0.132375113\n 3:               Stress_Levels 0.134981157 0.185245402 0.134184340\n 4:                Diastolic_BP 0.134287572 0.160294404 0.126846919\n 5:                         BMI 0.131566444 0.126584570 0.123932053\n 6:                  Heart_Rate 0.123786314 0.121115391 0.121921801\n 7:                         Age 0.083851778 0.050432098 0.092371093\n 8:           Family_HistoryYes 0.010253500 0.002744379 0.011056388\n 9:   Physical_ActivityModerate 0.009907179 0.004210412 0.011659463\n10:                 RegionUrban 0.009613210 0.001768051 0.012262539\n11:          Smoking_HistoryYes 0.009402740 0.003311462 0.011357925\n12:     Hypertension_HistoryYes 0.009347006 0.001676643 0.010754850\n13:                  GenderMale 0.009129164 0.005765353 0.011357925\n14:         Diabetes_HistoryYes 0.008797618 0.001136887 0.009850236\n15:      Alcohol_ConsumptionLow 0.008275298 0.001408138 0.010553825\n16:            Diet_QualityGood 0.007770305 0.001091494 0.009146648\n17: Alcohol_ConsumptionModerate 0.007592931 0.001353723 0.009850236\n18:            Diet_QualityPoor 0.007177992 0.003669527 0.007940497\n19:     Alcohol_ConsumptionNone 0.006439186 0.004718671 0.007638959\n20:        Physical_ActivityLow 0.005850960 0.000673683 0.008242034\n                        Feature        Gain       Cover   Frequency\n\n# Plot\nxgb.plot.importance(importance_xgb, top_n = 15, \n                    main=\"XGBoost Feature Importance\")\n\n\n\n\n\n\n\n\n\n\nInterpretation:\nWhile certain features (e.g., Cholesterol_Level, Systolic_BP) rank highest in splitting power, the test AUC is still low. This suggests either insufficient predictive signal or potential overfitting to training noise."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#final-thoughts",
    "href": "Take-home_Ex/Take-home_Ex1.html#final-thoughts",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Final thoughts",
    "text": "Final thoughts\nAcross multiple approaches (naive logistic, weighted logistic, XGBoost), the models struggle to achieve predictive accuracy on the test set. This may indicate that the available features (after dropping the undefined extras) do not strongly distinguish between heart attack occurrences. Additional data, refined feature engineering, or domain expertise might be necessary to improve predictive performance. Nonetheless, the exploratory visualizations provide insights into demographic and lifestyle patterns associated with heart attack risk in Japan."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608",
    "section": "",
    "text": "Welcome to Sindy’s ISSS608 Visual Analytics and Applications."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\n\n\n\nts_data &lt;- read_csv(\n  \"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)\n\n\nTip: make sure the format is correct (chr vs Date fields)\n\n19.2.3 Conventional base ts object versus tibble object\n\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\n\n\nDifferent dataframes… (explanation here) not a typical tibble dataframe &gt; class(ts_data_ts) [1] “mts” “ts” “matrix” “array” &gt; class(ts_data) [1] “spec_tbl_df” “tbl_df” “tbl” “data.frame”\nUse ts_data for data prep and conversion then convert to ts_data_ts for a timeseries object?\nsome explanation here!\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)\n\n\nclass(ts_tsibble) [1] “tbl_ts” “tbl_df” “tbl” “data.frame”\n\nsth that can be used by dpylr and tidyr and also timeseries time series manner?\nvisual time series.,,,\ntransform the series for you to work with first cause cannot see the header (numbers correspond to which country) - so transform to a long table. Prepare data by reordering data differently.\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nVisualising single time-series: ggplot2 methods\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\nor use facet you can define no of col –&gt; rows will be automatically calculated…\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE!!! intervals are not constant!!\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nUseful to start with\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\nshows you the distribution. e.g. Italians only come in at a big crowd in the month of Aug. Rest of the year hover around 5000.\nVietnams, higher in Jun and Jul months. Peak in jul. Sept to Dec relatively constant, Jan to May, gradual increase. Jun higher jump. Arrival count peak at jul\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)\n\n\n\n\n\n\n\n\n\n\n\nhow the time series correlated. only one variable - which is your time series variable\nauto correlation plots create 2 time lags… find the correlation between them - then you have multiple variables. - get the second variable by shifting lag=1 note the graph below starts from lag=1 (t vs t-1)\nChina - 6 mths period. Italy - 12 peak. weak correlation at 1st mth ~0.3 Note!!! 95% confidence level - should go above the blue line. China case, vietnam case, most/all the lags is statistically significant. China and vietnam, correlation decreases then increases again. The period different. Vietnam - 12 mths. China - 6 mths period UK lag t-1 significant… the rest of the year not sigificant, then 12 mths significant again - which means trend is not significant at all - drop down very fast - seasonal not important. (for UK and Italy)\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html#getting-started",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\n\n\n\nts_data &lt;- read_csv(\n  \"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)\n\n\nTip: make sure the format is correct (chr vs Date fields)\n\n19.2.3 Conventional base ts object versus tibble object\n\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\n\n\nDifferent dataframes… (explanation here) not a typical tibble dataframe &gt; class(ts_data_ts) [1] “mts” “ts” “matrix” “array” &gt; class(ts_data) [1] “spec_tbl_df” “tbl_df” “tbl” “data.frame”\nUse ts_data for data prep and conversion then convert to ts_data_ts for a timeseries object?\nsome explanation here!\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)\n\n\nclass(ts_tsibble) [1] “tbl_ts” “tbl_df” “tbl” “data.frame”\n\nsth that can be used by dpylr and tidyr and also timeseries time series manner?\nvisual time series.,,,\ntransform the series for you to work with first cause cannot see the header (numbers correspond to which country) - so transform to a long table. Prepare data by reordering data differently.\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nVisualising single time-series: ggplot2 methods\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\nor use facet you can define no of col –&gt; rows will be automatically calculated…\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE!!! intervals are not constant!!\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nUseful to start with\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\nshows you the distribution. e.g. Italians only come in at a big crowd in the month of Aug. Rest of the year hover around 5000.\nVietnams, higher in Jun and Jul months. Peak in jul. Sept to Dec relatively constant, Jan to May, gradual increase. Jun higher jump. Arrival count peak at jul\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)\n\n\n\n\n\n\n\n\n\n\n\nhow the time series correlated. only one variable - which is your time series variable\nauto correlation plots create 2 time lags… find the correlation between them - then you have multiple variables. - get the second variable by shifting lag=1 note the graph below starts from lag=1 (t vs t-1)\nChina - 6 mths period. Italy - 12 peak. weak correlation at 1st mth ~0.3 Note!!! 95% confidence level - should go above the blue line. China case, vietnam case, most/all the lags is statistically significant. China and vietnam, correlation decreases then increases again. The period different. Vietnam - 12 mths. China - 6 mths period UK lag t-1 significant… the rest of the year not sigificant, then 12 mths significant again - which means trend is not significant at all - drop down very fast - seasonal not important. (for UK and Italy)\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\")\n\n\n\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\nOr with glimpse:\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#getting-started",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\")\n\n\n\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\nOr with glimpse:\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#data-overview",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#data-overview",
    "title": "In-class Exercise 5",
    "section": "Data overview",
    "text": "Data overview\nType has 2 arguments: - Type = 1 - Type = 2…\n\nsummary1 &lt;- car_resale %&gt;%\n  ExpData(type=1)\n\nsummary1  # can be further customised (a table/df object)\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables        33\n4                              No. of factor variables         0\n5                                No. of text variables         5\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\nsummary2 &lt;- car_resale %&gt;%\n  ExpData(type=2)\n\nsummary2  # can be further customised (a table/df object)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\",\n           \"Cylinders\", \"Fuel_Type\", \"Color\",\n           \"Met_Color\", \"Automatic\", \"Mfr_Guarantee\", \"BOVAG_Guarantee\",\n           \"ABS\", \"Airbag_1\", \"Airbag_2\", \n           \"Airco\", \"Automatic_airco\", \"Boardcomputer\", \"CD_Player\",\n           \"Central_Lock\", \"Powered_Windows\", \"Power_Steering\",\"Radio\",\n           \"Mistlamps\", \"Sport_Model\", \"Backseat_Divider\", \"Metallic_Rim\",\n           \"Radio_cassette\", \"Tow_Bar\")\n\n\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each_(funs(factor(.)),cols)\n\n\nsummary(car_resale)\n\n      Id               Model               Price         Age_08_04    \n Length:1436        Length:1436        Min.   : 4350   Min.   : 1.00  \n Class :character   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Mode  :character   Mode  :character   Median : 9900   Median :61.00  \n                                       Mean   :10731   Mean   :55.95  \n                                       3rd Qu.:11950   3rd Qu.:70.00  \n                                       Max.   :32500   Max.   :80.00  \n                                                                      \n   Mfg_Month      Mfg_Year          KM         Quarterly_Tax        Weight    \n 1      :207   Min.   :1998   Min.   :     1   Min.   : 19.00   Min.   :1000  \n 4      :154   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00   1st Qu.:1040  \n 3      :138   Median :1999   Median : 63390   Median : 85.00   Median :1070  \n 2      :134   Mean   :2000   Mean   : 68533   Mean   : 87.12   Mean   :1072  \n 7      :133   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00   3rd Qu.:1085  \n 6      :120   Max.   :2004   Max.   :243000   Max.   :283.00   Max.   :1615  \n (Other):550                                                                  \n Guarantee_Period     HP_Bin      CC_bin    Doors   Gears    Cylinders\n Min.   : 3.000   &lt; 100  :560   &lt;1600:416   2:  2   3:   2   4:1436   \n 1st Qu.: 3.000   &gt; 120  : 11   &gt;1600:166   3:622   4:   1            \n Median : 3.000   100-120:865   1600 :854   4:138   5:1390            \n Mean   : 3.815                             5:674   6:  43            \n 3rd Qu.: 3.000                                                       \n Max.   :36.000                                                       \n                                                                      \n  Fuel_Type        Color     Met_Color Automatic Mfr_Guarantee BOVAG_Guarantee\n CNG   :  17   Grey   :301   0:467     0:1356    0:848         0: 150         \n Diesel: 155   Blue   :283   1:969     1:  80    1:588         1:1286         \n Petrol:1264   Red    :278                                                    \n               Green  :220                                                    \n               Black  :191                                                    \n               Silver :122                                                    \n               (Other): 41                                                    \n ABS      Airbag_1 Airbag_2 Airco   Automatic_airco Boardcomputer CD_Player\n 0: 268   0:  42   0: 398   0:706   0:1355          0:1013        0:1122   \n 1:1168   1:1394   1:1038   1:730   1:  81          1: 423        1: 314   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Central_Lock Powered_Windows Power_Steering Radio    Mistlamps Sport_Model\n 0:603        0:629           0:  32         0:1226   0:1067    0:1005     \n 1:833        1:807           1:1404         1: 210   1: 369    1: 431     \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Backseat_Divider Metallic_Rim Radio_cassette Tow_Bar \n 0: 330           0:1142       0:1227         0:1037  \n 1:1106           1: 294       1: 209         1: 399  \n                                                      \n                                                      \n                                                      \n                                                      \n                                                      \n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=NULL,\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=\"Price\",\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\nBar plots for all categorial variables\n\ncar_resale %&gt;%\n  ExpCatViz(target=NULL,\n            col=\"sky blue\",\n            clim=10,\n            margin=2,\n            Page=c(4,4),\n            sample=16)\n\n$`0`\n\n\n\n\n\n\n\n\n\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, data=car_resale)\n\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n# take out manufacturing year because of collinearlity \nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + Weight + Guarantee_Period, data=car_resale)\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\n\ncheck_heteroscedasticity(model1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n2 ways: 1. correlation matrix 2. vif\ngtsummary\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\ntbl_regression(model1, \n               intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-2,186\n-4,093, -278\n0.025\n\n\nAge_08_04\n-119\n-125, -114\n&lt;0.001\n\n\nKM\n-0.02\n-0.03, -0.02\n&lt;0.001\n\n\nWeight\n20\n18, 21\n&lt;0.001\n\n\nGuarantee_Period\n27\n2.1, 52\n0.034\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nBasic Regression Table\n\ntbl_regression(model1, \n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    # \"\\U03C3\" to extract the sigma value\n    label = list(sigma ~ \"\\U03C3\"),  # can ignore if you do not want the sigma\n    include = c(r.squared, adj.r.squared,\n                AIC, statistic,\n                p.value, sigma)\n  )\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "",
    "text": "Install and load the necessary packages:\n\npacman::p_load(scales, viridis, lubridate, ggthemes, \n               gridExtra, readxl, knitr, data.table, \n               CGPfunctions, ggHoriPlot, tidyverse, plotly)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#getting-started",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "",
    "text": "Install and load the necessary packages:\n\npacman::p_load(scales, viridis, lubridate, ggthemes, \n               gridExtra, readxl, knitr, data.table, \n               CGPfunctions, ggHoriPlot, tidyverse, plotly)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-calendar-heatmap",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "2. Plotting a Calendar Heatmap",
    "text": "2. Plotting a Calendar Heatmap\nA calendar heatmap can provide a quick visual cue of how data points (e.g., attacks, sales, events) distribute across days and hours of the week.\n\n2.1 The Data\nWe will use a CSV file (eventlog.csv) containing time-series data of cyber attacks by country.\n\n\n2.2 Import and Examine the Data\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n# Quick peek at the data\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n2.3 Data Preparation\nWe want to extract weekday and hour from the timestamp.\n\nStep 1: Write a function\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, tz = tz[1], quiet = TRUE)\n  dt &lt;- data.table(\n    source_country = sc,\n    wkday = weekdays(real_times),\n    hour  = hour(real_times)\n  )\n  return(dt)\n}\n\n\n\nStep 2: Use the function and re-factor\n\nwkday_levels &lt;- c('Saturday','Friday','Thursday','Wednesday','Tuesday','Monday','Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, .$source_country, .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    wkday = factor(wkday, levels = wkday_levels),\n    hour  = factor(hour, levels = 0:23)\n  )\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n2.4 Building the Calendar Heatmap\nAggregate the data and visualize:\n\ngrouped &lt;- attacks %&gt;%\n  count(wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, aes(hour, wkday, fill = n)) +\n  geom_tile(color = \"white\", size = 0.1) +\n  theme_tufte(base_family = \"Helvetica\") +\n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\", \n                      low = \"sky blue\", high = \"dark blue\") +\n  labs(x = NULL, y = NULL,\n       title = \"Attacks by weekday and time of day\") +\n  theme(\n    axis.ticks = element_blank(),\n    plot.title = element_text(hjust = 0.5),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 6)\n  )\n\n\n\n\n\n\n\n\n\n\n2.5 Multiple Calendar Heatmaps (Facets)\n\nStep 1: Find top 4 countries\n\nattacks_by_country &lt;- count(attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\n\nStep 2: Extract only these top 4 countries\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\nStep 3: Plot\n\nggplot(top4_attacks, aes(hour, wkday, fill = n)) +\n  geom_tile(color = \"white\", size = 0.1) +\n  theme_tufte(base_family = \"Helvetica\") +\n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                      low = \"sky blue\", \n                      high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(\n    x = NULL, \n    y = NULL, \n    title = \"Attacks on top 4 countries by weekday and time of day\"\n  ) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text.x = element_text(size = 7),\n    plot.title = element_text(hjust = 0.5),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 6)\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-cycle-plot",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "3. Plotting a Cycle Plot",
    "text": "3. Plotting a Cycle Plot\nA cycle plot highlights intra-year (or intra-cycle) patterns and helps compare across years (cycles).\n\n3.1 Data Import\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n3.2 Derive Month and Year\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels = 1:12, \n                    labels = month.abb, \n                    ordered = TRUE)\nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n3.3 Extract Target Country\n\nVietnam &lt;- air %&gt;%\n  select(Vietnam, month, year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n3.4 Compute Averages\n\nhline.data &lt;- Vietnam %&gt;%\n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(Vietnam))\n\n\n\n3.5 Plot the Cycle Plot\n\nggplot(Vietnam, aes(x = year, y = Vietnam, group = month)) + \n  geom_line(color = \"black\") +\n  geom_hline(\n    data     = hline.data, \n    aes(yintercept = avgvalue),\n    linetype = 6, \n    colour   = \"red\",\n    size     = 0.5\n  ) + \n  facet_wrap(~month, nrow = 1) +\n  scale_x_continuous(breaks = seq(2010, 2019,3)) +\n  labs(\n    x     = \"Year\",\n    y     = \"No. of Visitors\",\n    title = \"Visitor arrivals from Vietnam by air, Jan 2010–Dec 2019\"\n  ) +\n  theme_gray(base_size = 8)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-slopegraph",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "4. Plotting a Slopegraph",
    "text": "4. Plotting a Slopegraph\nA slopegraph compares changes between two or more time points across different categories. We’ll use newggslopegraph() from CGPfunctions.\n\n4.1 Data Import\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n4.2 Plot the Slopegraph\n\nrice %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(\n    Year, \n    Yield, \n    Country,\n    Title = \"Rice Yield of Top 11 Asian Countries\",\n    SubTitle = \"1961-1980\",\n  )\n\n\n\n\n\n\n\n\n\nTip: Converting Year to a factor helps emphasize discrete time points."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#more-plots",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#more-plots",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "5. More plots",
    "text": "5. More plots\n\n5.1 Highlighting the Maximum/Minimum in a Cycle Plot\nTo highlight extremes in the cycle plot, we can try:\n\nVietnam_extremes &lt;- Vietnam %&gt;%\n  group_by(month) %&gt;%\n  mutate(\n    max_val = max(Vietnam),\n    min_val = min(Vietnam)\n  )\n\nggplot(Vietnam_extremes, aes(x = year, y = Vietnam, group = month)) +\n  geom_line() +\n  geom_point(data = subset(Vietnam_extremes, Vietnam == max_val),\n             aes(color = \"Max\"), size = 2) +\n  geom_point(data = subset(Vietnam_extremes, Vietnam == min_val),\n             aes(color = \"Min\"), size = 2) +\n  facet_wrap(~ month) +\n  scale_color_manual(values = c(\"Max\" = \"red\", \"Min\" = \"blue\")) +\n  labs(\n    title = \"Visitor Arrivals - Highlighting Max and Min by Month\",\n    color = \"\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n5.2 Adding a Trendline to the Slopegraph\nWhile slopegraphs usually connect discrete points, we can experiment with a small data set over more than two years by adding intermediate lines:\n\nrice %&gt;%\n  filter(Year %in% c(1961,1970,1980)) %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  newggslopegraph(\n    Year, \n    Yield, \n    Country,\n    Title = \"Rice Yield Over Multiple Time Points\",\n    SubTitle = \"Using Slopegraph with 1961, 1970, 1980\",\n    Caption = \"Extended example\"\n  )\n\n\n\n\n\n\n\n\n\n\n5.3 Plotting a Horizon Chart\nA horizon chart can compactly show how a time series changes relative to a baseline. The ggHoriPlot package can help.\nExample (using the arrivals_by_air.xlsx data for Vietnam):\n\nhorizon_data &lt;- Vietnam %&gt;%\n  mutate(Date = ymd(paste(year, match(month, month.abb), \"01\", sep=\"-\"))) %&gt;%\n  select(Date, Visitors = Vietnam)\n\nggplot(horizon_data, aes(x = Date, y = Visitors)) +\n  geom_horizon() +\n  theme_minimal() +\n  facet_wrap(~ \"Visitor Arrivals Horizon Chart\", ncol = 1)\n\n\n\n\n\n\n\n\n\nNote: Horizon charts are especially useful if you have many time series to stack vertically.\n\n\n\n5.4 Making an Interactive Time-Series Plot\nWe could also convert any of these ggplot objects to an interactive plot using plotly or ggiraph:\n\np &lt;- ggplot(horizon_data, aes(x = Date, y = Visitors)) +\n  geom_line() + theme_minimal()\n\nggplotly(p)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#reference",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "6. Reference",
    "text": "6. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "",
    "text": "Parallel coordinates plots are an effective way to visualize and analyze multivariate numerical data. They allow you to compare multiple variables simultaneously and reveal relationships among them—for example, how various indicators contribute to the World Happiness Index. In this exercise, we will create both static and interactive parallel coordinates plots using the World Happiness 2018 dataset. We also include a couple of additional insights through custom graphs.\n\n\nWe load the necessary R packages: GGally, parallelPlot, and tidyverse.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names (if needed), and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n# (Optional) Set country names as row identifiers if needed:\nrow.names(wh) &lt;- wh$Country\n\n# Select relevant columns for analysis (e.g., columns 7 to 12 contain numerical indicators)\nwh_selected &lt;- dplyr::select(wh, c(7:12))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#overview",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "",
    "text": "Parallel coordinates plots are an effective way to visualize and analyze multivariate numerical data. They allow you to compare multiple variables simultaneously and reveal relationships among them—for example, how various indicators contribute to the World Happiness Index. In this exercise, we will create both static and interactive parallel coordinates plots using the World Happiness 2018 dataset. We also include a couple of additional insights through custom graphs.\n\n\nWe load the necessary R packages: GGally, parallelPlot, and tidyverse.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names (if needed), and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n# (Optional) Set country names as row identifiers if needed:\nrow.names(wh) &lt;- wh$Country\n\n# Select relevant columns for analysis (e.g., columns 7 to 12 contain numerical indicators)\nwh_selected &lt;- dplyr::select(wh, c(7:12))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-ggally",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-ggally",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "2. Static Parallel Coordinates Plot with GGally",
    "text": "2. Static Parallel Coordinates Plot with GGally\nUsing the GGally package’s ggparcoord() function, we first create static parallel coordinates plots.\n\n2.1 Basic Parallel Coordinates Plot\nA simple parallel coordinates plot showing the selected numerical variables:\n\nggparcoord(data = wh, \n           columns = c(7:12)) +\n  labs(title = \"Basic Parallel Coordinates Plot\")\n\n\n\n\n\n\n\n\n\n\n2.2 Enhanced Parallel Coordinates Plot with Grouping\nHere we group observations by a variable (e.g., Region in column 2), scale variables using the uniminmax method, lower line opacity, and overlay boxplots to better reveal the distribution.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happiness Variables\") +\n  labs(x = \"Indicators\", y = \"Scaled Value\")\n\n\n\n\n\n\n\n\n\n\n2.3 Parallel Coordinates Plot with Facets\nWe can create small multiples by faceting the plot by region. This approach helps compare patterns across geographical areas.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plots by Region\") +\n  facet_wrap(~ Region) +\n  labs(x = \"Indicators\", y = \"Scaled Value\")\n\n\n\n\n\n\n\n\n\n\n2.4 Rotating and Adjusting x-axis Labels\nWhen variable names overlap on the x-axis, we can rotate and adjust their positioning using theme() from ggplot2.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plots by Region with Rotated x-axis Labels\") +\n  facet_wrap(~ Region) +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(x = \"Indicators\", y = \"Scaled Value\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#interactive-parallel-coordinates-plot-with-parallelplot",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#interactive-parallel-coordinates-plot-with-parallelplot",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "3. Interactive Parallel Coordinates Plot with parallelPlot",
    "text": "3. Interactive Parallel Coordinates Plot with parallelPlot\nThe parallelPlot package provides an interactive version of parallel coordinates plots built on htmlwidgets and d3.js.\n\n3.1 Basic Interactive Plot\nWe first create a basic interactive plot. For clarity, we select a subset of variables including the “Happiness score” (if available) along with the numerical indicators.\n\n# Adjust the data: select \"Happiness score\" (if present) and columns 7:12\nwh_interactive &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\n\n# Basic interactive parallel coordinates plot\nparallelPlot(wh_interactive,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n3.2 Rotate Axis Labels\nTo avoid overlapping axis labels, use the rotateTitle argument.\n\nparallelPlot(wh_interactive,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n3.3 Changing the Colour Scheme\nChange the default blue colour scheme to a yellow–orange–red palette using the continuousCS argument.\n\nparallelPlot(wh_interactive,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n3.4 Interactive Plot with Histograms\nDisplay histograms along each variable’s axis by setting the histoVisibility argument.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh_interactive))\nparallelPlot(wh_interactive,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-custom-theme",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-custom-theme",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "4. Static Parallel Coordinates Plot with Custom Theme",
    "text": "4. Static Parallel Coordinates Plot with Custom Theme\nUsing GGally together with additional ggplot2 theme modifications, we create a static parallel coordinates plot with a refined aesthetic.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.3,\n           boxplot = TRUE, \n           title = \"Enhanced Static Parallel Coordinates Plot\") +\n  labs(x = \"Happiness Indicators\", y = \"Scaled Value\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#reference",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\n\n\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#getting-started",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\n\n\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "2. Building Correlation Matrix: pairs() method",
    "text": "2. Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\n\n2.1 Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n2.2 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\n\nLower halfUpper half\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "3. Visualising Correlation Matrix: ggcormat()",
    "text": "3. Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\n\n3.1 The basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#building-multiple-plots",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "4.Building multiple plots",
    "text": "4.Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. This feature is available in grouped_ggcorrmat().\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "5. Visualising Correlation Matrix using corrplot Package",
    "text": "5. Visualising Correlation Matrix using corrplot Package\n\n5.1 Getting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n5.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. This default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n5.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n5.4 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n5.5 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n5.6 Reorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n5.7 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         lower = \"ellipse\", \n         upper = \"number\",\n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#reference",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "6. Reference",
    "text": "6. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities.\n\n\nIn this exercise, four R packages will be used.\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#overview",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities.\n\n\nIn this exercise, four R packages will be used.\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "2. FunnelPlotR methods",
    "text": "2. FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n2.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`, \n  data_type = \"SR\",       \n  limit = 95              \n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_type argument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n2.2 FunnelPlotR methods: Makeover 1\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),\n  yrange = c(0, 0.05) \n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\nxrange and yrange are used to set the range of x-axis and y-axis.\n\n\n\n2.3 FunnelPlotR methods: Makeover 2\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "3. Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "3. Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, we will build funnel plots step-by-step by using ggplot2.\n\n3.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n3.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n3.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n3.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#reference",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "4. Reference",
    "text": "4. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\n\n\n\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\ngghistostats(): This function creates a histogram with statistical annotations.\n\nx = ENGLISH: Specifies the variable (English scores) to be analyzed.\ntype = \"bayes\": Indicates a Bayesian one-sample test is conducted.\ntest.value = 60: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.\nxlab = \"English scores\": Labels the x-axis as “English scores.”\n\n\n\n\nHistogram: Displays the distribution of English scores with gray bars.\nY-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.\nDashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, \\(\\hat{\\mu}_{MAP}\\)), which is approximately 74.74.\nStatistical Annotations:\n\n\\(\\log_e(BF_{01}) = -31.45\\): The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).\n\\(\\delta_{\\text{difference}}^{\\text{posterior}} = 7.16\\): The estimated mean difference between the sample mean and 60.\n\\(CI^{ETI}_{95\\%} [5.54, 8.75]\\): The 95% credible interval (Highest Density Interval) for the mean difference.\n\\(r^{JZS}_{Cauchy} = 0.71\\): The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.\n\n\n\n\n\nThe English scores are right-skewed and centered around 74.74, which is significantly higher than the test value of 60.\nThe negative log Bayes factor (-31.45) provides overwhelming evidence against the null hypothesis.\nThe credible interval [5.54, 8.75] indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above 60."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#getting-started",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\n\n\n\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\ngghistostats(): This function creates a histogram with statistical annotations.\n\nx = ENGLISH: Specifies the variable (English scores) to be analyzed.\ntype = \"bayes\": Indicates a Bayesian one-sample test is conducted.\ntest.value = 60: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.\nxlab = \"English scores\": Labels the x-axis as “English scores.”\n\n\n\n\nHistogram: Displays the distribution of English scores with gray bars.\nY-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.\nDashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, \\(\\hat{\\mu}_{MAP}\\)), which is approximately 74.74.\nStatistical Annotations:\n\n\\(\\log_e(BF_{01}) = -31.45\\): The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).\n\\(\\delta_{\\text{difference}}^{\\text{posterior}} = 7.16\\): The estimated mean difference between the sample mean and 60.\n\\(CI^{ETI}_{95\\%} [5.54, 8.75]\\): The 95% credible interval (Highest Density Interval) for the mean difference.\n\\(r^{JZS}_{Cauchy} = 0.71\\): The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.\n\n\n\n\n\nThe English scores are right-skewed and centered around 74.74, which is significantly higher than the test value of 60.\nThe negative log Bayes factor (-31.45) provides overwhelming evidence against the null hypothesis.\nThe credible interval [5.54, 8.75] indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above 60."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#bayes-factor",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#bayes-factor",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "2. Bayes Factor",
    "text": "2. Bayes Factor\n\n2.1 Unpacking the Bayes Factor\n\nA Bayes Factor (BF) is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThe Bayes Factor provides a way to evaluate data in favor of a null hypothesis and to incorporate external information in doing so. It quantifies the weight of the evidence in favor of a given hypothesis.\nWhen comparing two hypotheses, \\(H_1\\) (the alternative hypothesis) and \\(H_0\\) (the null hypothesis), the Bayes Factor is often written as \\(BF_{10}\\). Mathematically, it is defined as:\n\\[\nBF_{10} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)}\n\\]\n\nwhere:\n\n\\(P(D \\mid H_1)\\) is the probability of the observed data given that the alternative hypothesis is true.\n\\(P(D \\mid H_0)\\) is the probability of the observed data given that the null hypothesis is true.\nA Bayes Factor greater than 1 indicates evidence in favor of \\(H_1\\), while a Bayes Factor less than 1 supports \\(H_0\\).\n\nThe Schwarz criterion (Bayesian Information Criterion, BIC) is one of the simplest ways to approximate the Bayes Factor.\n\n\n2.2 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#hypothesis-testing",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#hypothesis-testing",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "3. Hypothesis Testing",
    "text": "3. Hypothesis Testing\n\n3.1 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\n\nx = GENDER → Categorical variable (independent variable) representing gender groups (Male & Female).\ny = MATHS → Numeric variable (dependent variable) representing Maths scores.\ntype = \"np\" → Specifies a nonparametric test (Mann-Whitney U test, also known as the Wilcoxon rank-sum test) instead of a parametric t-test.\n\n\n\n\nViolin Plots:\n\nShow the distribution of Maths scores for Female (left, teal) and Male (right, orange).\nThe width of the violin represents the density of data points.\n\nBoxplots Inside Violin Plots:\n\nThe black box within each violin represents the interquartile range (IQR) (middle 50% of data).\nThe black horizontal line inside the box represents the median.\nThe whiskers extend to the smallest and largest values within 1.5 times the IQR.\n\nIndividual Data Points:\n\nEach dot represents an individual student’s Maths score.\nProvides insight into the spread and density of scores.\n\nMann-Whitney U Test Results (Top Annotation):\n\n\\(W_{Mann-Whitney} = 13011.00\\) → The Mann-Whitney U test statistic.\n\\(p = 0.91\\) → High p-value suggests no significant difference between the two groups.\n\\(\\hat{r}_{biserial}^{rank} = 7.04e-03\\) → Rank-biserial correlation effect size (very small effect).\n\\(CI_{95\\%} [-0.12, 0.13]\\) → 95% confidence interval for the effect size.\n\\(n_{obs} = 322\\) → Total number of observations (170 females, 152 males).\n\n\n\n\n\nThe p-value (0.91) is very high, suggesting no statistically significant difference in Maths scores between genders.\nThe confidence interval [-0.12, 0.13] includes zero, reinforcing the lack of a meaningful effect.\nThe effect size is nearly zero, further indicating no meaningful difference in Maths performance based on gender.\n\n\n\n\n\n\n3.2 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\n\ntype = \"p\" → Performs a parametric test (Welch’s ANOVA for unequal variances).\nmean.ci = TRUE → Displays mean and confidence intervals for each group.\npairwise.comparisons = TRUE → Conducts post-hoc pairwise comparisons (e.g., Games-Howell test for unequal variances).\npairwise.display = \"s\"\n\n\"ns\" → Shows only non-significant comparisons.\n\"s\" → Shows only significant comparisons (used in this case).\n\"all\" → Shows all comparisons.\n\np.adjust.method = \"fdr\" → Adjusts p-values for multiple comparisons using the False Discovery Rate (FDR) correction.\nmessages = FALSE → Suppresses console messages.\n\n\n\n\n1. One-Way Welch ANOVA Results\n\n\\(F_{\\text{Welch}}(3, 23.8) = 10.15\\) → The Welch’s ANOVA test statistic\n\n\\(p = 1.71 \\times 10^{-4}\\) → The p-value, indicating a statistically significant difference among groups\n\n\\(\\hat{\\omega}^2_p = 0.50\\) → Effect size (moderate to large effect)\n\n\\(CI_{95\\%} [0.21, 1.00]\\) → Confidence interval for the effect size\n\n\\(n_{\\text{obs}} = 322\\) → Number of observations\n\nThis suggests that there is a significant difference in English scores across racial groups.\n\n\n2. Violin & Boxplots\n\nThe violin plot shows the distribution of scores.\n\nThe boxplot (inside the violin plot) summarizes:\n\nThe median (middle line in the box)\nThe interquartile range (box)\nThe whiskers (spread of data)\nThe mean (red dot with label)\n\n\n\n\n3. Post-Hoc Pairwise Comparisons\n\nThe Games-Howell test is used for post-hoc analysis (adjusted for multiple comparisons).\n\nOnly significant comparisons are displayed.\n\nThe p-value adjustment method used is False Discovery Rate (FDR).\n\n\n\n4. Bayesian Statistics\nAt the bottom:\n\n\\(\\log_e(BF_{01}) = -11.63\\) → Bayesian evidence against the null hypothesis\n\n\\(R^2_{\\text{posterior}} = 0.09\\) → Bayesian effect size\n\n\\(CI_{95\\%} [0.04, 0.15]\\) → Bayesian credible interval\n\n\\(r_{\\text{Cauchy}} = 0.71\\) → Cauchy prior width used\n\nThe negative log Bayes Factor \\(BF_{01}\\) suggests strong evidence against the null hypothesis, meaning there is a significant difference between the groups.\n\n\n\n\nThe one-way Welch ANOVA confirms a significant difference in English scores across racial groups.\nThe Chinese and “Other” groups have higher mean scores than the Indian and Malay groups.\nPost-hoc Games-Howell comparisons highlight significant differences.\nBayesian analysis supports the findings from the frequentist approach.\n\n\n\n\n\n\n\n3.3 Significance Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visualization for the Significance Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n3.4 Significance Test of Association (Dependence): ggbarstats() Method\nIn the code chunk below, the Maths scores are binned into a 4-class variable using the cut() function.\n\nexam1 &lt;- exam_data %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0, 60, 75, 85, 100))\n  )\n\n\nggbarstats(\n  data = exam1, \n  x = MATHS_bins, \n  y = GENDER\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#visualising-models",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "4. Visualising Models",
    "text": "4. Visualising Models\nIn this section, Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n4.1 Visualising Models\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n4.2 Importing Excel File: readxl Methods\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNote that the output object car_resale is a tibble data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#building-and-diagnosing-a-multiple-regression-model",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#building-and-diagnosing-a-multiple-regression-model",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "5. Building and Diagnosing a Multiple Regression Model",
    "text": "5. Building and Diagnosing a Multiple Regression Model\n\n5.1 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n5.2 Multiple Regression Model using lm()\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n5.3 Model Diagnostic: checking normality assumption\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n5.4 Model Diagnostic: Check model for homogeneity of variances\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n5.5 Model Diagnostic: Complete check\n\ncheck_model(model1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#visualizing-regression-results",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#visualizing-regression-results",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "6. Visualizing Regression Results",
    "text": "6. Visualizing Regression Results\n\n6.1 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n6.2 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#reference",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "7. Reference",
    "text": "7. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics.\n\n\n\nLearn how to create animated data visualizations using gganimate and plotly in R.\nGain hands-on experience in reshaping data with the tidyr package and processing, wrangling, and transforming data with the dplyr package.\n\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#overview",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics.\n\n\n\nLearn how to create animated data visualizations using gganimate and plotly in R.\nGain hands-on experience in reshaping data with the tidyr package and processing, wrangling, and transforming data with the dplyr package.\n\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#getting-started",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 Loading the R packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n2.2 Importing the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nCode chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_at() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\nInstead of using mutate_at(), across() can be used to derive the same outputs."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "3. Animated Data Visualisation: gganimate methods",
    "text": "3. Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n3.1 Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n3.2 Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "4. Animated Data Visualisation: plotly",
    "text": "4. Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n4.1 Building an animated bubble plot: ggplotly() method\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component for controlling the animation\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk below.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n4,2 Building an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#reference",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "2. Beyond ggplot2 Annotation: ggrepel",
    "text": "2. Beyond ggplot2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nDefaultggrepel\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID),\n                   hjust = .5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\nWe simply replace geom_label() by geom_label_repel()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "3. Beyond ggplot2 Themes",
    "text": "3. Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n3.1 Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, The Economist theme is used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\nIt also provides some extra geoms and scales for ‘ggplot2’.\n\nScatter plotBar chart\n\n\n\nggplot(data = exam_data, aes(x = MATHS, y = ENGLISH, color = GENDER)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_economist() +\n  ggtitle(\"English vs. Maths Scores by Gender\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, aes(x = MATHS, fill = GENDER)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_wsj() +\n  ggtitle(\"Student Count by Maths and Gender\") +\n  theme_wsj() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Working with hrbthems package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 25)) + # Set x-axis breaks at 25-unit intervals\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "4. Beyond Single Graph",
    "text": "4. Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, you will learn how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\n\nGraph 1Graph 2Graph 3\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np1\n\n\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np2\n\n\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\np3\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 Creating Composite Graphics: pathwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, I am going to shared with you an ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n4.2 Combining two ggplot2 graphs\nFigure below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n4.3 Combining three ggplot2 graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n/ operator to stack two ggplot2 graphs,\n| operator to place the plots beside each other,\n() operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\n\n\n4.4 Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n\n\n4.5 Creating figure with inset\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n4.6 Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#reference",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html",
    "href": "Hands-on_Ex/Hands-on_Ex00.html",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#getting-started",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#importing-data",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Importing data",
    "text": "Importing data\nThis is an outdated version.\n\nrealis_csv &lt;- read.csv(\"data/REALIS2019.csv\")\n\nCode chunk below uses uses read_csv() of readr to import REALIS2019.csv into r environment as a tibble data.frame.\nUse _ instead of . functions to prevent changes made to column names\n\nrealis2019 &lt;- read_csv(\"data/REALIS2019.csv\")\n\n\npopdata_fat &lt;- read_csv(\"data/PopData2019_fat.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#pivoting-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#pivoting-data",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Pivoting data",
    "text": "Pivoting data\n\n#! eval: FALSE\npopdata_long &lt;- popdata_fat %&gt;%\n  pivot_longer(c(3:21),\n               names_to = \"Age Group\",\n               values_to = \"Population\")\n\n\nMore pipes:\n\n#popdata_long &lt;- popdata_fat %&gt;%\n#  pivot_longer(c(3:21),\n#               names_to = \"Age Group\",\n#               values_to = \"Population\") %&gt;%\n#  select(\"Age Group\" == \"5_to_9\")\n\n\n# Filter the dataset for a specific Age Group, e.g., \"0_to_4\"\npopdata_filtered &lt;- popdata_long %&gt;%\n  filter(`Age Group` == \"0_to_4\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#save-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#save-data",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Save data",
    "text": "Save data\nrds: r native file format - always good to save as rds format\n\nwrite_rds(popdata_fat, \"data/rds/popdata_fat.rds\")\nwrite_rds(popdata_long, \"data/rds/popdata_long.rds\")\n\n{r, eval=FALSE}: only display code, does not run the code\n{r, echo=FALSE}: you run the code in the background, without displaying code"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#working-with-dplyr",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#working-with-dplyr",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Working with dplyr",
    "text": "Working with dplyr\n\nSelecting columns\n\nrealis2019_selected &lt;- realis2019 %&gt;%\n  select(`Project Name`,\n         `Transacted Price ($)`,\n         `Property Type`,\n         `Type of Sale`,\n         `Unit Price ($ psm)`)\n\n\n\nFiltering columns\n\nrealis2019_filtered &lt;- realis2019_selected %&gt;%\n  filter(`Property Type` == \"Condominium\" |\n           `Property Type` == \"Apartment\") %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  filter(`Unit Price ($ psm)` &lt;= 13000)\n\n\n\nCombining select and filter with pipe\nWe can also combine the above two operations into a single call.\n\nrealis2019_end &lt;- realis2019 %&gt;%\n  select(`Project Name`,\n         `Transacted Price ($)`,\n         `Property Type`,\n         `Type of Sale`,\n         `Unit Price ($ psm)`) %&gt;%\n  filter(`Property Type` == \"Condominium\" |\n           `Property Type` == \"Apartment\") %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  filter(`Unit Price ($ psm)` &lt;= 13000)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! I’m Sindy! 👋\nI’m excited to start my first course with Professor Kam Tin Seong. 🎉 This is my first time using R and Quarto️, and I can’t wait to dive into Visual Analytics and Applications.\nLooking forward to this journey and seeing where it takes me. 🌈🌟"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "2. R Graphics VS ggplot",
    "text": "2. R Graphics VS ggplot\nFirst, let us compare how R Graphics, the core graphical functions of Base R and ggplot plot a simple histogram.\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "3. Grammar of Graphics",
    "text": "3. Grammar of Graphics\nGrammar of Graphics defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\nThere are two principles in Grammar of Graphics, they are:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mapping\n\nA good grammar of graphics will allow us to:\n\nGain insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox 1978).\nProvide a strong foundation for understanding a diverse range of graphics.\nGuide us on what a well-formed or correct graphic looks like.\n\nNote: there will still be many grammatically correct but nonsensical graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "4. Essential Grammatical Elements in ggplot2: data",
    "text": "4. Essential Grammatical Elements in ggplot2: data\n\nggplot(data=exam_data) # ggplot() initializes a ggplot object.\n# output:A blank canvas\n\n\n# Aesthetic mappings\nggplot(data=exam_data, \n       aes(x= MATHS))\n# output: ggplot that includes the x-axis and the axis’s label."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-geom",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5. Essential Grammatical Elements in ggplot2: geom",
    "text": "5. Essential Grammatical Elements in ggplot2: geom\nGeometric objects are the actual marks we put on a plot. Examples include:\n\n\nA plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator.\nFor complete list, please refer to here.\n\n\n5.1 Geometric Objects: geom_bar\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n5.2 Geometric Objects: geom_dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\n\n\n\n\n\n\n\n\nUnderstanding the parameters:\n\nbinwidth: The default binwidth is 1.\n\nEach bin will cover an interval of 1 unit on the x-axis.\nFor example, if your x-axis represents test scores ranging from 0 to 100, the bins will be [0-1), [1-2), [2-3), …, [99-100).\nbinwidth = 2.5 means that test scores are grouped into intervals of 2.5 units e.g. [0-2.5), [2.5, 5)…\n\nscale_y_continuous() is used to turn off the y-axis.\n\nThe range of y-axis is 0-1 which can can potentially distort the interpretation of the data.\n\n\n\n\n\n5.3 Geometric Objects: geom_histogram\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 25)\n\n\n\n\n\n\n\n\nThe default bin is 30. By setting bins = 25, the number of bins is consistent with the test scores along the x-axis.\n\n\n5.4 Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 25,\nfill argument is used to shade the histogram with light blue color, and\ncolor argument is used to change the outline colour of the bars in black\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\n\n\n\n\n\n\n\n5.5 Modifying a geometric object by changing aes()\nThe code chunk below changes the interior colour of the histogram (i.e. fill) by using sub-group of aesthetic().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric.\n\n\n5.6 Geometric Objects: geom-density()\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of MATHS scores in a kernel density estimate plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density() \n\n\n\n\n\n\n\n\nThe code chunk below plots two kernel density lines by using colour or fill arguments of aes()\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\n5.7 Geometric Objects: geom_boxplot()\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\nThe code chunk below plots boxplots by using geom_boxplot().\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()\n\n\n\n\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of MATHS scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\n\n5.8 Geometric Objects: geom_violin()\ngeom_violin() is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of Maths score by gender in violin plot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n5.9 Geometric Objects: geom_point()\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\n\n\n\n\n5.10 geom objects can be combined\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "6. Essential Grammatical Elements in ggplot2: stat",
    "text": "6. Essential Grammatical Elements in ggplot2: stat\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n6.1 Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n6.2 Working with stat - the stat_summary() method\nThe code chunk below adds mean values by using stat_summary() function and overriding the default geom.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour =\"red\",        \n               size=4)               \n\n\n\n\n\n\n\n\n\n\n6.3 Working with stat - the geom() method\nThe code chunk below adding mean values by using geom_() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour=\"red\",          \n             size=4)  \n\n\n\n\n\n\n\n\nSections 6.2 and 6.3 produce the same output. Both approaches add red points representing the mean values to the boxplots, but these are achieved through slightly different syntax.\n\n\n6.4 Adding a best fit curve on a scatterplot?\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve. In the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\n\n\n\n\n\n\n\nNote: The default method used is loess.\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "7. Essential Grammatical Elements in ggplot2: facets",
    "text": "7. Essential Grammatical Elements in ggplot2: facets\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_grid() and facet_wrap().\n\n7.1 Working with facet_wrap()\nfacet_wrap() wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nThe code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n7.2 facet_grid() function\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\nThe code chunk below plots a trellis plot using facet_grid()\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "8. Essential Grammatical Elements in ggplot2: Coordinates",
    "text": "8. Essential Grammatical Elements in ggplot2: Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use. They are:\n\ncoord_cartesian(): The default cartesian coordinate system, where you specify x and y values (e.g., allows you to zoom in or out).\ncoord_flip(): A cartesian system with the x and y axes flipped.\ncoord_fixed(): A cartesian system with a “fixed” aspect ratio (e.g., 1.78 for a “widescreen” plot).\ncoord_quickmap(): A coordinate system that approximates a good aspect ratio for maps.\n\n\n8.1 Working with Coordinate\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe code chunk below flips the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n8.2 Changing the y- and x-axis range\nThe scatterplot on the below is slightly misleading because the y-aixs and x-axis range are not equal.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\n\n\n\n\n\nThis is better\nThe code chunk below fixed both the y-axis and x-axis range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "9. Essential Grammatical Elements in ggplot2: themes",
    "text": "9. Essential Grammatical Elements in ggplot2: themes\nThemes control elements of the graph not related to the data. For example:\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\nBuilt-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\nDefault themeClassic themeMinimal theme\n\n\nThe code chunk below plot a horizontal bar chart using theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\nA horizontal bar chart plotted using theme_classic().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nA horizontal bar chart plotted using theme_minimal().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#reference",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "10. Reference",
    "text": "10. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "Beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#getting-started",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "Beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "2. Interactive Data Visualisation - ggiraph methods",
    "text": "2. Interactive Data Visualisation - ggiraph methods\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n2.1 Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package.\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nNotice that two steps are involved.\n\nFirst, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph.\nThen, girafe() will be used to generate an svg (scalable vector graphics) object to be displayed on an html page.\n\n\n\n2.2 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\n2.3 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold. Refer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n2.4 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n2.5 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n2.6 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n2.7 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n2.8 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#more-interactive-plots",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#more-interactive-plots",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3. More interactive plots",
    "text": "3. More interactive plots\n\n3.1 Coordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\n\n3.2 Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n\n3.3 Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n3.4 Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\nInteractive: Click on the colour symbol at the legend.\n\n\n3.5 Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n3.6 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\nThings to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\n\nVisit this link to learn more about crosstalk."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "4. Interactive Data Visualisation - crosstalk methods!",
    "text": "4. Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n4.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n4.2 Linked brushing: crosstalk method\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#reference",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "",
    "text": "The following R packages will be used:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a ggplot2 extension spacially desgin for visualising distribution and uncertainty\n\nThe code chunk below will be used load these R packages.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#getting-started",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "",
    "text": "The following R packages will be used:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a ggplot2 extension spacially desgin for visualising distribution and uncertainty\n\nThe code chunk below will be used load these R packages.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "2. Visualising Distribution with Ridgeline Plot",
    "text": "2. Visualising Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n2.1 Plotting ridgeline graph: ggridges method\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nggplot(exam_data, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n2.2 Varying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nVarying fill coloursTransparency\n\n\n\nggplot(exam_data, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"English score\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    fill = \"blue\",      # Choose a constant fill color\n    alpha = 0.5         # Set transparency level (0 = fully transparent, 1 = opaque)\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Mapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n2.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  # Use a brewer scale with pastel colors and custom labels for the quartiles\n  scale_fill_brewer(name = \"Quartile Range\", \n                    palette = \"Pastel1\", \n                    labels = c(\"0–25%\", \"25–50%\", \"50–75%\", \"75–100%\")) +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"white\", color = NA),\n        panel.grid.major = element_line(color = \"grey85\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  # Use more visually appealing muted colors with transparency\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#D55E00A0\",  # Muted orange\n               \"#999999A0\",  # Soft gray\n               \"#0072B2A0\"), # Muted blue\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  # Softer theme with a light background for better contrast\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"white\", color = NA),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.key = element_rect(fill = \"white\", color = NA)\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "3. Visualising Distribution with Raincloud Plot",
    "text": "3. Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n3.1 Plotting a Half Eye graph\nirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n3.2 Adding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\n\n3.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n3.4 Finishing touch\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#reference",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "4. Reference",
    "text": "4. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#overview",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "2. Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "2. Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\n\n2.1 Plot error bars of maths scores by race\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n2.2 Plotting standard error bars of point estimates\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n2.3 Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\n\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\n\n\n2.4 Visualizing the uncertainty of point estimates with interactive error bars\nPlot interactive error bars for the 99% confidence interval of mean maths score by race with the code below.\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "3. Visualising Uncertainty: ggdist package",
    "text": "3. Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n3.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n3.2 Set confidence interval\nIn the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n3.3 Setting multiple confidence intervals\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(\n    .width = c(0.95, 0.99),  # Includes both 95% and 99% CIs\n    .point = median,\n    .interval = qi\n  ) +\n  labs(\n    title = \"Visualising Confidence Intervals of Median Math Score\",\n    subtitle = \"Median Point + 95% and 99% Confidence Intervals\"\n  )\n\n\n\n\n\n\n\n\n\n\n3.4 Colouring the confidence intervals\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "4. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "4. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nLaunch the ungeviz package in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#reference",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will explore ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#overview",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will explore ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "2. Plotting Ternary Diagram with R",
    "text": "2. Plotting Ternary Diagram with R\n\n2.1 Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2018\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n2.2 Plotting an interative ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\",\n  # tooltips: displaying details on the underlying data for each planning area/subzone\n  text = ~paste(\"Total Population:\", TOTAL,\n                 \"&lt;br&gt;Young:\", YOUNG,\n                 \"&lt;br&gt;Active:\", ACTIVE,\n                 \"&lt;br&gt;Old:\", OLD),\n  hoverinfo = \"text\"\n) %&gt;%\n  layout(\n    annotations = label(\"Population Composition\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#reference",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "3. Reference",
    "text": "3. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "",
    "text": "Heatmaps are a powerful tool to visualise multivariate data. They help reveal patterns, clusters, and correlations among variables by mapping numerical values to colors. In this document, we demonstrate how to create both static and interactive heatmaps in R using data from the World Happiness Report (2018). We also provide an extra analysis—a correlation matrix heatmap—to offer additional insights into the relationships between the happiness indicators.\n\n\nWe load the necessary R packages: seriation, dendextend, heatmaply, tidyverse, and RColorBrewer.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, RColorBrewer)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names, and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#overview",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "",
    "text": "Heatmaps are a powerful tool to visualise multivariate data. They help reveal patterns, clusters, and correlations among variables by mapping numerical values to colors. In this document, we demonstrate how to create both static and interactive heatmaps in R using data from the World Happiness Report (2018). We also provide an extra analysis—a correlation matrix heatmap—to offer additional insights into the relationships between the happiness indicators.\n\n\nWe load the necessary R packages: seriation, dendextend, heatmaply, tidyverse, and RColorBrewer.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, RColorBrewer)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names, and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#static-heatmap-with-base-r",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#static-heatmap-with-base-r",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "2. Static Heatmap with Base R",
    "text": "2. Static Heatmap with Base R\nFirst, we create static heatmaps using the base R heatmap() function. We illustrate two examples: one without clustering dendrograms and one with the default clustering.\n\n# Static heatmap without dendrograms\nheatmap(wh_matrix, Rowv = NA, Colv = NA, \n        main = \"Static Heatmap (No Clustering)\")\n\n\n\n\n\n\n\n# Static heatmap with default hierarchical clustering\nheatmap(wh_matrix, \n        main = \"Static Heatmap with Clustering\")\n\n\n\n\n\n\n\n\nTo enhance interpretability—especially when variables have different scales—we scale the matrix by columns.\n\nheatmap(wh_matrix,\n        scale = \"column\",\n        cexRow = 0.6, \n        cexCol = 0.8,\n        margins = c(10, 4),\n        main = \"Column-Scaled Static Heatmap\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#interactive-heatmap-with-heatmaply",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#interactive-heatmap-with-heatmaply",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "3. Interactive Heatmap with Heatmaply",
    "text": "3. Interactive Heatmap with Heatmaply\nThe heatmaply package enables interactive heatmaps. Below are several examples that include data transformation and clustering options.\n\n3.1 Basic Interactive Heatmap\n\nheatmaply(wh_matrix,\n          main = \"Interactive Heatmap of World Happiness Data\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\n3.2 Data Transformation Methods\nTransforming the data can make variables on different scales comparable. Here are examples of scaling (column-wise), normalising, and percentising.\n\nColumn Scaling\n\nheatmaply(wh_matrix,\n          scale = \"column\",\n          main = \"Interactive Heatmap with Column Scaling\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\nNormalisation\n\nheatmaply(normalize(wh_matrix),\n          main = \"Interactive Heatmap with Normalisation\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\nPercentising\n\nheatmaply(percentize(wh_matrix),\n          main = \"Interactive Heatmap with Percentising\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\n\n3.3 Clustering and Optimal Ordering\nWe can further improve the heatmap by clustering the rows (countries) using hierarchical clustering (with the “average” method) and applying an optimal leaf ordering via seriation.\n\nheatmaply(normalize(wh_matrix),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          seriate = \"OLO\",\n          k_row = 3,\n          main = \"Interactive Heatmap with Clustering and Optimal Leaf Ordering\",\n          fontsize_row = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#working-with-colour-palettes",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#working-with-colour-palettes",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "4. Working with Colour Palettes",
    "text": "4. Working with Colour Palettes\nTo enhance the visual appeal, we can change the color palette. In the example below, we use the “Blues” palette from RColorBrewer.\n\nheatmaply(normalize(wh_matrix),\n          seriate = \"none\",\n          colors = RColorBrewer::brewer.pal(9, \"Blues\"),\n          k_row = 5,\n          margins = c(NA, 200, 60, NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main = \"World Happiness Data (Normalised)\",\n          xlab = \"Happiness Indicators\",\n          ylab = \"Countries\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#correlation-matrix-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#correlation-matrix-heatmap",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "5. Correlation Matrix Heatmap",
    "text": "5. Correlation Matrix Heatmap\nTo further explore the relationships among the variables, we calculate a correlation matrix and display it as an interactive heatmap. This visualization highlights which indicators are strongly correlated.\n\n# Compute the correlation matrix\ncorr_matrix &lt;- cor(wh_matrix, use = \"complete.obs\")\n\n# Plot an interactive correlation heatmap using a yellow–orange–red palette\nheatmaply(corr_matrix,\n          colors = RColorBrewer::brewer.pal(9, \"YlOrRd\"),\n          main = \"Correlation Matrix of Happiness Indicators\",\n          xlab = \"Indicators\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#reference",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "6. Reference",
    "text": "6. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "",
    "text": "Treemaps are a visualization technique used to represent hierarchical data through nested rectangles, where the size and color of each rectangle convey different attributes. They are particularly effective for displaying proportions within categories, making it easy to compare parts of a whole at a glance."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#overview",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "",
    "text": "Treemaps are a visualization technique used to represent hierarchical data through nested rectangles, where the size and color of each rectangle convey different attributes. They are particularly effective for displaying proportions within categories, making it easy to compare parts of a whole at a glance."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "1. Installing and Launching R Packages",
    "text": "1. Installing and Launching R Packages\nYou will need the following packages. Here, we use pacman::p_load() for convenience:\n\npacman::p_load(treemap, treemapify, tidyverse, d3treeR)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#data-wrangling",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "2. Data Wrangling",
    "text": "2. Data Wrangling\nFor this exercise, the dataset REALIS2018.csv is used, containing private property transaction records in 2018 from Singapore’s Urban Redevelopment Authority (URA).\n\n2.1 Importing the Dataset\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\n\n2.2 Transforming the Data\nTreemap visualizations often require aggregated information (e.g., by project, region). Here we:\n\nGroup by Project Name, Planning Region, Planning Area, Property Type, and Type of Sale.\nCompute:\n\nTotal Units Sold (sum of No. of Units),\nTotal Area (sum of Area (sqm)),\nMedian Unit Price (median of Unit Price ($ psm)),\nMedian Transacted Price (median of Transacted Price ($)).\n\n\nWe will use two key dplyr verbs: group_by() and summarise().\n\nWithout using the pipe:\n\n# Group the data\nrealis2018_grouped &lt;- group_by(\n  realis2018, \n  `Project Name`,\n  `Planning Region`, \n  `Planning Area`, \n  `Property Type`, \n  `Type of Sale`\n)\n\n# Summarise the grouped data\nrealis2018_summarised &lt;- summarise(\n  realis2018_grouped, \n  `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n  `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n  `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n  `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE)\n)\n\n\n\nWith the pipe:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(\n    `Project Name`,\n    `Planning Region`,\n    `Planning Area`,\n    `Property Type`,\n    `Type of Sale`\n  ) %&gt;%\n  summarise(\n    `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n    `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n    `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n    `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE)\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemap-package",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "3. Designing Treemaps with treemap Package",
    "text": "3. Designing Treemaps with treemap Package\n\n3.1 Subset the Data (Resale Condominium)\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(\n    `Property Type` == \"Condominium\", \n    `Type of Sale` == \"Resale\"\n  )\n\n\n\n3.2 Basic Arguments in treemap()\n\nindex: defines the hierarchical structure (e.g., Region → Area → Project).\nvSize: a numeric column that defines the rectangle sizes.\nvColor: a numeric column that defines the rectangle colors.\ntype: how color values will be interpreted.\n\n\nFirst Attempt\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\nNotice that without specifying type, treemap assumes type = \"index\", which produces unexpected colors. We fix this below.\n\n\n\n\n3.3 Correct Color Mapping with type = \"value\"\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"value\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\n\n3.4 Color Palettes\nWhen type = \"value\", the default color palette is a diverging palette. You can manually choose a palette using the palette argument.\n\nExample: “RdYlBu”\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"value\",\n  palette = \"RdYlBu\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\nBecause our values are all positive, you may not see the full gradient (e.g., no reds if all prices are above zero).\n\n\n\n3.5 “manual” Type\nWith type = \"manual\", the data is linearly mapped onto the palette. A single-hue palette like “Blues” is more intuitive if all values are positive.\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"manual\",\n  palette = \"Blues\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\n\n3.6 Treemap Layout Algorithms\n\n“pivotSize” (default) respects the order of the data but is slightly less balanced visually.\n“squarified” often produces more balanced aspect ratios.\n\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"manual\",\n  palette = \"Blues\",\n  algorithm = \"squarified\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\nUsing sortID\nWhen using \"pivotSize\", you can specify sortID to control the rectangle order:\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"manual\",\n  palette = \"Blues\",\n  algorithm = \"pivotSize\",\n  sortID = \"Median Transacted Price\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemapify",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemapify",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "4. Designing Treemaps with treemapify",
    "text": "4. Designing Treemaps with treemapify\nThe treemapify package uses the ggplot2 framework for treemaps. It provides geom_treemap() and additional layers like geom_treemap_subgroup_border() for hierarchical boundaries.\n\n4.1 Basic Treemap\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`\n  )\n) + \n  geom_treemap() +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  labs(title = \"Basic Treemap with treemapify (2018 Data)\",\n       fill = \"Median Unit Price ($ psm)\")\n\n\n\n\n\n\n\n\n\n\n4.2 Defining a Hierarchy\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    subgroup = `Planning Region`\n  )\n) + \n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"grey20\", size = 2) +\n  labs(\n    title = \"Treemap Grouped by Planning Region\",\n    fill = \"Median Unit Price ($ psm)\"\n  )\n\n\n\n\n\n\n\n\nTo add another hierarchy level (e.g., Planning Area):\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    subgroup = `Planning Region`,\n    subgroup2 = `Planning Area`\n  )\n) + \n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"grey20\") +\n  geom_treemap_subgroup2_border(colour = \"grey40\", size = 2) +\n  labs(\n    title = \"Treemap Grouped by Planning Region & Area\",\n    fill = \"Median Unit Price ($ psm)\"\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#designing-interactive-treemaps-with-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#designing-interactive-treemaps-with-d3treer",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "5. Designing Interactive Treemaps with d3treeR",
    "text": "5. Designing Interactive Treemaps with d3treeR\n\n5.1 Building the Interactive Treemap\nFirst, create a treemap object with treemap:\n\ntm &lt;- treemap(\n  realis2018_summarised,\n  index = c(\"Planning Region\", \"Planning Area\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"value\",\n  title = \"Private Residential Property Sold, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\nThen convert it to an interactive visualization with d3tree():\n\nd3tree(\n  tm,\n  rootname = \"Singapore\"\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#more-exploration",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#more-exploration",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "6. More exploration",
    "text": "6. More exploration\n\n6.1 Adding Labels Inside Treemap (treemapify)\nYou can add text labels to each rectangle using geom_treemap_text():\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    label = `Planning Area`\n  )\n) + \n  geom_treemap() +\n  geom_treemap_text(\n    colour = \"white\",\n    place = \"centre\",\n    grow = TRUE\n  ) +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  labs(\n    title = \"Treemap with Labels (Planning Area)\",\n    fill = \"Median Unit Price ($ psm)\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.2 Filtering for Top N Areas\nSuppose you only want to visualize the top 10 Planning Areas by Total Unit Sold:\n\ntop10_areas &lt;- realis2018_selected %&gt;%\n  group_by(`Planning Area`) %&gt;%\n  summarise(total_unit_sold = sum(`Total Unit Sold`)) %&gt;%\n  slice_max(order_by = total_unit_sold, n = 10) %&gt;%\n  pull(`Planning Area`)\n\ntop10_data &lt;- realis2018_selected %&gt;%\n  filter(`Planning Area` %in% top10_areas)\n\nggplot(\n  data = top10_data,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    label = `Planning Area`\n  )\n) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\", place = \"centre\", grow = TRUE) +\n  labs(title = \"Top 10 Planning Areas by Total Unit Sold (2018)\")\n\n\n\n\n\n\n\n\n\n\n6.3 Comparing Two Property Types Side by Side\nYou can create two treemaps and place them side by side using patchwork or cowplot. For instance, comparing Condominium vs. Executive Condominium:\n\npacman::p_load(patchwork)\n\ncondo_data &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\")\n\nec_data &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Executive Condominium\")\n\nplot_condo &lt;- ggplot(\n  data = condo_data,\n  aes(area = `Total Unit Sold`, fill = `Median Unit Price ($ psm)`)\n) + \n  geom_treemap() +\n  labs(title = \"Condominium\")\n\nplot_ec &lt;- ggplot(\n  data = ec_data,\n  aes(area = `Total Unit Sold`, fill = `Median Unit Price ($ psm)`)\n) + \n  geom_treemap() +\n  labs(title = \"Executive Condominium\")\n\nplot_condo\n\n\n\n\n\n\n\n\n\nplot_ec\n\n\n\n\n\n\n\n\n\n\n6.4 Highlighting Specific Regions (Conditional Coloring)\nYou might want to color certain regions differently if they meet certain criteria, e.g., a Median Unit Price above $20,000 psm:\n\nrealis2018_selected &lt;- realis2018_selected %&gt;%\n  mutate(\n    PriceCategory = if_else(\n      `Median Unit Price ($ psm)` &gt; 20000, \n      \"High Price\", \n      \"Others\"\n    )\n  )\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = PriceCategory\n  )\n) + \n  geom_treemap() +\n  scale_fill_manual(values = c(\"High Price\" = \"red\", \"Others\" = \"grey70\")) +\n  labs(\n    title = \"Conditional Coloring: High vs. Other Price Levels\",\n    fill = \"Price Category\"\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#reference",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "7. Reference",
    "text": "7. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(haven, SmartEDA, tidyverse, tidymodels, ggridges)\n\n\nexam_data &lt;- read_csv(\"./data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(haven, SmartEDA, tidyverse, tidymodels, ggridges)\n\n\nexam_data &lt;- read_csv(\"./data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#box-plot",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#box-plot",
    "title": "In-class Exercise 4",
    "section": "Box plot",
    "text": "Box plot\n\nggplot(data = exam_data,\n       aes(x = ENGLISH,\n           y = CLASS)) + \n  geom_boxplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#ridgeline-and-boxplot",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#ridgeline-and-boxplot",
    "title": "In-class Exercise 4",
    "section": "Ridgeline and Boxplot",
    "text": "Ridgeline and Boxplot\nBoxplots provide essential summary statistics—medians, quartiles, and whiskers—but they are incomplete, they don’t reveal the full distribution of the data.\nIn contrast, ridgeline plots illustrate the underlying density, highlighting features like multiple peaks (as seen in classes 3G and 3H) and outliers (evident in class 3F).\nCombining both provides a more complete view by showing both the summary metrics and the detailed distribution.\n\n# Create the combined plot\nggplot(data = exam_data, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 1.1,        # Adjusts the vertical scaling of the ridgelines\n    alpha = 0.5,    \n    fill = \"lightblue\"\n  ) +\n  geom_boxplot(\n    width = 0.15,\n    position = position_nudge(y = -0.2),  \n    outlier.colour = \"red\",              \n    alpha = 0.7                       \n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Combined Plot of English Scores by Class\",\n    x = \"English Score\",\n    y = \"Class\"\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "",
    "text": "We begin by loading all necessary libraries.\n\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "",
    "text": "We begin by loading all necessary libraries.\n\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-loading-and-preparation",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-loading-and-preparation",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "2. Data Loading and Preparation",
    "text": "2. Data Loading and Preparation\nWe load the dataset from an Excel file and convert selected columns to factors for further analysis.\nConverting columns to factors helps in treating categorical variables appropriately during analysis.\n\n# Load the Toyota Corolla dataset from the 'data' sheet\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\")\n\n# Specify columns to be converted to factors\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\",\n          \"Cylinders\", \"Fuel_Type\", \"Color\", \"Met_Color\", \"Automatic\",\n          \"Mfr_Guarantee\", \"BOVAG_Guarantee\", \"ABS\", \"Airbag_1\",\n          \"Airbag_2\", \"Airco\", \"Automatic_airco\", \"Boardcomputer\",\n          \"CD_Player\", \"Central_Lock\", \"Powered_Windows\",\n          \"Power_Steering\", \"Radio\", \"Mistlamps\", \"Sport_Model\",\n          \"Backseat_Divider\", \"Metallic_Rim\", \"Radio_cassette\",\n          \"Tow_Bar\")\n\n# Convert the 'Id' column to character and specified columns to factors\ncar_resale &lt;- car_resale %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_at(vars(one_of(cols)), as.factor)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-overview",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-overview",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "3. Data Overview",
    "text": "3. Data Overview\n\n3.1 Structural Overview\nWe can inspect the overall structure of the dataset using a summary and glimpse.\n\nSummaryGlimpse\n\n\n\n# Display a summary of the dataset\nsummary(car_resale)\n\n      Id               Model               Price         Age_08_04    \n Length:1436        Length:1436        Min.   : 4350   Min.   : 1.00  \n Class :character   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Mode  :character   Mode  :character   Median : 9900   Median :61.00  \n                                       Mean   :10731   Mean   :55.95  \n                                       3rd Qu.:11950   3rd Qu.:70.00  \n                                       Max.   :32500   Max.   :80.00  \n                                                                      \n   Mfg_Month      Mfg_Year          KM         Quarterly_Tax        Weight    \n 1      :207   Min.   :1998   Min.   :     1   Min.   : 19.00   Min.   :1000  \n 4      :154   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00   1st Qu.:1040  \n 3      :138   Median :1999   Median : 63390   Median : 85.00   Median :1070  \n 2      :134   Mean   :2000   Mean   : 68533   Mean   : 87.12   Mean   :1072  \n 7      :133   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00   3rd Qu.:1085  \n 6      :120   Max.   :2004   Max.   :243000   Max.   :283.00   Max.   :1615  \n (Other):550                                                                  \n Guarantee_Period     HP_Bin      CC_bin    Doors   Gears    Cylinders\n Min.   : 3.000   &lt; 100  :560   &lt;1600:416   2:  2   3:   2   4:1436   \n 1st Qu.: 3.000   &gt; 120  : 11   &gt;1600:166   3:622   4:   1            \n Median : 3.000   100-120:865   1600 :854   4:138   5:1390            \n Mean   : 3.815                             5:674   6:  43            \n 3rd Qu.: 3.000                                                       \n Max.   :36.000                                                       \n                                                                      \n  Fuel_Type        Color     Met_Color Automatic Mfr_Guarantee BOVAG_Guarantee\n CNG   :  17   Grey   :301   0:467     0:1356    0:848         0: 150         \n Diesel: 155   Blue   :283   1:969     1:  80    1:588         1:1286         \n Petrol:1264   Red    :278                                                    \n               Green  :220                                                    \n               Black  :191                                                    \n               Silver :122                                                    \n               (Other): 41                                                    \n ABS      Airbag_1 Airbag_2 Airco   Automatic_airco Boardcomputer CD_Player\n 0: 268   0:  42   0: 398   0:706   0:1355          0:1013        0:1122   \n 1:1168   1:1394   1:1038   1:730   1:  81          1: 423        1: 314   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Central_Lock Powered_Windows Power_Steering Radio    Mistlamps Sport_Model\n 0:603        0:629           0:  32         0:1226   0:1067    0:1005     \n 1:833        1:807           1:1404         1: 210   1: 369    1: 431     \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Backseat_Divider Metallic_Rim Radio_cassette Tow_Bar \n 0: 330           0:1142       0:1227         0:1037  \n 1:1106           1: 294       1: 209         1: 399  \n                                                      \n                                                      \n                                                      \n                                                      \n                                                      \n\n\n\n\n\n# Show a concise structure overview\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;chr&gt; \"81\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"44\", \"…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;fct&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;fct&gt; 100-120, &lt; 100, &lt; 100, &lt; 100, &lt; 100, &lt; 100, &lt; 100, &lt; …\n$ CC_bin           &lt;fct&gt; 1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600…\n$ Doors            &lt;fct&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;fct&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;fct&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;fct&gt; Petrol, Diesel, Diesel, Diesel, Diesel, Diesel, Diese…\n$ Color            &lt;fct&gt; Blue, Blue, Silver, Blue, Black, Black, White, Grey, …\n$ Met_Color        &lt;fct&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;fct&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;fct&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;fct&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;fct&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;fct&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;fct&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;fct&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;fct&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\n\n3.2 List records\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n   Id    Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1 81    TOYOTA … 18950        25 8             2002  20019           100   1180\n 2 1     TOYOTA … 13500        23 10            2002  46986           210   1165\n 3 2     TOYOTA … 13750        23 10            2002  72937           210   1165\n 4 3      TOYOTA… 13950        24 9             2002  41711           210   1165\n 5 4     TOYOTA … 14950        26 7             2002  48000           210   1165\n 6 5     TOYOTA … 13750        30 3             2002  38500           210   1170\n 7 6     TOYOTA … 12950        32 1             2002  61000           210   1170\n 8 7      TOYOTA… 16900        27 6             2002  94612           210   1245\n 9 8     TOYOTA … 18600        30 3             2002  75889           210   1245\n10 44    TOYOTA … 16950        27 6             2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;fct&gt;, CC_bin &lt;fct&gt;,\n#   Doors &lt;fct&gt;, Gears &lt;fct&gt;, Cylinders &lt;fct&gt;, Fuel_Type &lt;fct&gt;, Color &lt;fct&gt;,\n#   Met_Color &lt;fct&gt;, Automatic &lt;fct&gt;, Mfr_Guarantee &lt;fct&gt;,\n#   BOVAG_Guarantee &lt;fct&gt;, ABS &lt;fct&gt;, Airbag_1 &lt;fct&gt;, Airbag_2 &lt;fct&gt;,\n#   Airco &lt;fct&gt;, Automatic_airco &lt;fct&gt;, Boardcomputer &lt;fct&gt;, CD_Player &lt;fct&gt;,\n#   Central_Lock &lt;fct&gt;, Powered_Windows &lt;fct&gt;, Power_Steering &lt;fct&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-summaries",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-summaries",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "4. Data Summaries",
    "text": "4. Data Summaries\n\nType 1Type 2\n\n\nType 1 provides an overall data summary.\n\nsummary1 &lt;- car_resale %&gt;%\n  ExpData(type = 1)\n\n# Display the summary (further customization possible)\nsummary1\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables         7\n4                              No. of factor variables        29\n5                                No. of text variables         2\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\nType 2 provides a variable level summary.\n\nsummary2 &lt;- car_resale %&gt;%\n  ExpData(type = 2)\n\n# Display the detailed summary\nsummary2\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id     character     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month        factor     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin        factor     1436             0              0\n12    12           CC_bin        factor     1436             0              0\n13    13            Doors        factor     1436             0              0\n14    14            Gears        factor     1436             0              0\n15    15        Cylinders        factor     1436             0              0\n16    16        Fuel_Type        factor     1436             0              0\n17    17            Color        factor     1436             0              0\n18    18        Met_Color        factor     1436             0              0\n19    19        Automatic        factor     1436             0              0\n20    20    Mfr_Guarantee        factor     1436             0              0\n21    21  BOVAG_Guarantee        factor     1436             0              0\n22    22              ABS        factor     1436             0              0\n23    23         Airbag_1        factor     1436             0              0\n24    24         Airbag_2        factor     1436             0              0\n25    25            Airco        factor     1436             0              0\n26    26  Automatic_airco        factor     1436             0              0\n27    27    Boardcomputer        factor     1436             0              0\n28    28        CD_Player        factor     1436             0              0\n29    29     Central_Lock        factor     1436             0              0\n30    30  Powered_Windows        factor     1436             0              0\n31    31   Power_Steering        factor     1436             0              0\n32    32            Radio        factor     1436             0              0\n33    33        Mistlamps        factor     1436             0              0\n34    34      Sport_Model        factor     1436             0              0\n35    35 Backseat_Divider        factor     1436             0              0\n36    36     Metallic_Rim        factor     1436             0              0\n37    37   Radio_cassette        factor     1436             0              0\n38    38          Tow_Bar        factor     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualizations",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualizations",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "5. Visualizations",
    "text": "5. Visualizations\n\n5.1 Numerical Data Visualizations\nWe create visualizations for numerical variables.\nTwo versions are provided:\n\nWithout specifying a target variable.\nFocusing on the “Price” variable.\n\n\nWithout Target Variable\n\ncar_resale %&gt;%\n  ExpNumViz(target = NULL,\n            nlim = 10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Price Distribution\n\ncar_resale %&gt;%\n  ExpNumViz(target = \"Price\",\n            nlim = 10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 Categorical Data Visualizations\nThe following plot displays bar charts for categorical variables to visualize the distribution of categories.\n\ncar_resale %&gt;%\n  ExpCatViz(target = NULL,\n            col = \"sky blue\",\n            clim = 10,\n            margin = 2,\n            Page = c(4,4),\n            sample = 16)\n\n$`0`"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#checking-multicollinearity",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#checking-multicollinearity",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "6. Checking Multicollinearity",
    "text": "6. Checking Multicollinearity\nIn this section, we assess multicollinearity using two approaches: the correlation matrix and the Variance Inflation Factor (VIF). We start by fitting a regression model, then examine multicollinearity diagnostics using the check_collinearity() function, which provides both correlation and VIF information.\n\n# Fit an initial model including potential multicollinear predictors\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, data = car_resale)\n\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n# Plot the multicollinearity diagnostics for a visual overview\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\nSince the variable “Mfg_Year” shows multicollinearity issues, we remove it and fit an updated model.\n\n# Fit an updated model excluding manufacturing year\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + Weight + Guarantee_Period, data = car_resale)\n\n\n# Check the normality of residuals and heteroscedasticity for the updated model\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\ncheck_heteroscedasticity(model1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nWe can also generate a comprehensive set of diagnostic plots for our fitted model using the check_model() function.\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\nApproaches to Assess Multicollinearity:\n\nCorrelation Matrix: Examines pairwise correlations between predictors to identify highly correlated variables.\nVariance Inflation Factor (VIF): Quantifies how much the variance of a regression coefficient is inflated due to multicollinearity.\n\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#model-summary",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#model-summary",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "7. Model Summary",
    "text": "7. Model Summary\n\n7.1 gtsummary\nFinally, we can summarize the updated model using gtsummary.\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n7.2 tbl_regression\nWe can also generate regression tables with tbl_regression\n\nBasic regression table\n\ntbl_regression(model1, \n               intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-2,186\n-4,093, -278\n0.025\n\n\nAge_08_04\n-119\n-125, -114\n&lt;0.001\n\n\nKM\n-0.02\n-0.03, -0.02\n&lt;0.001\n\n\nWeight\n20\n18, 21\n&lt;0.001\n\n\nGuarantee_Period\n27\n2.1, 52\n0.034\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nRegression table with additional statistics\n\ntbl_regression(model1, \n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    # \"\\U03C3\" to extract the sigma value\n    label = list(sigma ~ \"\\U03C3\"),  # can ignore if you do not want the sigma\n    include = c(r.squared, adj.r.squared,\n                AIC, statistic,\n                p.value, sigma)\n  )\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\nThe packages provide the following functionality: - tidyverse: Collection of packages for data manipulation and visualization - tsibble: Provides a data infrastructure for tidy temporal data - feasts: Feature Extraction And Statistics for Time Series analysis - fable: Forecasting models including exponential smoothing and ARIMA - seasonal: Seasonal decomposition of time series\n\n\n\n\nts_data &lt;- read_csv(\"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(ts_data$`Month-Year`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\nThe packages provide the following functionality: - tidyverse: Collection of packages for data manipulation and visualization - tsibble: Provides a data infrastructure for tidy temporal data - feasts: Feature Extraction And Statistics for Time Series analysis - fable: Forecasting models including exponential smoothing and ARIMA - seasonal: Seasonal decomposition of time series\n\n\n\n\nts_data &lt;- read_csv(\"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(ts_data$`Month-Year`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-data-formats-for-time-series-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-data-formats-for-time-series-analysis",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "2. Converting Data Formats for Time Series Analysis",
    "text": "2. Converting Data Formats for Time Series Analysis\n\n2.1 Base ts object versus tibble object\n\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\n\n\nBase ts objects differ from tibble dataframes in several ways. A ts object is specifically designed for time series analysis and has class types “mts”, “ts”, “matrix”, and “array”, whereas a tibble dataframe has class types “spec_tbl_df”, “tbl_df”, “tbl”, and “data.frame”.\n\n\n2.2 Converting to tsibble format\nThe tsibble format allows us to work with time series data in a tidy framework, making it compatible with both dplyr/tidyr functions and time series analysis functions.\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = Month)\n\nThis creates a tbl_ts object which can be used in both dplyr/tidyr operations and time series analysis."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-transformation-for-visualization",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-transformation-for-visualization",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "3. Data Transformation for Visualization",
    "text": "3. Data Transformation for Visualization\nTo visualize time series data effectively, we need to transform the data from wide to long format.\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualizing-time-series-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualizing-time-series-data",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "4. Visualizing Time Series Data",
    "text": "4. Visualizing Time Series Data\n\n4.1 Single Time Series Visualization\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\n\n4.2 Multiple Time Series Visualization\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n4.3 Using Facets for Better Comparison\nUsing facet_wrap allows us to compare multiple time series more effectively by giving each country its own panel.\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt’s important to note that intervals are not constant across these visualizations, as pointed out by the professor."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-tsibble-for-further-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-tsibble-for-further-analysis",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "5. Converting tsibble for Further Analysis",
    "text": "5. Converting tsibble for Further Analysis\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#analyzing-time-series-patterns",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#analyzing-time-series-patterns",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "6. Analyzing Time Series Patterns",
    "text": "6. Analyzing Time Series Patterns\n\n6.1 Comparing Country-Specific Patterns\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\nThis visualization shows distinct patterns for different countries: - Italy shows a strong seasonal pattern with a large spike in August and relatively consistent lower values during the rest of the year - Vietnam shows higher arrivals in June and July with a peak in July, relatively stable numbers from September to December, and a gradual increase from January to May\n\n\n6.2 Seasonal Subseries Analysis\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)\n\n\n\n\n\n\n\n\nThe subseries plot helps visualize seasonal patterns by month across years, making it easier to identify consistent seasonal behaviors."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#autocorrelation-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#autocorrelation-analysis",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "7. Autocorrelation Analysis",
    "text": "7. Autocorrelation Analysis\n\n7.1 ACF Plots\nAutocorrelation Function (ACF) plots show how time series data is correlated with its lagged values.\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\" |\n         Country == \"United Kingdom\" |\n         Country == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nThe ACF plots reveal important patterns: - China shows a 6-month periodicity - Italy shows a 12-month peak with weak correlation at first month (~0.3) - For statistically significant correlation, values should exceed the blue line (95% confidence level) - For China and Vietnam, most/all lags show statistical significance - For China and Vietnam, correlation decreases then increases again, but with different periodicity (Vietnam: 12 months, China: 6 months) - UK shows significant lag at t-1, then non-significant values, then significance again at 12 months, indicating weak trend but strong annual seasonality - Both UK and Italy show less pronounced seasonal patterns compared to Vietnam and China\n\n\n7.2 PACF Plots\nPartial Autocorrelation Function (PACF) plots show the direct correlation between observations at different lags after removing the effects of intermediate lags.\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\" |\n         Country == \"United Kingdom\" |\n         Country == \"China\") %&gt;%\n  PACF(Arrivals) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nPACF plots help identify what might happen with further decomposition: - For the UK, the first lag is positive while the second lag is negative, suggesting a turning point where arrival patterns change - Statistical significance is indicated when values extend beyond the blue confidence interval - These visualizations help identify both statistically significant patterns and potential turning points in the data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#time-series-decomposition",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#time-series-decomposition",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "8. Time Series Decomposition",
    "text": "8. Time Series Decomposition\nDecomposing time series helps separate the data into trend, seasonal, and remainder components.\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nThe decomposition shows: - The trend component shows the long-term movement - The seasonal component shows regular patterns - The remainder shows what’s left after removing trend and seasonality\nIf the remainder shows no clear pattern, it’s considered “white noise” - indicating a good decomposition. When patterns remain in the remainder, it suggests the decomposition is incomplete, which may indicate that more advanced methods like machine learning might be needed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#forecasting",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#forecasting",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "9. Forecasting",
    "text": "9. Forecasting\n\n9.1 Creating Training and Hold-out Sets\nFor time series forecasting, we can’t use random sampling for validation. Instead, we keep the most recent data as a hold-out set.\n\nvietnam_ts &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;% \n  mutate(Type = if_else(\n    `Month-Year` &gt;= \"2019-01-01\", \n    \"Hold-out\", \"Training\"))\n\n\nvietnam_train &lt;- vietnam_ts %&gt;%\n  filter(`Month-Year` &lt; \"2019-01-01\")\n\n\n\n9.2 Decomposing the Training Data\nIt’s important to analyze training data before forecasting to understand its components.\n\nvietnam_train %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n9.3 Fitting Forecast Models\nWhen fitting a good forecast model, the residuals should follow a normal distribution, indicating that the model has captured the systematic patterns in the data.\nHere we create an automatic ETS (Error, Trend, Seasonal) model:\n\nfit_autoETS &lt;- vietnam_train %&gt;%\n  model(ETS(Arrivals))\n\n\n\n9.4 Visualizing the Forecast Results\n\nfc_autoETS &lt;- fit_autoETS %&gt;%\n  forecast(h = \"12 months\")\n\nvietnam_ts %&gt;%\n  ggplot(aes(x = Month, \n             y = Arrivals)) +\n  autolayer(fc_autoETS, \n            alpha = 0.6) +\n  geom_line(aes(\n    color = Type), \n    alpha = 0.8) + \n  geom_line(aes(\n    y = .mean, \n    colour = \"Forecast\"), \n    data = fc_autoETS) +\n  geom_line(aes(\n    y = .fitted, \n    colour = \"Fitted\"), \n    data = augment(fit_autoETS))\n\n\n\n\n\n\n\n\nWhen visualizing forecast results, it’s most useful to focus on the last few cycles rather than the entire history, as we want to assess the accuracy of predictions compared to the hold-out data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#additional-insights",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#additional-insights",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "10. Additional Insights",
    "text": "10. Additional Insights\n\n10.1 Comparing Seasonal Patterns Across Countries\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\" |\n         Country == \"Malaysia\" |\n         Country == \"Germany\") %&gt;% \n  gg_season(Arrivals) +\n  labs(title = \"Seasonal Patterns by Country\",\n       y = \"Visitor Arrivals\")\n\n\n\n\n\n\n\n\nThis visualization shows how visitor arrivals vary by month across different countries, highlighting distinct seasonal tourism patterns.\n\n\n10.2 Exploring Trend vs Seasonality Strength\n\ncountry_features &lt;- tsibble_longer %&gt;%\n  features(Arrivals, feat_stl)\n\nggplot(country_features, aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point() +\n  geom_text(aes(label = Country), check_overlap = TRUE, hjust = -0.1, vjust = -0.1) +\n  labs(x = \"Trend Strength\", y = \"Seasonal Strength\", \n       title = \"Trend vs Seasonal Strength by Country\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis scatter plot helps identify which countries have strong seasonal patterns versus consistent growth trends, providing insight for targeted tourism strategies.\n\n\n10.3 ARIMA Model Fitting and Comparison\n\nfit_arima &lt;- vietnam_train %&gt;%\n  model(\n    arima_auto = ARIMA(Arrivals),\n    ets_auto = ETS(Arrivals)\n  )\n\nfit_arima %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(vietnam_ts, level = NULL) +\n  labs(title = \"ARIMA vs ETS Model Comparison\",\n       y = \"Visitor Arrivals\")\n\n\n\n\n\n\n\n\nComparing different forecasting methods helps identify which approach works best for specific time series patterns.\n\n\n10.4 Detecting Anomalies in Visitor Arrivals\n\naugment(fit_autoETS) %&gt;%\n  mutate(\n    anomaly = abs(.resid) &gt; 2*sd(.resid, na.rm = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = Month, y = Arrivals)) +\n  geom_line() +\n  geom_point(aes(color = anomaly), size = 1) +\n  scale_color_manual(values = c(\"FALSE\" = NA, \"TRUE\" = \"red\")) +\n  labs(title = \"Anomaly Detection in Vietnam Visitor Arrivals\",\n       color = \"Anomaly\")\n\n\n\n\n\n\n\n\nThis visualization helps identify unusual spikes or drops in visitor arrivals that might warrant further investigation.\n\n\n10.5 Visualizing Forecast Uncertainty\n\nfc_autoETS %&gt;%\n  autoplot(vietnam_ts) +\n  labs(title = \"Forecast with Prediction Intervals\",\n       y = \"Visitor Arrivals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnderstanding forecast uncertainty is crucial for planning, as it shows the range of likely outcomes rather than just point estimates."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "",
    "text": "This dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n\nIn this exercise, we will:\n\nExamine Heart Attack Occurrence: Explore how heart attacks are distributed across the dataset and identify any relevant predictors.\nConduct Demographic Analysis: Investigate the role of age, gender, and region, especially comparing younger vs. older cohorts.\nExplore Health Metrics: Visualize relationships among BMI, blood pressure, cholesterol, and heart attack status.\nAssess Lifestyle Factors: Evaluate how smoking, physical activity, diet quality, alcohol consumption, and stress influence heart health."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#overview",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "",
    "text": "This dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n\nIn this exercise, we will:\n\nExamine Heart Attack Occurrence: Explore how heart attacks are distributed across the dataset and identify any relevant predictors.\nConduct Demographic Analysis: Investigate the role of age, gender, and region, especially comparing younger vs. older cohorts.\nExplore Health Metrics: Visualize relationships among BMI, blood pressure, cholesterol, and heart attack status.\nAssess Lifestyle Factors: Evaluate how smoking, physical activity, diet quality, alcohol consumption, and stress influence heart health."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#getting-started",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nWe load the following R packages using the pacman::p_load() function:\n\ntidyverse: Core collection of R packages for data wrangling and visualization (e.g., dplyr, ggplot2)\n\nSmartEDA: For the ExpData() function used in exploratory data analysis\n\neasystats: Specifically for check_collinearity() to diagnose multicollinearity issues\n\nreshape2: Provides the melt() function for reshaping data from wide to long format\n\ncaret: Functions for data partitioning (createDataPartition) and model training workflows\n\nyardstick: Offers conf_mat() and other classification metrics\n\npROC: For ROC curves and AUC calculations (roc, auc)\n\nGGally: For the ggpairs() function to create pairwise scatterplot matrices\n\nggmosaic: To create mosaic plots via geom_mosaic()\n\npatchwork: For arranging multiple ggplot figures into a composite layout\n\nxgboost: Gradient boosting library for classification and regression tasks\n\n\npacman::p_load(\n  tidyverse,\n  SmartEDA,\n  easystats,\n  reshape2,\n  caret,\n  yardstick,\n  pROC,\n  GGally,\n  ggmosaic,\n  patchwork,\n  xgboost\n)\n\nThis dataset contains information about heart attack occurrences in Japan, focusing on various demographic and health-related factors.\n\n\nImport data\n\nheart_data &lt;- read_csv(\"./data/japan_heart_attack_dataset.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#data-pre-processing",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#data-pre-processing",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Data pre-processing",
    "text": "Data pre-processing\n\nGlimpse of data\nUsing the glimpse() function, we see that the dataset consists of 30,000 rows and 32 columns. The output displays the column names, their data types, and the first few entries for each variable. Additionally, there are 15 extra columns (Extra_Column_1 to Extra_Column_15) which are not clearly defined.\n\nglimpse(heart_data)\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\nThe following provides an overview of the Japan Heart Attack dataset using the ExpData() function, summarizing both overall and variable-level details.\n\nOverall data summaryVariable level summary\n\n\n\nsummary1 &lt;- heart_data %&gt;%\n  ExpData(type = 1)\n\n# Display the summary (further customization possible)\nsummary1\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)     30000\n2                              No. of variables (ncol)        32\n3                    No. of numeric/interger variables        22\n4                              No. of factor variables         0\n5                                No. of text variables        10\n6                             No. of logical variables         0\n7                          No. of identifier variables        20\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         0\n10               %. of variables having complete cases 100% (32)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\n\nsummary2 &lt;- heart_data %&gt;%\n  ExpData(type = 2)\n\n# Display the summary (further customization possible)\nsummary2\n\n   Index           Variable_Name Variable_Type Sample_n Missing_Count\n1      1                     Age       numeric    30000             0\n2      2                  Gender     character    30000             0\n3      3                  Region     character    30000             0\n4      4         Smoking_History     character    30000             0\n5      5        Diabetes_History     character    30000             0\n6      6    Hypertension_History     character    30000             0\n7      7       Cholesterol_Level       numeric    30000             0\n8      8       Physical_Activity     character    30000             0\n9      9            Diet_Quality     character    30000             0\n10    10     Alcohol_Consumption     character    30000             0\n11    11           Stress_Levels       numeric    30000             0\n12    12                     BMI       numeric    30000             0\n13    13              Heart_Rate       numeric    30000             0\n14    14             Systolic_BP       numeric    30000             0\n15    15            Diastolic_BP       numeric    30000             0\n16    16          Family_History     character    30000             0\n17    17 Heart_Attack_Occurrence     character    30000             0\n18    18          Extra_Column_1       numeric    30000             0\n19    19          Extra_Column_2       numeric    30000             0\n20    20          Extra_Column_3       numeric    30000             0\n21    21          Extra_Column_4       numeric    30000             0\n22    22          Extra_Column_5       numeric    30000             0\n23    23          Extra_Column_6       numeric    30000             0\n24    24          Extra_Column_7       numeric    30000             0\n25    25          Extra_Column_8       numeric    30000             0\n26    26          Extra_Column_9       numeric    30000             0\n27    27         Extra_Column_10       numeric    30000             0\n28    28         Extra_Column_11       numeric    30000             0\n29    29         Extra_Column_12       numeric    30000             0\n30    30         Extra_Column_13       numeric    30000             0\n31    31         Extra_Column_14       numeric    30000             0\n32    32         Extra_Column_15       numeric    30000             0\n   Per_of_Missing No_of_distinct_values\n1               0                    62\n2               0                     2\n3               0                     2\n4               0                     2\n5               0                     2\n6               0                     2\n7               0                 30000\n8               0                     3\n9               0                     3\n10              0                     4\n11              0                 29613\n12              0                 30000\n13              0                 30000\n14              0                 30000\n15              0                 30000\n16              0                     2\n17              0                     2\n18              0                 30000\n19              0                 30000\n20              0                 30000\n21              0                 30000\n22              0                 30000\n23              0                 30000\n24              0                 30000\n25              0                 30000\n26              0                 30000\n27              0                 30000\n28              0                 30000\n29              0                 30000\n30              0                 30000\n31              0                 30000\n32              0                 30000\n\n\n\n\n\n\n\nConvert categorical variables to factors\nFrom the overview above, we see that the dataset contains no missing values, and the categorical variables have a maximum of 4 unique values. Converting these variables into factors ensures they are correctly treated as categorical data during analysis and visualization.\n\n# Convert selected categorical variables into factors\nheart_data &lt;- heart_data %&gt;%\n  mutate(\n    Gender = as.factor(Gender),\n    Region = as.factor(Region),\n    Smoking_History = as.factor(Smoking_History),\n    Diabetes_History = as.factor(Diabetes_History),\n    Hypertension_History = as.factor(Hypertension_History),\n    Physical_Activity = as.factor(Physical_Activity),\n    Diet_Quality = as.factor(Diet_Quality),\n    Alcohol_Consumption = as.factor(Alcohol_Consumption),\n    Family_History = as.factor(Family_History),\n    Heart_Attack_Occurrence = as.factor(Heart_Attack_Occurrence)\n  )\n\n\n\nDrop extra columns\n\n# Select only the Extra_Columns and the outcome variable\nextra_data &lt;- heart_data %&gt;%\n  select(starts_with(\"Extra_Column_\"), Heart_Attack_Occurrence)\n\n# Reshape to long format\nextra_data_long &lt;- melt(extra_data, id.vars = \"Heart_Attack_Occurrence\")\n\n# Create boxplots comparing each Extra_Column by Heart_Attack_Occurrence\nggplot(extra_data_long, aes(x = Heart_Attack_Occurrence, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = \"free\") +\n  labs(\n    title = \"Distribution of Extra Columns by Heart Attack Occurrence\",\n    x = \"Heart Attack Occurrence\",\n    y = \"Value\"\n  )\n\n\n\n\n\n\n\n\nSince these variables do not appear to vary by heart attack status, they are unlikely to provide useful information for any downstream analysis (e.g., modeling, hypothesis testing). Dropping them will simplify the dataset and help focus on variables that do relate to heart attack risk.\nWe can drop them with the following code:\n\nheart_data &lt;- heart_data %&gt;%\n  select(-starts_with(\"Extra_Column_\"))\n\n\n\nCleaned dataset\n\nglimpse(heart_data)\n\nRows: 30,000\nColumns: 17\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;fct&gt; Male, Male, Male, Female, Female, Female, Male…\n$ Region                  &lt;fct&gt; Urban, Urban, Rural, Urban, Rural, Rural, Urba…\n$ Smoking_History         &lt;fct&gt; Yes, No, Yes, No, No, No, No, Yes, No, No, No,…\n$ Diabetes_History        &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, No, N…\n$ Hypertension_History    &lt;fct&gt; No, No, No, No, No, No, Yes, No, Yes, No, Yes,…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;fct&gt; Moderate, Low, Low, Moderate, High, Low, High,…\n$ Diet_Quality            &lt;fct&gt; Poor, Good, Average, Good, Good, Good, Poor, P…\n$ Alcohol_Consumption     &lt;fct&gt; Low, Low, Moderate, High, High, High, High, No…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, Yes,…\n$ Heart_Attack_Occurrence &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, No, N…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#exploratory-visuals",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#exploratory-visuals",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Exploratory visuals",
    "text": "Exploratory visuals\n\nCreate new variables\nWe create a new variable, Age_Group, classifying individuals as “Over50” or “50OrBelow” to compare younger vs. older individuals.\n\nheart_data_eda &lt;- heart_data %&gt;%\n  mutate(Age_Group = ifelse(Age &gt; 50, \"Over50\", \"50OrBelow\") %&gt;% as.factor())\n\nWe create AgeGender by combining the Age_Group and gender. We also combine smoking status and physical activity into SmokeAct and reorder alcohol consumption levels.\n\n# Demographic variables\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    AgeGender = case_when(\n      Age_Group == \"Over50\" & Gender == \"Male\"   ~ \"Over 50 Male\",\n      Age_Group == \"Over50\" & Gender == \"Female\" ~ \"Over 50 Female\",\n      Age_Group == \"50OrBelow\" & Gender == \"Male\"   ~ \"≤50 Male\",\n      Age_Group == \"50OrBelow\" & Gender == \"Female\" ~ \"≤50 Female\"\n    ) %&gt;% factor(levels = c(\"≤50 Female\",\"≤50 Male\",\"Over 50 Female\",\"Over 50 Male\"))\n  )\n\n# Lifestyle variables\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    SmokeAct = case_when(\n      Smoking_History == \"Yes\" & Physical_Activity == \"Low\"      ~ \"Smoker, PA:Low\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"Moderate\" ~ \"Smoker, PA:Mod\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"High\"     ~ \"Smoker, PA:High\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Low\"      ~ \"Non-Smoker, PA:Low\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Moderate\" ~ \"Non-Smoker, PA:Mod\",\n      Smoking_History == \"No\"  & Physical_Activity == \"High\"     ~ \"Non-Smoker, PA:High\"\n    ) %&gt;% \n    # Order them in a sensible sequence:\n    factor(levels = c(\"Non-Smoker, PA:Low\",\"Non-Smoker, PA:Mod\",\"Non-Smoker, PA:High\",\n                      \"Smoker, PA:Low\",\"Smoker, PA:Mod\",\"Smoker, PA:High\"))\n  )\n\n\n\nMosaic Plot: Demographic Analysis\nWe plot a mosaic where AgeGender is on the x-axis, color indicates heart attack occurrence, and each facet represents a different region.\n\np_demo &lt;- ggplot(heart_data_eda) +\n  geom_mosaic(\n    aes(x = product(AgeGender),\n        fill = Heart_Attack_Occurrence,\n        text = paste0(\"Group: \", AgeGender,\n                      \"&lt;br&gt;Region: \", Region,\n                      \"&lt;br&gt;Heart Attack: \", Heart_Attack_Occurrence)\n    ),\n    alpha = 0.9\n  ) +\n  facet_wrap(~ Region) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Demographic Mosaic: Age & Gender by Region vs. Heart Attack\",\n    x     = \"Age & Gender\",\n    y     = \" \",\n    fill  = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\np_demo\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis mosaic plot illustrates heart attack occurrences across different age and gender groups within rural and urban regions. The width of each bar segment corresponds to the relative size of that demographic group, while the height indicates the proportion of individuals who experienced a heart attack.\nOverall, heart attack rates remain relatively consistent between rural and urban areas. However, males tend to have a higher probability of heart attack than females, regardless of age or region.\n\n\n\nMosaic plot: Lifestyle factors\nWe create a mosaic plot with SmokeAct on the x-axis, color by heart attack occurrence, and facet by the four alcohol consumption levels.\n\n# Reorder factor levels for Alcohol_Consumption\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    Alcohol_Consumption = factor(\n      Alcohol_Consumption,\n      levels = c(\"High\", \"Moderate\", \"Low\", \"None\")\n    )\n  )\n\nggplot(heart_data_eda) +\n  geom_mosaic(aes(\n    x    = product(SmokeAct),\n    fill = Heart_Attack_Occurrence\n  ), alpha = 0.9) +\n  facet_wrap(~ Alcohol_Consumption, ncol = 2) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Lifestyle Mosaic: Smoking, Activity, and Alcohol vs. Heart Attack\",\n    subtitle = \"PA = Physical Activity. Each facet represents a different Alcohol Consumption level.\",\n    x = \"Smoking & PA Group\",\n    y = \"\",\n    fill = \"Heart Attack\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 14, hjust=0.5),\n    plot.subtitle = element_text(size = 10, hjust=0.5),\n    strip.text    = element_text(face=\"bold\"),\n    axis.text.x   = element_text(angle=40, hjust=1, size=7),\n    panel.spacing = unit(2, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis mosaic plot explores how smoking, physical activity (PA), and alcohol consumption interact to influence heart attack occurrences. Each facet represents a different alcohol consumption level (High, Moderate, Low, None).\nInterestingly, non-smokers who report no alcohol consumption but high physical activity exhibit one of the highest heart attack rates. Additionally, smokers with moderate physical activity tend to have higher heart attack rates compared to smokers with low or high physical activity.\n\n\n\nPairwise numeric plot (Health metrics)\nThis code uses ggpairs() to create a matrix of pairwise plots for all numeric variables in heart_data. The mapping = aes(color = Heart_Attack_Occurrence) argument adds a color-coded grouping by heart attack status.\n\n# Automatically select all numeric columns from the dataset\nnumeric_cols &lt;- sapply(heart_data, is.numeric)\n\npairwise_plot &lt;- ggpairs(\n  data = heart_data,\n  columns = which(numeric_cols),\n  mapping = aes(color = Heart_Attack_Occurrence),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, size = 0.5)),\n  diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5)),\n  upper = list(continuous = wrap(\"cor\", size = 4))\n) +\n  ggtitle(\"Pairwise Correlations Among All Numeric Metrics\")\n\npairwise_plot\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis grid compares health metrics like BMI, blood pressure, cholesterol, and stress. The diagonal panels show density curves for each variable, revealing, for instance, that Age has a broader distribution compared to the other variables.\nThe upper panels list correlation coefficients and their significance, most of which are near zero (e.g., Corr: 0.025, 0.048), indicating that these variables do not strongly co-vary. In the lower scatter plots, points are colored by heart attack occurrence; no tight clustering suggests no single numeric threshold exclusively separates “Yes” vs. “No.” For instance, Systolic_BP and Diastolic_BP show little correlation as high Systolic_BP often coexists with both high and low Diastolic_BP. Overall, no single numeric factor stands out as a strictly linear driver of heart attack, though there may be subtle nonlinear or interactive effects to explore later."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#train-test-split",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#train-test-split",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Train test split",
    "text": "Train test split\nBefore building a predictive model, it is best practice to split the data into training and testing sets. The createDataPartition function ensures that the distribution of the target class is approximately the same in both sets. Here, we allocate 80% of the data for training and 20% for testing.\n\nset.seed(123)\n\ntrain_index &lt;- createDataPartition(heart_data$Heart_Attack_Occurrence, p = 0.8, list = FALSE)\n\ntrain_data &lt;- heart_data[train_index, ]\ntest_data  &lt;- heart_data[-train_index, ]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#naive-logistic-regression",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#naive-logistic-regression",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Naive logistic regression",
    "text": "Naive logistic regression\nHere, we build an initial (“naive”) logistic regression model that includes all available predictors (except the 15 “Extra_Column” variables we dropped). This approach gives us a baseline.\n\nFit the model\nWe will fit a logistic regression using glm().\n\n# Use a standard glm with all predictors\nnaive_glm &lt;- glm(\n  Heart_Attack_Occurrence ~ .,\n  data   = train_data,\n  family = binomial\n)\n\n\n\nUnderstanding the model\nWe use check_collinearity() to see if any variables are highly correlated or cause near‐complete separation. A “good” logistic regression typically avoids extremely high VIFs or indefinite confidence intervals.\n\n# Capture the output\nresult &lt;- check_collinearity(naive_glm)\n\n# Coerce to a data frame\ndf &lt;- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n\n\nCheck for Multicollinearity\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nAge\n1.001325\n1.000000\n1.991615e+01\n1.000662\n0.9986771\n0.0502105\n0.9999999\n\n\nGender\n1.000943\n1.000000\n6.378053e+02\n1.000472\n0.9990575\n0.0015679\n1.0000000\n\n\nRegion\n1.000670\n1.000000\n1.062715e+05\n1.000335\n0.9993302\n0.0000094\n1.0000000\n\n\nSmoking_History\n1.000446\n1.000000\n9.589080e+08\n1.000223\n0.9995547\n0.0000000\n1.0000000\n\n\nDiabetes_History\n1.001228\n1.000000\n3.808479e+01\n1.000614\n0.9987733\n0.0262572\n1.0000000\n\n\nHypertension_History\n1.000834\n1.000000\n3.250209e+03\n1.000417\n0.9991664\n0.0003077\n1.0000000\n\n\nCholesterol_Level\n1.000587\n1.000000\n1.351909e+06\n1.000293\n0.9994134\n0.0000007\n1.0000000\n\n\nPhysical_Activity\n1.001852\n1.000002\n2.745436e+00\n1.000926\n0.9981510\n0.3642408\n0.9999980\n\n\nDiet_Quality\n1.001445\n1.000000\n1.030279e+01\n1.000722\n0.9985567\n0.0970611\n0.9999998\n\n\nAlcohol_Consumption\n1.002116\n1.000005\n1.852209e+00\n1.001057\n0.9978885\n0.5398958\n0.9999947\n\n\nStress_Levels\n1.000919\n1.000000\n8.893873e+02\n1.000459\n0.9990821\n0.0011244\n1.0000000\n\n\nBMI\n1.000849\n1.000000\n2.561245e+03\n1.000424\n0.9991522\n0.0003904\n1.0000000\n\n\nHeart_Rate\n1.000834\n1.000000\n3.252876e+03\n1.000417\n0.9991665\n0.0003074\n1.0000000\n\n\nSystolic_BP\n1.000939\n1.000000\n6.753992e+02\n1.000469\n0.9990618\n0.0014806\n1.0000000\n\n\nDiastolic_BP\n1.000920\n1.000000\n8.726101e+02\n1.000460\n0.9990807\n0.0011460\n1.0000000\n\n\nFamily_History\n1.000826\n1.000000\n3.716482e+03\n1.000413\n0.9991743\n0.0002691\n1.0000000\n\n\n\n\n\n\n\n\nInterpreting the collinearity results\n\nVIF ~1.0 but extremely large upper confidence bounds: This indicates the algorithm is unsure about the exact magnitude of possible collinearity. In simpler terms, the model’s variance–covariance matrix is nearly singular.\nThis often happens when:\n\nQuasi‐complete separation: Certain variables or combinations nearly “perfectly” predict the outcome.\nImbalance in the dataset (many more “No” than “Yes”) plus insufficient signal in some predictors.\nOver‐parametrization: Too many correlated predictors for the sample size.\n\n\n\n\n\nModel performance\n\n# 1) Collinearity plot\ncheck_c &lt;- check_collinearity(naive_glm)\np_collinearity &lt;- plot(check_c) +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\n# 2) Confusion matrix heatmap\npred_prob_naive &lt;- predict(naive_glm, newdata = test_data, type = \"response\")\n\npred_class_naive &lt;- ifelse(pred_prob_naive &gt;= 0.5, \"Yes\", \"No\") %&gt;%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\n\n# Evaluate\nnaive_results &lt;- data.frame(\n  obs   = test_data$Heart_Attack_Occurrence,\n  pred  = pred_class_naive,\n  prob  = pred_prob_naive\n)\n\nnaive_cm &lt;- naive_results %&gt;%\n  conf_mat(obs, pred)\n\np_confmat &lt;- autoplot(naive_cm, type = \"heatmap\") +\n  labs(title = \"Naive Logistic Regression: Confusion Matrix\")\n\n# 3) ROC curve as a ggplot object using ggroc()\nroc_naive &lt;- roc(\n  response  = as.numeric(naive_results$obs),\n  predictor = as.numeric(naive_results$prob)\n)\n\np_roc &lt;- ggroc(roc_naive, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Naïve Logistic Model\") +\n  theme_minimal()\n\n\ncombined_plot &lt;- p_collinearity / (p_confmat + p_roc)\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\nExplanation of the plot\n\nCollinearity Plot:\n\n\n\nThis bar chart displays VIF estimates, where the ideal values fall in the green region.\nIndividual VIF point estimates hover around 1.0, but their upper confidence intervals extend into the red, indicating extremely high values.\nThis suggests that the model’s parameter estimates are unstable due to quasi‐complete separation or an excess of correlated predictors relative to the sample size. In essence, the model cannot reliably discern each variable’s true contribution, leading to artificially low VIF point estimates paired with massive uncertainty bounds.\n\n\nConfusion Matrix:\n\n\n\nThe matrix shows 0 true positives.\nThis is typical when a logistic model either encounters near-complete separation or opts to disregard the minority class in imbalanced datasets.\n\n\nROC Curve:\n\n\n\nThe ROC curve lies near the diagonal reference line, confirming that the model lacks predictive power and is essentially guessing."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#improving-logistic-regression",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#improving-logistic-regression",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Improving logistic regression",
    "text": "Improving logistic regression\n\nRationale\nOur naïve logistic regression suggested potential issues: - Very low sensitivity (predicting all “No”) - Large variance inflation factor (VIF) intervals\nTherefore, we refine the logistic model by:\n\nUse weighted logistic regression to handle class imbalance,\n\nIncorporate mild non-linear terms for BMI, Systolic_BP, and Diastolic_BP with polynomial expansions,\n\nRemove redundant variables.\n\n\nnew_train &lt;- train_data\nnew_test  &lt;- test_data\n\n\nnew_formula &lt;- as.formula(\n  \"Heart_Attack_Occurrence ~ Age + Gender + Family_History + poly(BMI, 2, raw=TRUE) + \n   Heart_Rate + poly(Systolic_BP, 2, raw=TRUE) + poly(Diastolic_BP, 2, raw=TRUE) + Cholesterol_Level + \n   Diabetes_History + Hypertension_History + Physical_Activity + Smoking_History + \n   Diet_Quality + Alcohol_Consumption + Stress_Levels\"\n)\n\n\nCreating observation weights\nWe create balanced weights to give more importance to the minority class. This ensures misclassifying a minority‐class “Yes” is penalized more strongly than misclassifying a “No.”\n\nn_yes &lt;- sum(new_train$Heart_Attack_Occurrence == \"Yes\")\nn_no  &lt;- sum(new_train$Heart_Attack_Occurrence == \"No\")\nN     &lt;- n_yes + n_no\n\nw_yes &lt;- N / (2 * n_yes)\nw_no  &lt;- N / (2 * n_no)\n\n# Assign weights in the new training dataset\nnew_train$weights_col &lt;- ifelse(\n  new_train$Heart_Attack_Occurrence == \"Yes\",\n  w_yes,\n  w_no\n)\n\n\n\n\nFit weighted logistic model\n\nmodel_glm_weighted &lt;- glm(\n  formula = new_formula,\n  data    = new_train,\n  family  = binomial(link = \"logit\"),\n  weights = weights_col\n)\n\n\n# Capture the output\nresult &lt;- check_collinearity(model_glm_weighted)\n\n# Coerce to a data frame\ndf &lt;- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n\n\nCheck for Multicollinearity\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nAge\n1.004556\n1.000277\n1.074857\n1.002275\n0.9954646\n0.9303566\n0.9997228\n\n\nGender\n1.003347\n1.000075\n1.149809\n1.001672\n0.9966644\n0.8697096\n0.9999252\n\n\nFamily_History\n1.001847\n1.000002\n2.774072\n1.000923\n0.9981564\n0.3604809\n0.9999981\n\n\npoly(BMI, 2, raw = TRUE)\n1.003982\n1.000162\n1.097600\n1.001989\n0.9960340\n0.9110785\n0.9998376\n\n\nHeart_Rate\n1.002266\n1.000008\n1.614256\n1.001132\n0.9977391\n0.6194804\n0.9999916\n\n\npoly(Systolic_BP, 2, raw = TRUE)\n1.005107\n1.000419\n1.062202\n1.002550\n0.9949186\n0.9414402\n0.9995808\n\n\npoly(Diastolic_BP, 2, raw = TRUE)\n1.004244\n1.000211\n1.085491\n1.002120\n0.9957736\n0.9212419\n0.9997893\n\n\nCholesterol_Level\n1.001808\n1.000002\n3.013365\n1.000904\n0.9981953\n0.3318549\n0.9999984\n\n\nDiabetes_History\n1.002857\n1.000033\n1.244352\n1.001427\n0.9971512\n0.8036315\n0.9999666\n\n\nHypertension_History\n1.002289\n1.000009\n1.586196\n1.001144\n0.9977158\n0.6304391\n0.9999911\n\n\nPhysical_Activity\n1.004614\n1.000291\n1.073216\n1.002304\n0.9954073\n0.9317786\n0.9997093\n\n\nSmoking_History\n1.002518\n1.000016\n1.390398\n1.001258\n0.9974880\n0.7192187\n0.9999838\n\n\nDiet_Quality\n1.003581\n1.000102\n1.125179\n1.001789\n0.9964314\n0.8887471\n0.9998975\n\n\nAlcohol_Consumption\n1.007005\n1.001124\n1.043643\n1.003496\n0.9930440\n0.9581821\n0.9988770\n\n\nStress_Levels\n1.002948\n1.000039\n1.219987\n1.001473\n0.9970609\n0.8196808\n0.9999605\n\n\n\n\n\n\n\n\n\nVisualizing the model\n\npred_prob_improved &lt;- predict(model_glm_weighted, newdata = test_data, type = \"response\")\n\n\nggplot(mapping = aes(x = pred_prob_improved)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Distribution of Predicted Probabilities (Naïve Logistic)\",\n       x = \"Predicted Probability of Heart Attack\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nFrom the histogram above, we can see that the model is overpredicting positive cases at 0.50 threshold. As the positive cases is approximately 10% of the dataset, we will take a threshold of 0.55 instead.\n\ncheck_c &lt;- check_collinearity(model_glm_weighted)\np_collinearity &lt;- plot(check_c) +\n  labs(title = \"Collinearity of Weighted Logistic Model\") +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\npred_prob_improved &lt;- predict(model_glm_weighted, newdata = test_data, type = \"response\")\npred_class_improved &lt;- ifelse(pred_prob_improved &gt;= 0.55, \"Yes\", \"No\") %&gt;%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\nimproved_cm &lt;- data.frame(\n  obs  = test_data$Heart_Attack_Occurrence,\n  pred = pred_class_improved\n) %&gt;% conf_mat(obs, pred)\n\np_confmat &lt;- autoplot(improved_cm, type = \"heatmap\") +\n  labs(title = \"Weighted Logistic: Confusion Matrix\")\n\nroc_improved &lt;- roc(\n  response  = as.numeric(test_data$Heart_Attack_Occurrence),\n  predictor = as.numeric(pred_prob_improved)\n)\np_roc &lt;- ggroc(roc_improved, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Weighted Logistic Model\") +\n  theme_minimal()\n\n## Combine the three plots\nlibrary(patchwork)\ncombined_plot &lt;- p_collinearity / (p_confmat + p_roc)\ncombined_plot\n\n\n\n\n\n\n\n\nWeighted Logistic Regression Results\nAfter applying class weights and mild polynomial terms to BMI and Blood Pressure, our weighted logistic model shows some improvements compared to the naïve model:\n\nNo longer predicting all “No”\n\nThe confusion matrix now reflects at least some “Yes” predictions: it is not trivially predicting “No” for every observation. This is a direct benefit of giving more weight to “Yes” in the training procedure, which pushes the model to separate out at least a portion of positive cases.\n\nCollinearity appears more stable\n\nThe VIF plot still shows point estimates around 1.0 for most variables, with moderate spikes in confidence intervals for a few. However, these are less extreme than in the naïve model, suggesting the parameter estimates are more stable overall.\n\nLittle change to overall accuracy (AUC)\n\nWhile the model does somewhat better at identifying positives, the ROC curve remains fairly close to the diagonal, reflecting an AUC only slightly better than 0.5.\nIn other words, the model is still not very accurate overall, indicating that the available predictors may not strongly discriminate between “Yes” and “No”—or that we need further refinements (e.g., more complex interactions, alternative transformations, or additional data)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#fitting-a-xgboost",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#fitting-a-xgboost",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Fitting a xgboost",
    "text": "Fitting a xgboost\n\nData preparation\nWe first convert the outcome Heart_Attack_Occurrence to a 0/1 numeric variable. Then, we build model matrices using model.matrix() which transforms both categorical and numeric predictors into a suitable format for XGBoost.\n\n# Convert outcome to 0/1\ntrain_data_xgb &lt;- train_data %&gt;%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\nx_train &lt;- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=train_data_xgb)\ny_train &lt;- train_data_xgb$YesNo\n\ntest_data_xgb &lt;- test_data %&gt;%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\nx_test &lt;- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=test_data_xgb)\ny_test &lt;- test_data_xgb$YesNo\n\nn_yes &lt;- sum(y_train == 1)\nn_no  &lt;- sum(y_train == 0)\nscale_pos &lt;- n_no / n_yes\n\n\n\nXGBoost model training\nWe create DMatrix objects for both training and test sets, then specify key hyperparameters like max_depth, eta, and scale_pos_weight. The model is trained with early stopping if the test AUC does not improve after a certain number of rounds.\n\ndtrain &lt;- xgb.DMatrix(data = x_train, label = y_train)\ndtest  &lt;- xgb.DMatrix(data = x_test,  label = y_test)\n\nparam &lt;- list(\n  objective        = \"binary:logistic\",\n  eval_metric      = \"auc\",             # can also track \"error\" or \"logloss\"\n  max_depth        = 10,\n  eta              = 0.2,\n  scale_pos_weight = scale_pos          # imbalance correction\n)\n\n# Train with 100 rounds\nset.seed(123)\nxgb_model &lt;- xgb.train(\n  params   = param,\n  data     = dtrain,\n  nrounds  = 1000,\n  watchlist= list(train=dtrain, test=dtest),\n  early_stopping_rounds = 50,  # optional, for early stop\n  print_every_n          = 10\n)\n\n[1] train-auc:0.703267  test-auc:0.493556 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 50 rounds.\n\n[11]    train-auc:0.893593  test-auc:0.519498 \n[21]    train-auc:0.955144  test-auc:0.513634 \n[31]    train-auc:0.977267  test-auc:0.519925 \n[41]    train-auc:0.985509  test-auc:0.514492 \n[51]    train-auc:0.993414  test-auc:0.505698 \n[61]    train-auc:0.996438  test-auc:0.503955 \n[71]    train-auc:0.998506  test-auc:0.502356 \nStopping. Best iteration:\n[28]    train-auc:0.972693  test-auc:0.525233\n\n\nThe training AUC climbs to ~0.999 or even 1.0, while the test set AUC remains near 0.52, suggesting no robust pattern is learnable from these features. This might indicate insufficient predictive signal in the data.\n\n\nModel evaluation\n\npred_prob_xgb &lt;- predict(xgb_model, newdata = dtest, ntreelimit = xgb_model$best_iteration)\n\n[23:42:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\npred_class_xgb &lt;- ifelse(pred_prob_xgb &gt;= 0.5, 1, 0)\n\nresults_xgb &lt;- data.frame(\n  obs  = factor(y_test, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  pred = factor(pred_class_xgb, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  prob = pred_prob_xgb\n)\n\nxgb_cm &lt;- conf_mat(results_xgb, truth=obs, estimate=pred)\nxgb_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.763 \n 2 kap                  binary        0.0124\n 3 sens                 binary        0.826 \n 4 spec                 binary        0.191 \n 5 ppv                  binary        0.903 \n 6 npv                  binary        0.107 \n 7 mcc                  binary        0.0131\n 8 j_index              binary        0.0167\n 9 bal_accuracy         binary        0.508 \n10 detection_prevalence binary        0.824 \n11 precision            binary        0.903 \n12 recall               binary        0.826 \n13 f_meas               binary        0.863 \n\n\n\n\nROC curve and confusion matrix\n\np_confmat &lt;- autoplot(xgb_cm, type=\"heatmap\") +\n  labs(title=\"XGBoost Confusion Matrix\", fill=\"Count\")\n\nroc_xgb &lt;- roc(response = as.numeric(results_xgb$obs), predictor = results_xgb$prob)\nauc_val &lt;- auc(roc_xgb)\n\np_roc &lt;- ggroc(roc_xgb, colour=\"#1c61b6\") +\n  labs(\n    title = paste0(\"XGBoost ROC Curve (AUC=\", round(auc_val,3), \")\"),\n    x     = \"1 - Specificity\",\n    y     = \"Sensitivity\"\n  ) +\n  theme_minimal()\n\n\n\ncombined_plot &lt;- p_confmat | p_roc\n\ncombined_plot\n\n\n\n\n\n\n\n\n\nPlot explanation:\n\nConfusion matrix: Shows how many “No” vs. “Yes” cases are classified correctly vs. incorrectly. Despite the class‐imbalance correction, the model still misclassifies most “Yes” events.\nROC curve: The curve hovers close to the diagonal, with an AUC near ~0.52. This is only marginally better than random guessing (AUC=0.5).\n\n\nimportance_xgb &lt;- xgb.importance(model = xgb_model)\nimportance_xgb  # see a data frame of feature importances\n\n                        Feature        Gain       Cover   Frequency\n                         &lt;char&gt;       &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n 1:           Cholesterol_Level 0.145242292 0.155522476 0.136697155\n 2:                 Systolic_BP 0.136727355 0.167277235 0.132375113\n 3:               Stress_Levels 0.134981157 0.185245402 0.134184340\n 4:                Diastolic_BP 0.134287572 0.160294404 0.126846919\n 5:                         BMI 0.131566444 0.126584570 0.123932053\n 6:                  Heart_Rate 0.123786314 0.121115391 0.121921801\n 7:                         Age 0.083851778 0.050432098 0.092371093\n 8:           Family_HistoryYes 0.010253500 0.002744379 0.011056388\n 9:   Physical_ActivityModerate 0.009907179 0.004210412 0.011659463\n10:                 RegionUrban 0.009613210 0.001768051 0.012262539\n11:          Smoking_HistoryYes 0.009402740 0.003311462 0.011357925\n12:     Hypertension_HistoryYes 0.009347006 0.001676643 0.010754850\n13:                  GenderMale 0.009129164 0.005765353 0.011357925\n14:         Diabetes_HistoryYes 0.008797618 0.001136887 0.009850236\n15:      Alcohol_ConsumptionLow 0.008275298 0.001408138 0.010553825\n16:            Diet_QualityGood 0.007770305 0.001091494 0.009146648\n17: Alcohol_ConsumptionModerate 0.007592931 0.001353723 0.009850236\n18:            Diet_QualityPoor 0.007177992 0.003669527 0.007940497\n19:     Alcohol_ConsumptionNone 0.006439186 0.004718671 0.007638959\n20:        Physical_ActivityLow 0.005850960 0.000673683 0.008242034\n                        Feature        Gain       Cover   Frequency\n\n# Plot\nxgb.plot.importance(importance_xgb, top_n = 15, \n                    main=\"XGBoost Feature Importance\")\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe plot ranks features by how frequently and effectively they split the data.\nEven though certain variables like Cholesterol_Level or Systolic_BP appear at the top, the final predictive performance on the test set remains poor, indicating limited actual signal or potential overfitting to training noise."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#final-thoughts",
    "href": "Take-home_Ex/Take-home_Ex1-draft - Copy.html#final-thoughts",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Final thoughts",
    "text": "Final thoughts\nAcross multiple approaches (naive logistic, weighted logistic, XGBoost), the models struggle to achieve predictive accuracy on the test set. This may indicate that the available features (after dropping the undefined extras) do not strongly distinguish between heart attack occurrences. Additional data, refined feature engineering, or domain expertise might be necessary to improve predictive performance. Nonetheless, the exploratory visualizations provide insights into demographic and lifestyle patterns associated with heart attack risk in Japan."
  }
]