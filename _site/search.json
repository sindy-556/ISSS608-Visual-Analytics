[
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html",
    "href": "Take-home_Ex/Take-home_Ex3.html",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "",
    "text": "This document presents a comprehensive visualization prototype for GeBiz procurement data. It features two complementary analytical approaches: Sankey diagrams to visualize procurement flows from ministries to agencies to suppliers, and RFM (Recency, Frequency, Monetary) analysis to segment and evaluate supplier relationships. Together, these visualizations provide powerful insights into government procurement patterns and supplier management.\n\n# Load required packages\npacman::p_load(dplyr, stringr, lubridate, networkD3, tidyverse, \n               plotly, DT, viridis, scales, htmlwidgets, htmltools,\n               RColorBrewer, treemapify)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#overview",
    "href": "Take-home_Ex/Take-home_Ex3.html#overview",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "",
    "text": "This document presents a comprehensive visualization prototype for GeBiz procurement data. It features two complementary analytical approaches: Sankey diagrams to visualize procurement flows from ministries to agencies to suppliers, and RFM (Recency, Frequency, Monetary) analysis to segment and evaluate supplier relationships. Together, these visualizations provide powerful insights into government procurement patterns and supplier management.\n\n# Load required packages\npacman::p_load(dplyr, stringr, lubridate, networkD3, tidyverse, \n               plotly, DT, viridis, scales, htmlwidgets, htmltools,\n               RColorBrewer, treemapify)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex3.html#data-preparation",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "2. Data Preparation",
    "text": "2. Data Preparation\n\n2.1 Loading and Enriching Data\nBefore creating our visualizations, we need to load and prepare the relevant datasets. We’re working with with the GeBiz dataset and 2 other dataset that sourced online:\n\nProcurement transaction data - the core dataset with details of each procurement activity from GeBiz\nSupplier financial information - containing grading data for suppliers (note: this dataset is incomplete, as we were unable to obtain financial grades for all suppliers)\n\n3, Agency mapping data - providing the hierarchical relationships between agencies and ministries\nThe code below loads these datasets from CSV files into R data frames. We’ll need to handle the incomplete nature of the supplier financial data during our analysis.\n\n# Load the necessary datasets\nprocurement_df &lt;- read.csv('data/GovernmentProcurementviaGeBIZ.csv')  # Procurement data CSV\nfinancial_df &lt;- read.csv('data/supplier_details.csv')  # CSV with supplier financial grades\nagency_mapping_df &lt;- read.csv('data/agency_mapping.csv')  # Agency mapping data\n\n\n\n2.2 Extracting Supplier Grades\nThe financial_df dataset contains supplier grade information, which indicates the financial capacity of each supplier. The Singapore government classifies suppliers on a scale from S2 to S10, with each grade corresponding to specific monetary thresholds:\n\nS2: $100,000 (EPU S2)\nS3: $250,000 (EPU S3)\nS4: $500,000 (EPU S4)\nS5: $1,000,000 (EPU S5)\nS6: $3,000,000 (EPU S6)\nS7: $5,000,000 (EPU S7)\nS8: $10,000,000 (EPU S8)\nS9: $30,000,000 (EPU S9)\nS10: &gt;$30,000,000 (EPU S10)\n\nHigher grades (S8-S10) represent larger companies with greater financial capacity, while lower grades (S2-S4) represent smaller contractors.\n\n# Function to extract just the S-grade (S2-S10) from the financial grade string\nextract_s_grade &lt;- function(grade_string) {\n  if (is.na(grade_string)) {\n    return(NA)\n  }\n  \n  # Use regex to extract S followed by a number\n  match &lt;- str_extract(grade_string, \"S\\\\d+\")\n  return(match)\n}\n\n# Create simplified financial grades\nfinancial_df$financial_grade &lt;- sapply(financial_df$financial_grade, extract_s_grade)\n\n# Create mapping dataframes for lookups\nsupplier_grade_map &lt;- financial_df %&gt;% \n  select(supplier_name, financial_grade) %&gt;%\n  distinct()\n\nSince we were unable to obtain financial grades for all suppliers in our procurement dataset, we’ll need to handle missing values appropriately in our analysis. We’ll need to account for the incomplete coverage in our visualizations and interpretations.\n\n\n2.3 Joining Data and Creating an Enriched Dataset\nTo create comprehensive visualizations, we need to combine all our datasets into one enriched procurement dataset. This includes:\n\nAdding supplier financial grades to the procurement records\nAdding ministry information to the agencies\nCreating standardized abbreviations for agencies and ministries for cleaner visualization\n\nThe code below performs these join operations and creates our main analysis dataset.\n\nprocurement_enriched &lt;- procurement_df %&gt;%\n  left_join(supplier_grade_map, by = \"supplier_name\")\n\nagency_mapping &lt;- agency_mapping_df %&gt;%\n  select(agency, agency_abbr, ministry, ministry_abbr)\n\nprocurement_enriched &lt;- procurement_enriched %&gt;%\n  left_join(agency_mapping, by = \"agency\")\n\nwrite.csv(procurement_enriched, 'procurement_output.csv', row.names = FALSE)\n\n\n\n2.4 Verifying Data Enrichment\nAfter joining the datasets, it’s important to verify that the enrichment was successful and understand the extent of missing data. The code below reports statistics on the completeness of our enriched dataset, including:\n\nTotal number of procurement records\nRecords with financial grades (expected to be incomplete due to data limitations)\nRecords with agency abbreviations\nRecords with ministry information\n\nThis verification step is particularly important given that we’re working with external datasets of varying completeness. Understanding these limitations will help us design appropriate visualizations and interpret results accurately.\n\ncat(\"Total procurement records:\", nrow(procurement_enriched), \"\\n\")\n\nTotal procurement records: 18638 \n\ncat(\"Records with financial grades:\", sum(!is.na(procurement_enriched$financial_grade)), \"\\n\")\n\nRecords with financial grades: 11994 \n\ncat(\"Records with agency abbreviations:\", sum(!is.na(procurement_enriched$agency_abbr)), \"\\n\")\n\nRecords with agency abbreviations: 18638 \n\ncat(\"Records with ministry information:\", sum(!is.na(procurement_enriched$ministry)), \"\\n\")\n\nRecords with ministry information: 18638 \n\n\n\nglimpse(procurement_enriched)\n\nRows: 18,638\nColumns: 11\n$ tender_no            &lt;chr&gt; \"ACR000ETT18300010\", \"ACR000ETT18300011\", \"ACR000…\n$ tender_description   &lt;chr&gt; \"SUPPLY, DESIGN, DEVELOPMENT, CUSTOMIZATION, DELI…\n$ agency               &lt;chr&gt; \"Accounting And Corporate Regulatory Authority\", …\n$ award_date           &lt;chr&gt; \"11/6/2019\", \"10/5/2019\", \"30/4/2019\", \"29/8/2019…\n$ tender_detail_status &lt;chr&gt; \"Awarded to Suppliers\", \"Awarded to No Suppliers\"…\n$ supplier_name        &lt;chr&gt; \"AZAAS PTE. LTD.\", \"Unknown\", \"ACCENTURE SG SERVI…\n$ awarded_amt          &lt;dbl&gt; 2305880.0, 0.0, 2035000.0, 30700373.9, 178800.0, …\n$ financial_grade      &lt;chr&gt; \"S9\", NA, \"S10\", \"S10\", NA, \"S10\", \"S10\", \"S10\", …\n$ agency_abbr          &lt;chr&gt; \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"…\n$ ministry             &lt;chr&gt; \"MINISTRY OF FINANCE\", \"MINISTRY OF FINANCE\", \"MI…\n$ ministry_abbr        &lt;chr&gt; \"MOF\", \"MOF\", \"MOF\", \"MOF\", \"MOF\", \"MOF\", \"MOF\", …\n\n\n\n\n2.5 Exploratory Data Analysis\n\nBasic summary statistics for key numeric variables.\n\nsummary(procurement_df$awarded_amt)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.000e+00 7.000e+03 1.647e+05 5.540e+06 8.227e+05 1.493e+09 \n\n\n\n\nCount of records by year to understand the temporal distribution\n\nyearly_counts &lt;- procurement_df %&gt;%\n  mutate(year = year(as.Date(award_date, format=\"%d/%m/%Y\"))) %&gt;%\n  group_by(year) %&gt;%\n  summarise(count = n(),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            avg_value = mean(awarded_amt, na.rm = TRUE),\n            median_value = median(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(year)\n\nprint(yearly_counts)\n\n# A tibble: 6 × 5\n   year count  total_value avg_value median_value\n  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1  2019  3085 16059483098.  5205667.      188446 \n2  2020  3475 16464137733.  4737881.      139185.\n3  2021  3968 22112113338.  5572609.      154595 \n4  2022  3464 21410618127.  6180894.      168960 \n5  2023  3712 20563061381.  5539618.      156255 \n6  2024   934  6641402748.  7110710.      243375 \n\n\n\n\nExamine the distribution of procurement by ministry and agency\n\nministry_distribution &lt;- procurement_enriched %&gt;%\n  group_by(ministry) %&gt;%\n  summarise(count = n(),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            avg_value = mean(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value))\n\nhead(ministry_distribution, 10)\n\n# A tibble: 10 × 4\n   ministry                                        count  total_value avg_value\n   &lt;chr&gt;                                           &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 MINISTRY OF TRANSPORT                            1163 35664601010. 30666037.\n 2 MINISTRY OF NATIONAL DEVELOPMENT                 2033 25698029271. 12640447.\n 3 MINISTRY OF SUSTAINABILITY AND THE ENVIRONMENT   1712 12576675306.  7346189.\n 4 MINISTRY OF TRADE AND INDUSTRY                   2628  6357920701   2419300.\n 5 MINISTRY OF EDUCATION                            3011  5411058603.  1797097.\n 6 MINISTRY OF HOME AFFAIRS                          824  5016127066.  6087533.\n 7 MINISTRY OF HEALTH                               1142  3593118020.  3146338.\n 8 MINISTRY OF CULTURE, COMMUNITY AND YOUTH         1587  2410346023.  1518807.\n 9 MINISTRY OF FINANCE                               848  1305103449.  1539037.\n10 MINISTRY OF DIGITAL DEVELOPMENT AND INFORMATION  1296  1158374359.   893807.\n\n\n\n\nTop agencies by procurement value\n\nagency_distribution &lt;- procurement_enriched %&gt;%\n  group_by(agency) %&gt;%\n  summarise(count = n(),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            avg_value = mean(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value))\n\nhead(agency_distribution, 10)\n\n# A tibble: 10 × 4\n   agency                                            count total_value avg_value\n   &lt;chr&gt;                                             &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 Land Transport Authority                            753     3.47e10 46117155.\n 2 Housing and Development Board                      1218     2.44e10 20000790.\n 3 Public Utilities Board                              938     6.92e 9  7372103.\n 4 National Environment Agency                         508     5.30e 9 10430255.\n 5 Jurong Town Corporation                             589     3.91e 9  6632663.\n 6 Ministry of Home Affairs - Ministry Headquarter 1   465     3.55e 9  7631497.\n 7 Ministry of Education                               647     3.49e 9  5389535.\n 8 Ministry of Health-Ministry Headquarter             381     2.88e 9  7564464.\n 9 Ministry of Home Affairs-Ministry Headquarter       207     1.32e 9  6376422.\n10 People's Association                                884     1.01e 9  1137689.\n\n\n\n\nTop suppliers by procurement value\n\nsupplier_distribution &lt;- procurement_enriched %&gt;%\n  group_by(supplier_name) %&gt;%\n  summarise(count = n(),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            avg_value = mean(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value))\n\nhead(supplier_distribution, 10)\n\n# A tibble: 10 × 4\n   supplier_name                                     count total_value avg_value\n   &lt;chr&gt;                                             &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 PENTA-OCEAN CONSTRUCTION COMPANY LIMITED              5 2703160550     5.41e8\n 2 WOH HUP (PRIVATE) LIMITED                             3 2241896200     7.47e8\n 3 CHINA COMMUNICATIONS CONSTRUCTION COMPANY LTD.        4 1836067980.    4.59e8\n 4 RICH CONSTRUCTION COMPANY PTE. LTD.                   6 1733331271     2.89e8\n 5 SHANGHAI TUNNEL ENGINEERING CO (SINGAPORE) PTE L…     4 1682948000     4.21e8\n 6 KEPPEL SEGHERS ENGINEERING SINGAPORE PTE. LTD.        1 1493179167     1.49e9\n 7 KAY LIM CONSTRUCTION & TRADING PTE LTD                9 1394038000     1.55e8\n 8 CES ENGINEERING & CONSTRUCTION PTE. LTD.              4 1223080000     3.06e8\n 9 CES_SDC PTE. LTD.                                     3 1222110222     4.07e8\n10 NEWCON BUILDERS PTE. LTD.                             7 1217160000     1.74e8\n\n\n\n\nDistribution of suppliers by financial grade\n\ngrade_distribution &lt;- procurement_enriched %&gt;%\n  group_by(financial_grade) %&gt;%\n  summarise(count = n(),\n            unique_suppliers = n_distinct(supplier_name),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            pct_value = round(total_value/sum(procurement_enriched$awarded_amt, na.rm = TRUE)*100, 2),\n            avg_contract_value = mean(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value))\n\nprint(grade_distribution)\n\n# A tibble: 10 × 6\n   financial_grade count unique_suppliers  total_value pct_value\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             6644             3178 71313233781.     69.1 \n 2 S10              4414              569 25177217610.     24.4 \n 3 S8               2919              751  2442910142.      2.37\n 4 S9               1068              245  2413997429.      2.34\n 5 S7               2318              860  1535306836.      1.49\n 6 S6                618              264   216864117.      0.21\n 7 S5                368              191    96560958.      0.09\n 8 S4                212              124    38664019.      0.04\n 9 S3                 48               34    10734680.      0.01\n10 S2                 29               14     5326854.      0.01\n# ℹ 1 more variable: avg_contract_value &lt;dbl&gt;\n\n\n\n\nDistribution by consolidated financial category\n\nif(!\"financial_category\" %in% colnames(procurement_enriched)) {\n  procurement_enriched &lt;- procurement_enriched %&gt;%\n    mutate(financial_category = case_when(\n      financial_grade %in% c(\"S1\", \"S2\", \"S3\", \"S4\") ~ \"Small (S1-S4)\",\n      financial_grade %in% c(\"S5\", \"S6\", \"S7\") ~ \"Medium (S5-S7)\",\n      financial_grade %in% c(\"S8\", \"S9\", \"S10\") ~ \"Large (S8-S10)\",\n      TRUE ~ \"Unspecified Grade\"\n    ))\n}\ncategory_distribution &lt;- procurement_enriched %&gt;%\n  group_by(financial_category) %&gt;%\n  summarise(count = n(),\n            unique_suppliers = n_distinct(supplier_name),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            pct_value = round(total_value/sum(procurement_enriched$awarded_amt, na.rm = TRUE)*100, 2),\n            avg_contract_value = mean(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value))\n\nprint(category_distribution)\n\n# A tibble: 4 × 6\n  financial_category count unique_suppliers  total_value pct_value\n  &lt;chr&gt;              &lt;int&gt;            &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Unspecified Grade   6644             3178 71313233781.     69.1 \n2 Large (S8-S10)      8401             1565 30034125180.     29.1 \n3 Medium (S5-S7)      3304             1315  1848731910.      1.79\n4 Small (S1-S4)        289              172    54725553.      0.05\n# ℹ 1 more variable: avg_contract_value &lt;dbl&gt;\n\n\n\n\nAre contracts awarded consistently with financial capacity?\nThis checks if higher-grade suppliers tend to get higher-value contracts.\n\ngrade_contract_match &lt;- procurement_enriched %&gt;%\n  filter(!is.na(financial_grade)) %&gt;%\n  group_by(financial_grade) %&gt;%\n  summarise(\n    median_contract_value = median(awarded_amt, na.rm = TRUE),\n    avg_contract_value = mean(awarded_amt, na.rm = TRUE),\n    max_contract_value = max(awarded_amt, na.rm = TRUE),\n    total_awarded = sum(awarded_amt, na.rm = TRUE),\n    contract_count = n()\n  ) %&gt;%\n  arrange(financial_grade)\n\nprint(grade_contract_match)\n\n# A tibble: 9 × 6\n  financial_grade median_contract_value avg_contract_value max_contract_value\n  &lt;chr&gt;                           &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n1 S10                           423925            5703946.        1025102939 \n2 S2                             40000             183685.           3204100 \n3 S3                             53175             223639.           1785714.\n4 S4                             43324.            182377.           4467000 \n5 S5                             71979.            262394.           5588000 \n6 S6                            112100             350913.           7242693.\n7 S7                            133288.            662341.          36968832 \n8 S8                            140480.            836900.          95742051.\n9 S9                            338275            2260297.          80784392 \n# ℹ 2 more variables: total_awarded &lt;dbl&gt;, contract_count &lt;int&gt;\n\n\n\n\n\n2.6 Temporal Patterns\n\nExamine monthly patterns in procurement\n\nif(!inherits(procurement_enriched$award_date, \"Date\")) {\n  procurement_enriched &lt;- procurement_enriched %&gt;%\n    mutate(award_date = as.Date(award_date, format=\"%d/%m/%Y\"))\n}\nmonthly_patterns &lt;- procurement_enriched %&gt;%\n  mutate(month = month(award_date, label = TRUE),\n         year = year(award_date)) %&gt;%\n  # Filter to include only the relevant years (2019-2023)\n  filter(year &gt;= 2019 & year &lt;= 2023) %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(count = n(),\n            total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n\n\nPlot monthly patterns across years with improved styling\n\nyear_colors &lt;- c(\"2019\" = \"#1f77b4\", \"2020\" = \"#ff7f0e\", \n                \"2021\" = \"#2ca02c\", \"2022\" = \"#d62728\", \n                \"2023\" = \"#9467bd\")\n\nggplot(monthly_patterns, aes(x = month, y = total_value, group = year, color = factor(year))) +\n  geom_line(size = 1, alpha = 0.3) + \n  geom_point(size = 2.5, alpha = 0.7) +  \n  labs(title = \"Monthly Procurement Patterns (2019-2023)\",\n       subtitle = \"Comparing monthly procurement values across years\",\n       x = \"Month\",\n       y = \"Total Procurement Value (SGD)\",\n       color = \"Year\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", size = 16),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_color_manual(values = year_colors)\n\n\n\n\n\n\n\n\n\n\n\n2.7 Distribution of Award Amounts\nCreate a histogram of award amounts to see the distribution\n\nggplot(procurement_enriched, aes(x = awarded_amt)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  scale_x_log10() +  # Log scale for better visualization\n  labs(title = \"Distribution of Award Amounts (Log Scale)\",\n       x = \"Award Amount (Log Scale)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBox plot of award amounts by financial category\n\nif(!\"financial_category\" %in% colnames(procurement_enriched)) {\n  procurement_enriched &lt;- procurement_enriched %&gt;%\n    mutate(financial_category = case_when(\n      financial_grade %in% c(\"S1\", \"S2\", \"S3\", \"S4\") ~ \"Small (S1-S4)\",\n      financial_grade %in% c(\"S5\", \"S6\", \"S7\") ~ \"Medium (S5-S7)\",\n      financial_grade %in% c(\"S8\", \"S9\", \"S10\") ~ \"Large (S8-S10)\",\n      TRUE ~ \"Unspecified Grade\"\n    ))\n}\n\nggplot(procurement_enriched, aes(x = financial_category, y = awarded_amt)) +\n  geom_boxplot(fill = \"steelblue\", alpha = 0.7) +\n  scale_y_log10() +  # Log scale for better visualization\n  labs(title = \"Distribution of Award Amounts by Supplier Category\",\n       x = \"Supplier Financial Category\",\n       y = \"Award Amount (Log Scale)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nUnderstanding missing supplier grade information.\n\nfinancial_grade_coverage &lt;- sum(!is.na(procurement_enriched$financial_grade)) / nrow(procurement_enriched) * 100\ncat(\"Percentage of procurement records with financial grade information:\", round(financial_grade_coverage, 2), \"%\\n\")\n\nPercentage of procurement records with financial grade information: 64.35 %\n\nmissing_grade_by_size &lt;- procurement_enriched %&gt;%\n  group_by(is_grade_missing = is.na(financial_grade)) %&gt;%\n  summarise(\n    count = n(),\n    total_value = sum(awarded_amt, na.rm = TRUE),\n    avg_contract_value = mean(awarded_amt, na.rm = TRUE),\n    median_contract_value = median(awarded_amt, na.rm = TRUE),\n    pct_of_records = n() / nrow(procurement_enriched) * 100,\n    pct_of_value = sum(awarded_amt, na.rm = TRUE) / sum(procurement_enriched$awarded_amt, na.rm = TRUE) * 100\n  )\n\nprint(missing_grade_by_size)\n\n# A tibble: 2 × 7\n  is_grade_missing count  total_value avg_contract_value median_contract_value\n  &lt;lgl&gt;            &lt;int&gt;        &lt;dbl&gt;              &lt;dbl&gt;                 &lt;dbl&gt;\n1 FALSE            11994 31937582643.           2662797.               201464.\n2 TRUE              6644 71313233781.          10733479.                90000 \n# ℹ 2 more variables: pct_of_records &lt;dbl&gt;, pct_of_value &lt;dbl&gt;\n\n\n\n\n\n2.8 Relationship Between Variables\nLet’s see if there’s a relationship between award amount and number of contracts per supplier\n\nsupplier_counts &lt;- procurement_enriched %&gt;%\n  group_by(supplier_name) %&gt;%\n  summarise(contract_count = n(),\n            total_value = sum(awarded_amt, na.rm = TRUE),\n            avg_value = mean(awarded_amt, na.rm = TRUE))\n\nggplot(supplier_counts, aes(x = contract_count, y = avg_value)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(title = \"Relationship Between Number of Contracts and Average Value\",\n       x = \"Number of Contracts (Log Scale)\",\n       y = \"Average Contract Value (Log Scale)\") +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#data-preprocessing",
    "href": "Take-home_Ex/Take-home_Ex3.html#data-preprocessing",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "3. Data Preprocessing",
    "text": "3. Data Preprocessing\n\n3.1 Date Conversion and Handling Missing Values\nWe’ll perform several preprocessing steps to improve data quality:\n\nConvert date strings to proper Date objects\nCreate a year field for filtering\nHandle missing values in supplier names\nCreate financial grade categories for visualization\n\nFor the financial grades, we’ll consolidate the nine different S-grades (S2-S10) into three more manageable categories:\n\nSmall (S1-S4): Suppliers with financial capacity up to $500,000\nMedium (S5-S7): Suppliers with financial capacity between $1,000,000 and $5,000,000\nLarge (S8-S10): Suppliers with financial capacity of $10,000,000 and above\n\nThese preprocessing steps ensure our data is clean, properly formatted, and appropriately categorized for visualization.\n\n# Convert award_date to proper date format\nprocurement_enriched &lt;- procurement_enriched %&gt;%\n  mutate(award_date = as.Date(award_date, format=\"%d/%m/%Y\"))\n\n# Create a year field for filtering\nprocurement_enriched &lt;- procurement_enriched %&gt;%\n  mutate(year = year(award_date))\n\n# Handle missing values in supplier_name\nprocurement_enriched &lt;- procurement_enriched %&gt;%\n  mutate(supplier_name = ifelse(is.na(supplier_name) | supplier_name == \"Unknown\", \"Unspecified Supplier\", supplier_name))\n\n# Create financial grade categories for better visualization\nprocurement_enriched &lt;- procurement_enriched %&gt;%\n  mutate(financial_category = case_when(\n    financial_grade %in% c(\"S1\", \"S2\", \"S3\", \"S4\") ~ \"Small (S1-S4)\",\n    financial_grade %in% c(\"S5\", \"S6\", \"S7\") ~ \"Medium (S5-S7)\",\n    financial_grade %in% c(\"S8\", \"S9\", \"S10\") ~ \"Large (S8-S10)\",\n    TRUE ~ \"Unspecified Grade\"\n  ))\n\n# Preview the processed data\nglimpse(procurement_enriched)\n\nRows: 18,638\nColumns: 13\n$ tender_no            &lt;chr&gt; \"ACR000ETT18300010\", \"ACR000ETT18300011\", \"ACR000…\n$ tender_description   &lt;chr&gt; \"SUPPLY, DESIGN, DEVELOPMENT, CUSTOMIZATION, DELI…\n$ agency               &lt;chr&gt; \"Accounting And Corporate Regulatory Authority\", …\n$ award_date           &lt;date&gt; 2019-06-11, 2019-05-10, 2019-04-30, 2019-08-29, …\n$ tender_detail_status &lt;chr&gt; \"Awarded to Suppliers\", \"Awarded to No Suppliers\"…\n$ supplier_name        &lt;chr&gt; \"AZAAS PTE. LTD.\", \"Unspecified Supplier\", \"ACCEN…\n$ awarded_amt          &lt;dbl&gt; 2305880.0, 0.0, 2035000.0, 30700373.9, 178800.0, …\n$ financial_grade      &lt;chr&gt; \"S9\", NA, \"S10\", \"S10\", NA, \"S10\", \"S10\", \"S10\", …\n$ agency_abbr          &lt;chr&gt; \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"…\n$ ministry             &lt;chr&gt; \"MINISTRY OF FINANCE\", \"MINISTRY OF FINANCE\", \"MI…\n$ ministry_abbr        &lt;chr&gt; \"MOF\", \"MOF\", \"MOF\", \"MOF\", \"MOF\", \"MOF\", \"MOF\", …\n$ financial_category   &lt;chr&gt; \"Large (S8-S10)\", \"Unspecified Grade\", \"Large (S8…\n$ year                 &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#building-the-sankey-diagram-visualization",
    "href": "Take-home_Ex/Take-home_Ex3.html#building-the-sankey-diagram-visualization",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "4. Building the Sankey Diagram Visualization",
    "text": "4. Building the Sankey Diagram Visualization\nThe heart of our visualization is the create_sankey() function defined below. This function creates a Sankey diagram showing the flow of procurement funds from ministries to agencies to suppliers.\nThe function is highly customizable, allowing users to:\n\nFilter by year, ministry, agency, or supplier\nShow only the top K ministries, agencies, or suppliers by procurement value\nSet a minimum value threshold to include only significant flows\nEnable debug mode for troubleshooting when developing the Shiny application\n\nThis customizability makes the function valuable for exploring different aspects of the procurement network.\n\ncreate_sankey &lt;- function(data, \n                          year_filter = NULL, \n                          ministry_filter = NULL,\n                          agency_filter = NULL,\n                          supplier_filter = NULL,\n                          top_k_ministries = 10,\n                          top_k_agencies = 10,\n                          top_k_suppliers = 10,\n                          min_value = 1000000,\n                          supplier_char_limit = 50,\n                          group_by_grade = FALSE,\n                          debug = FALSE) {\n  \n  # Log data diagnostics if debug is enabled\n  if(debug) {\n    message(\"Data dimensions: \", nrow(data), \" rows x \", ncol(data), \" columns\")\n    if(\"ministry\" %in% colnames(data)) {\n      message(\"Unique ministries: \", length(unique(data$ministry)))\n      if(!is.null(ministry_filter)) {\n        message(\"Checking for requested ministry: \", \n                ifelse(any(ministry_filter %in% unique(data$ministry)), \"Found\", \"Not found\"))\n      }\n    } else {\n      warning(\"'ministry' column not found in data\")\n    }\n  }\n  \n  # Make a copy of the data\n  filtered_data &lt;- data\n  \n  # Apply year filter if specified\n  if(!is.null(year_filter)) {\n    if(\"year\" %in% colnames(filtered_data)) {\n      filtered_data &lt;- filtered_data %&gt;% filter(year == year_filter)\n      if(debug) message(\"After year filter: \", nrow(filtered_data), \" rows\")\n    } else {\n      warning(\"'year' column not found, skipping year filter\")\n    }\n  }\n  \n  # 1. Determine which ministries to include\n  if(!is.null(ministry_filter)) {\n    # Use the specified ministries\n    ministries_to_include &lt;- ministry_filter\n  } else {\n    # Get top ministries by award amount\n    ministries_to_include &lt;- filtered_data %&gt;%\n      group_by(ministry) %&gt;%\n      summarize(total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n      arrange(desc(total_value)) %&gt;%\n      head(top_k_ministries) %&gt;%\n      pull(ministry)\n  }\n  \n  # 2. Determine which agencies to include\n  if(!is.null(agency_filter)) {\n    # Use the specified agencies\n    agencies_to_include &lt;- agency_filter\n  } else {\n    # Get top agencies by award amount\n    agencies_to_include &lt;- filtered_data %&gt;%\n      filter(ministry %in% ministries_to_include) %&gt;%\n      group_by(agency) %&gt;%\n      summarize(total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n      arrange(desc(total_value)) %&gt;%\n      head(top_k_agencies) %&gt;%\n      pull(agency)\n  }\n  \n  # 3. Determine which suppliers to include based on the view mode\n  if(!is.null(supplier_filter)) {\n    # Use the specified suppliers\n    suppliers_to_include &lt;- supplier_filter\n  } else {\n    # Get top suppliers by award amount\n    if(group_by_grade) {\n      # When grouping by grade, supplier categories will be the financial_category (e.g., \"Large (S8-S10)\")\n      # We still need to filter by top suppliers for consistency, but we'll sum across all suppliers\n      suppliers_to_include &lt;- filtered_data %&gt;%\n        filter(agency %in% agencies_to_include) %&gt;%\n        group_by(financial_category) %&gt;%\n        summarize(total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n        filter(!is.na(financial_category)) %&gt;%  # Exclude NA categories\n        arrange(desc(total_value)) %&gt;%\n        head(top_k_suppliers) %&gt;%\n        pull(financial_category)\n    } else {\n      # When showing individual suppliers\n      suppliers_to_include &lt;- filtered_data %&gt;%\n        filter(agency %in% agencies_to_include) %&gt;%\n        group_by(supplier_name) %&gt;%\n        summarize(total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n        arrange(desc(total_value)) %&gt;%\n        head(top_k_suppliers) %&gt;%\n        pull(supplier_name)\n    }\n  }\n  \n  if(debug) {\n    message(\"Ministries to include: \", length(ministries_to_include))\n    message(\"Agencies to include: \", length(agencies_to_include))\n    message(\"Suppliers to include: \", length(suppliers_to_include))\n  }\n  \n  # Filter the data to only include the selected entities\n  if(group_by_grade) {\n    # When grouping by grade, filter by financial_category\n    filtered_data &lt;- filtered_data %&gt;%\n      filter(\n        ministry %in% ministries_to_include,\n        agency %in% agencies_to_include,\n        financial_category %in% suppliers_to_include\n      )\n  } else {\n    # When showing individual suppliers, filter by supplier_name\n    filtered_data &lt;- filtered_data %&gt;%\n      filter(\n        ministry %in% ministries_to_include,\n        agency %in% agencies_to_include,\n        supplier_name %in% suppliers_to_include\n      )\n  }\n  \n  if(debug) message(\"After entity filtering: \", nrow(filtered_data), \" rows\")\n  \n  # Check if we have any data left\n  if(nrow(filtered_data) == 0) {\n    warning(\"No data remains after filtering\")\n    return(NULL)\n  }\n  \n  # Create ministry-to-agency flow\n  ministry_agency_flow &lt;- filtered_data %&gt;%\n    group_by(ministry, agency) %&gt;%\n    summarize(value = sum(awarded_amt, na.rm = TRUE), .groups = 'drop') %&gt;%\n    filter(value &gt;= min_value)\n  \n  # Create agency-to-supplier flow based on the view mode\n  if(group_by_grade) {\n    # Group by financial_category (S-grades)\n    agency_supplier_flow &lt;- filtered_data %&gt;%\n      group_by(agency, financial_category) %&gt;%\n      summarize(value = sum(awarded_amt, na.rm = TRUE), .groups = 'drop') %&gt;%\n      filter(value &gt;= min_value) %&gt;%\n      rename(supplier_name = financial_category)\n  } else {\n    # Group by individual supplier_name\n    agency_supplier_flow &lt;- filtered_data %&gt;%\n      group_by(agency, supplier_name) %&gt;%\n      summarize(value = sum(awarded_amt, na.rm = TRUE), .groups = 'drop') %&gt;%\n      filter(value &gt;= min_value)\n  }\n  \n  if(debug) {\n    message(\"Ministry-agency flows: \", nrow(ministry_agency_flow))\n    message(\"Agency-supplier flows: \", nrow(agency_supplier_flow))\n  }\n  \n  # Check if we have any flows above the minimum value\n  if(nrow(ministry_agency_flow) == 0 && nrow(agency_supplier_flow) == 0) {\n    warning(\"No flows remain after applying minimum value filter (\", \n            min_value, \")\")\n    return(NULL)\n  }\n  \n  # Get unique lists of each entity type\n  ministry_nodes &lt;- unique(ministry_agency_flow$ministry)\n  agency_nodes &lt;- unique(c(ministry_agency_flow$agency, agency_supplier_flow$agency))\n  supplier_nodes &lt;- unique(agency_supplier_flow$supplier_name)\n  \n  # Create a data frame to hold both display names and full names for tooltips\n  nodes &lt;- data.frame(\n    name = character(),        # Full name\n    display_name = character(),  # Display name (full name for ministry/agency)\n    group = character(),       # Node type\n    stringsAsFactors = FALSE\n  )\n  \n  # Add ministries with full name\n  for (ministry in ministry_nodes) {\n    display &lt;- ministry\n    \n    nodes &lt;- rbind(nodes, data.frame(\n      name = ministry,\n      display_name = display,\n      group = \"ministry\",\n      stringsAsFactors = FALSE\n    ))\n  }\n  \n  # Add agencies with full name\n  for (agency in agency_nodes) {\n    display &lt;- agency\n    \n    nodes &lt;- rbind(nodes, data.frame(\n      name = agency,\n      display_name = display,\n      group = \"agency\",\n      stringsAsFactors = FALSE\n    ))\n  }\n  \n  # Add suppliers with truncation if needed\n  for (supplier in supplier_nodes) {\n    # Truncate long supplier names based on the supplier_char_limit parameter\n    display &lt;- if(nchar(supplier) &gt; supplier_char_limit) \n                 paste0(substr(supplier, 1, supplier_char_limit - 3), \"...\") \n               else \n                 supplier\n    \n    nodes &lt;- rbind(nodes, data.frame(\n      name = supplier,\n      display_name = display,\n      group = \"supplier\",\n      stringsAsFactors = FALSE\n    ))\n  }\n  \n  # Get the indices of each node\n  node_indices &lt;- setNames(0:(nrow(nodes)-1), nodes$name)\n  \n  # Create the ministry to agency links\n  links_m2a &lt;- ministry_agency_flow %&gt;%\n    mutate(\n      source = node_indices[ministry],\n      target = node_indices[agency]\n    ) %&gt;%\n    select(source, target, value)\n  \n  # Create the agency to supplier links\n  links_a2s &lt;- agency_supplier_flow %&gt;%\n    mutate(\n      source = node_indices[agency],\n      target = node_indices[supplier_name]\n    ) %&gt;%\n    select(source, target, value)\n  \n  # Combine all links\n  links &lt;- bind_rows(links_m2a, links_a2s)\n  \n  # Check if we have any links\n  if(nrow(links) == 0) {\n    warning(\"No links could be created\")\n    return(NULL)\n  }\n  \n  # Create Sankey diagram\n  sankey &lt;- sankeyNetwork(\n    Links = links, \n    Nodes = nodes, \n    Source = \"source\", \n    Target = \"target\", \n    Value = \"value\", \n    NodeID = \"display_name\",\n    NodeGroup = \"group\",\n    # Custom color scheme\n    colourScale = JS('d3.scaleOrdinal()\n                     .domain([\"ministry\", \"agency\", \"supplier\"])\n                     .range([\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])'),\n    # Layout parameters\n    nodeWidth = 30,\n    nodePadding = 20,\n    margin = list(top = 30, right = 30, bottom = 30, left = 30),\n    sinksRight = TRUE,\n    fontSize = 11,\n    height = 800,\n    width = 1200,\n    iterations = 64  # More iterations for better layout\n  )\n  \n  # Add JavaScript for better rendering and tooltip formatting\n  sankey &lt;- htmlwidgets::onRender(\n    sankey,\n    '\n    function(el, x) {\n      // Ensure diagram is properly sized\n      d3.select(el).select(\"svg\")\n        .attr(\"viewBox\", \"0 0 1200 800\")\n        .attr(\"preserveAspectRatio\", \"xMidYMid meet\");\n        \n      // Add formatted tooltips to links\n      d3.select(el).selectAll(\".link\")\n        .append(\"title\")\n        .text(function(d) { \n          return d.source.name + \" → \" + d.target.name + \n                 \"\\\\nValue: $\" + d3.format(\",.0f\")(d.value); \n        });\n    }\n    '\n  )\n  \n  return(sankey)\n}\n\nThe create_sankey() function is highly customizable through various parameters that control filtering, display options, and performance.\nFiltering parameters:\n\nyear_filter: Numeric or NULL. Filter records by a specific year (e.g., 2023). If NULL (default), all years are included.\nministry_filter: Character vector or NULL. Filter by specific ministry names. If NULL (default), top ministries are selected based on top_k_ministries.\nagency_filter: Character vector or NULL. Filter by specific agency names. If NULL (default), top agencies are selected based on top_k_agencies.\nsupplier_filter: Character vector or NULL. Filter by specific supplier names or categories (depending on group_by_grade). If NULL (default), top suppliers are selected based on top_k_suppliers.\n\nTop-k selection parameters:\n\ntop_k_ministries: Numeric, default 10. When no ministry filter is provided, specifies how many top ministries to include, ranked by total procurement value.\ntop_k_agencies: Numeric, default 10. When no agency filter is provided, specifies how many top agencies to include, ranked by total procurement value.\ntop_k_suppliers: Numeric, default 10. When no supplier filter is provided, specifies how many top suppliers or categories to include, ranked by total procurement value.\n\nOther parameters:\n\nmin_value: Numeric, default 1,000,000. Minimum threshold value for flows to be included in the diagram. Flows with values below this threshold are excluded, helping to simplify complex diagrams.\nsupplier_char_limit: Numeric, default 50. Maximum number of characters to display for supplier names. Longer names are truncated with “…”.\ngroup_by_grade: Logical, default FALSE. Controls whether suppliers are grouped by financial grade categories (TRUE) or shown as individual suppliers (FALSE).\ndebug: Logical, default FALSE. When TRUE, prints diagnostic messages during function execution to help troubleshoot issues."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#creating-sankey-visualization-examples",
    "href": "Take-home_Ex/Take-home_Ex3.html#creating-sankey-visualization-examples",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "5. Creating Sankey Visualization Examples",
    "text": "5. Creating Sankey Visualization Examples\nThis section presents several Sankey diagram visualizations that demonstrate different perspectives on the GeBiz procurement data. Each visualization highlights specific patterns and relationships within the procurement network, helping stakeholders understand how government funds flow from ministries to agencies to suppliers.\n\n5.1 Filtering by Ministry\nThis visualization focuses on the Ministry of Education’s procurement flows, revealing how education funding distributes across agencies and suppliers. It highlights the ministry’s key procurement channels and major supplier relationships.\n\n# Create Sankey diagram for Ministry of Education\nmoe_sankey &lt;- create_sankey(\n  data = procurement_enriched,\n  year_filter = 2023,\n  ministry_filter = c(\"MINISTRY OF EDUCATION\"),\n  min_value = 1000000,\n  top_k_agencies = 100,\n  top_k_suppliers = 20,\n  supplier_char_limit = 30\n)\n\n# Save the widget to an HTML file\nsaveNetwork(moe_sankey, file = paste0(\"sankey_moe_\", 2023, \".html\"))\n\n\nThe second visualization compares the Ministry of Health and Ministry of Education, with suppliers grouped by financial grade. This comparison reveals differences in agency structures, supplier profiles, and procurement value distributions between these major ministries.\n\n# Create Sankey diagram comparing two major ministries\nhealth_defence_sankey &lt;- create_sankey(\n  data = procurement_enriched,\n  year_filter = 2023,\n    ministry_filter = c(\"MINISTRY OF HEALTH\", \"MINISTRY OF EDUCATION\"),\n  min_value = 5000000,\n  top_k_agencies = 10, \n  top_k_suppliers = 15,  \n  group_by_grade = TRUE\n)\n\nsaveNetwork(health_defence_sankey, file = paste0(\"sankey_health_education_\", 2023, \".html\"))\n\n\n\n\n5.2 Filtering by Agency\nThis Sankey diagram examines the Land Transport Authority’s procurement patterns, showing its ministry connection and how its funds flow to suppliers of varying financial capacities. This view helps understand procurement strategies within key infrastructure agencies.\n\n# Create Sankey diagram for Land Transport Authority\nlta_sankey &lt;- create_sankey(\n  data = procurement_enriched,\n  year_filter = 2023,\n  agency_filter = c(\"Land Transport Authority\"),\n  min_value = 500000, \n  top_k_suppliers = 25\n)\n\nsaveNetwork(lta_sankey, file = paste0(\"sankey_lta_\", 2023, \".html\"))\n\n\n\n\n5.3 Filtering by Suppliers\n\nLarge Suppliers Analysis\nThis visualization focuses on the largest suppliers (S8-S10 financial grades) to understand how government procurement from major companies flows through different ministries and agencies. This perspective helps identify which large suppliers are receiving significant government contracts and through which channels.\n\n# Identify top 10 suppliers by total contract value\ntop_suppliers &lt;- procurement_enriched %&gt;%\n  group_by(supplier_name) %&gt;%\n  summarize(total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value)) %&gt;%\n  head(10) %&gt;%\n  pull(supplier_name)\n\n# Create Sankey diagram for top suppliers\ntop_suppliers_sankey &lt;- create_sankey(\n  data = procurement_enriched,\n  supplier_filter = top_suppliers,\n  min_value = 500000, \n  top_k_ministries = 15,\n  top_k_agencies = 25,   \n  debug = FALSE\n)\n\nsaveNetwork(top_suppliers_sankey, file = paste0(\"sankey_top_suppliers_\", 2023, \".html\"))\n\n\n\n\nSME Suppliers Analysis\nThis view highlights Small and Medium Enterprise suppliers (S2-S7 financial grades), showing which ministries and agencies engage more with smaller businesses. The analysis supports policy objectives related to SME participation in government procurement.\n\n# Get small-medium sized suppliers (those with lower financial grades)\nsme_suppliers &lt;- procurement_enriched %&gt;%\n  filter(financial_grade %in% c(\"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\")) %&gt;%\n  group_by(supplier_name) %&gt;%\n  summarize(total_value = sum(awarded_amt, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_value)) %&gt;%\n  head(15) %&gt;%\n  pull(supplier_name)\n\n# Create Sankey diagram for SME suppliers\nsme_suppliers_sankey &lt;- create_sankey(\n  data = procurement_enriched,\n  # Consider removing the year filter to see all data for these suppliers\n  # year_filter = 2023,  \n  supplier_filter = sme_suppliers,\n  min_value = 100000,   # Lower threshold for SMEs\n  top_k_ministries = 10,  # Show up to 10 ministries\n  top_k_agencies = 20,\n  group_by_grade = FALSE\n)\n\nsaveNetwork(sme_suppliers_sankey, file = paste0(\"sankey_sme_suppliers_\", 2023, \".html\"))\n\n\n\n\n\n5.4 Top K Analysis\nThis visualization presents the most significant procurement flows across the entire government for 2023. It quickly identifies the dominant ministries, key agencies, and major suppliers by procurement value, providing a strategic overview of government spending patterns.\n\n# Create Sankey diagram showing top entities across all dimensions\ntop_k_sankey &lt;- create_sankey(\n  data = procurement_enriched,\n  year_filter = 2023,\n  top_k_ministries = 5,  \n  top_k_agencies = 10, \n  top_k_suppliers = 20, \n  min_value = 500000\n)\n\nsaveNetwork(top_k_sankey, file = \"sankey_top_k_2023.html\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#ui-design-for-interactive-dashboard",
    "href": "Take-home_Ex/Take-home_Ex3.html#ui-design-for-interactive-dashboard",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "6. UI Design for Interactive Dashboard",
    "text": "6. UI Design for Interactive Dashboard\nThis section outlines the user interface design for the GeBiz Procurement Data Visualization dashboard. The interactive dashboard is designed to enable users to explore complex procurement flows through an intuitive and insight-focused interface.\n\n6.1 Dashboard Layout\nThe dashboard follows a three-panel layout design optimized for both analytical depth and ease of use:\n\nFilter & Control Panel (Left): Provides structured access to filters and visualization settings\nVisualization Area (Center): Displays the interactive Sankey diagram with supporting tools\nInsights Panel (Right): Shows context-aware metrics and analytical highlights\n\nThis organization balances user control with information discovery, enabling both directed analysis and exploratory data visualization.\n\n\n6.2 Filter & Control Panel\nThe left panel is organized into two distinct functional areas to improve workflow efficiency:\n\nData Filtering Section\n\nTime Period Filter\n\nYear dropdown (2019-2023)\nQuarter selection (optional)\nDefault: Most recent complete year\n\nFocus Selection\n\nTabbed interface (Ministry/Agency/Supplier)\nSearchable entity list with checkboxes\n“Select Top N” shortcut buttons\nMulti-select capabilities with clear indicators\n\nQuick Filters\n\nPredefined filters (e.g., “Education Sector”, “Healthcare”, “Top 5 by Value”)\nRecently used filter combinations\nSave/load filter configurations\n\n\n\n\nVisualization Settings\n\nThreshold Controls\n\nMinimum flow value slider with formatted value display\nTop N entities to display (separate controls for ministries, agencies, suppliers)\n\nDisplay Options\n\nGroup suppliers by financial grade toggle\nUse abbreviations toggle\nNode sizing options (fixed vs. proportional)\nColor scheme selector\n\n\n\n\n\n6.3 Visualization Area\nThe central area features a dynamically updated Sankey diagram with supporting tools:\n\nVisualization Toolbar\n\nView Controls: Zoom in/out, reset view, fit to screen\nLayout Options: Auto-arrange, lock/unlock node positions\nExport Tools: Download as SVG/PNG, copy link, get embed code\nHelp Features: Diagram explanation, interaction guide\n\n\n\nInteractive Sankey Diagram\n\nEnhanced Interactions:\n\nHover: Highlight related flows, show tooltip with key metrics\nClick: Select node, update insights panel, persist highlight\nDrag: Reposition nodes for custom layouts\nDouble-click: Expand/collapse node to show/hide details\n\nVisual Elements:\n\nColor-coded nodes (blue: ministries, orange: agencies, green: suppliers)\nLink thickness proportional to value\nColor intensity indicating relative importance\nConsistent visual language across the application\n\n\n\n\nLegend and Context\n\nColor Legend: Explains node and link color coding\nFilter Status: Shows active filters and view parameters\nData Coverage: Indicates data completeness (especially for financial grades)\n\n\n\n\n6.4 Insights Panel\nThe right panel provides context-aware information that updates dynamically based on user interactions:\n\nSelection Context\n\nEntity Profile: Detailed information about selected ministry/agency/supplier\nKey Metrics: Procurement volume, contract count, average value\nComparative Indicators: Performance relative to peers and historical trends\n\n\n\nFlow Analysis\n\nTop Flows: Identifies largest procurement channels\nFlow Distribution: Shows how funds distribute across the procurement network\nConcentration Analysis: Highlights diversification or concentration patterns\n\n\n\nSupplier Analysis\n\nFinancial Grade Distribution:\n\nBreakdown by supplier capacity (S1-S10)\nComparison with government-wide averages\nSME participation metrics\n\nTemporal Patterns:\n\nSparklines showing procurement trends\nSeasonal patterns visualization\nYear-over-year comparisons\n\n\n\n\nActionable Insights\n\nKey Findings: Automatically generated observations about the visualized data\nSuggested Explorations: Recommended filters or views for deeper analysis\nAnomaly Detection: Highlights unusual patterns or outliers\n\n\n\n\n6.5 Interactive Features\nThe dashboard implements several advanced interactive features:\n\nCross-filtering: Selections in one view affect other views for consistent analysis\nProgressive Disclosure: Interface elements appear contextually based on user actions\nState Persistence: User selections and custom views are maintained during analysis\nResponsive Layout: Interface adapts to available screen space\nKeyboard Navigation: Full keyboard access for accessibility compliance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#rfm-analysis-module",
    "href": "Take-home_Ex/Take-home_Ex3.html#rfm-analysis-module",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "7. RFM Analysis Module",
    "text": "7. RFM Analysis Module\nThe RFM (Recency, Frequency, Monetary) analysis module provides powerful supplier segmentation capabilities based on procurement behavior patterns.\n\n7.1 RFM Analysis Function\nThe create_rfm_analysis() function segments suppliers based on their procurement patterns.\n\ncreate_rfm_analysis &lt;- function(data, \n                               year_filter = NULL,\n                               ministry_filter = NULL,\n                               agency_filter = NULL,\n                               supplier_filter = NULL,\n                               min_monetary_value = 10000,\n                               max_suppliers = 1000,\n                               recency_quartiles = 4,\n                               frequency_quartiles = 4,\n                               monetary_quartiles = 4,\n                               include_financial_grade = TRUE,\n                               analysis_date = NULL,\n                               debug = FALSE) {\n  \n  # Make a copy of the data to work with\n  filtered_data &lt;- data\n  \n  # Apply year filter if specified\n  if(!is.null(year_filter)) {\n    if(\"year\" %in% colnames(filtered_data)) {\n      filtered_data &lt;- filtered_data %&gt;% filter(year == year_filter)\n      if(debug) message(\"After year filter: \", nrow(filtered_data), \" rows\")\n    } else {\n      warning(\"'year' column not found, skipping year filter\")\n    }\n  }\n  \n  # Apply ministry filter if specified\n  if(!is.null(ministry_filter)) {\n    filtered_data &lt;- filtered_data %&gt;% filter(ministry %in% ministry_filter)\n    if(debug) message(\"After ministry filter: \", nrow(filtered_data), \" rows\")\n  }\n  \n  # Apply agency filter if specified\n  if(!is.null(agency_filter)) {\n    filtered_data &lt;- filtered_data %&gt;% filter(agency %in% agency_filter)\n    if(debug) message(\"After agency filter: \", nrow(filtered_data), \" rows\")\n  }\n  \n  # Apply supplier filter if specified\n  if(!is.null(supplier_filter)) {\n    filtered_data &lt;- filtered_data %&gt;% filter(supplier_name %in% supplier_filter)\n    if(debug) message(\"After supplier filter: \", nrow(filtered_data), \" rows\")\n  }\n  \n  # Ensure date column is properly formatted\n  if(!\"award_date\" %in% colnames(filtered_data) || !inherits(filtered_data$award_date, \"Date\")) {\n    filtered_data &lt;- filtered_data %&gt;%\n      mutate(award_date = as.Date(award_date, format=\"%d/%m/%Y\"))\n  }\n  \n  # Set analysis date if not provided\n  if(is.null(analysis_date)) {\n    analysis_date &lt;- max(filtered_data$award_date, na.rm = TRUE) + days(1)\n    if(debug) message(\"Analysis date set to: \", analysis_date)\n  }\n  \n  # Calculate RFM metrics for each supplier\n  supplier_rfm &lt;- filtered_data %&gt;%\n    group_by(supplier_name) %&gt;%\n    summarize(\n      # Recency: days since last contract (lower is better)\n      recency = as.numeric(difftime(analysis_date, max(award_date, na.rm = TRUE), units = \"days\")),\n      # Frequency: number of contracts\n      frequency = n(),\n      # Monetary: total value of contracts\n      monetary = sum(awarded_amt, na.rm = TRUE),\n      # Additional metrics\n      avg_contract = monetary / frequency,\n      first_contract = min(award_date, na.rm = TRUE),\n      last_contract = max(award_date, na.rm = TRUE),\n      days_active = as.numeric(difftime(max(award_date, na.rm = TRUE), \n                                      min(award_date, na.rm = TRUE), \n                                      units = \"days\")) + 1,\n      .groups = 'drop'\n    )\n  \n  # Add financial grade information if requested\n  if(include_financial_grade && \"financial_grade\" %in% colnames(filtered_data)) {\n    financial_info &lt;- filtered_data %&gt;%\n      group_by(supplier_name) %&gt;%\n      summarize(\n        financial_grade = first(financial_grade),\n        financial_category = first(financial_category),\n        .groups = 'drop'\n      )\n    \n    supplier_rfm &lt;- supplier_rfm %&gt;%\n      left_join(financial_info, by = \"supplier_name\")\n  }\n  \n  # Remove suppliers with missing names or identified as \"Unknown\"\n  supplier_rfm &lt;- supplier_rfm %&gt;%\n    filter(!is.na(supplier_name), \n           supplier_name != \"Unknown\", \n           supplier_name != \"Unspecified Supplier\") %&gt;%\n    # Filter out suppliers with extremely low values\n    filter(monetary &gt; min_monetary_value)\n  \n  # Calculate RFM scores (quartiles)\n  supplier_rfm_scored &lt;- supplier_rfm %&gt;%\n    mutate(\n      # Recency score (note: lower recency days = higher score)\n      r_score = ntile(desc(recency), recency_quartiles),\n      # Frequency score\n      f_score = ntile(frequency, frequency_quartiles),\n      # Monetary score\n      m_score = ntile(monetary, monetary_quartiles),\n      # Combined RFM score\n      rfm_score = r_score + f_score + m_score\n    )\n  \n  # Create meaningful segment labels based on RFM scores\n  supplier_rfm_scored &lt;- supplier_rfm_scored %&gt;%\n    mutate(\n      segment = case_when(\n        r_score &gt;= 3 & f_score &gt;= 3 & m_score &gt;= 3 ~ \"Top Suppliers\",\n        r_score &gt;= 3 & f_score &gt;= 3 ~ \"Active Contractors\",\n        r_score &gt;= 3 & m_score &gt;= 3 ~ \"Big-Ticket Recent\",\n        f_score &gt;= 3 & m_score &gt;= 3 ~ \"High Value Regular\",\n        r_score &gt;= 3 ~ \"Recent Contractors\",\n        f_score &gt;= 3 ~ \"Regular Contractors\",\n        m_score &gt;= 3 ~ \"Big Spenders\",\n        r_score == 1 & f_score == 1 & m_score == 1 ~ \"Least Engaged\",\n        TRUE ~ \"Average Contractors\"\n      )\n    )\n  \n  # Add segment description for tooltip and display\n  segment_descriptions &lt;- c(\n    \"Top Suppliers\" = \"Recent, frequent, and high-value contractors\",\n    \"Active Contractors\" = \"Recent and frequent, but lower contract values\",\n    \"Big-Ticket Recent\" = \"Recent and high-value, but less frequent\",\n    \"High Value Regular\" = \"Frequent and high-value, but less recent\",\n    \"Recent Contractors\" = \"Recently engaged, but lower frequency and value\",\n    \"Regular Contractors\" = \"Frequent engagement, but lower recency and value\",\n    \"Big Spenders\" = \"High contract values, but less recent and frequent\",\n    \"Average Contractors\" = \"Middle-range performance across RFM metrics\",\n    \"Least Engaged\" = \"Low scores across all RFM dimensions\"\n  )\n  \n  supplier_rfm_scored &lt;- supplier_rfm_scored %&gt;%\n    mutate(segment_description = segment_descriptions[segment])\n  \n  # Create summary by segment\n  segment_summary &lt;- supplier_rfm_scored %&gt;%\n    group_by(segment) %&gt;%\n    summarize(\n      count = n(),\n      pct_suppliers = round(n() / nrow(supplier_rfm_scored) * 100, 1),\n      avg_recency = round(mean(recency)),\n      avg_frequency = round(mean(frequency), 1),\n      avg_monetary = round(mean(monetary)),\n      total_monetary = sum(monetary),\n      pct_spend = round(sum(monetary) / sum(supplier_rfm_scored$monetary) * 100, 1),\n      .groups = 'drop'\n    ) %&gt;%\n    arrange(desc(total_monetary))\n  \n  # Sample data if there are too many suppliers for visualization\n  if(nrow(supplier_rfm_scored) &gt; max_suppliers) {\n    supplier_rfm_sampled &lt;- supplier_rfm_scored %&gt;%\n      # Stratified sampling to maintain segment proportions\n      group_by(segment) %&gt;%\n      sample_frac(min(1, max_suppliers / nrow(supplier_rfm_scored))) %&gt;%\n      ungroup()\n    \n    if(debug) message(\"Sampled \", nrow(supplier_rfm_sampled), \" suppliers from \", \n                    nrow(supplier_rfm_scored), \" total suppliers for visualization\")\n  } else {\n    supplier_rfm_sampled &lt;- supplier_rfm_scored\n  }\n  \n  # Return all objects needed for analysis in a list\n  return(list(\n    rfm_data = supplier_rfm_scored,\n    rfm_sample = supplier_rfm_sampled,\n    segment_summary = segment_summary,\n    analysis_date = analysis_date\n  ))\n}\n\n\nRFM Analysis Parameters\nThe create_rfm_analysis() function offers extensive customization through various parameters that control filtering, segmentation granularity, and output configuration. Understanding these parameters is crucial for effectively tailoring the RFM analysis to specific analytical needs.\nFiltering Parameters\n\nyear_filter: Numeric or NULL. Restricts analysis to a specific year. When NULL (default), all years are included.\nministry_filter: Character vector or NULL. Limits analysis to specific ministries. When NULL (default), all ministries are included.\nagency_filter: Character vector or NULL. Restricts analysis to specific agencies. When NULL (default), all agencies are included.\nsupplier_filter: Character vector or NULL. Limits analysis to specific suppliers. When NULL (default), all suppliers are included.\n\nValue and Sample Size Controls\n\nmin_monetary_value: Numeric, default 10,000. Sets the minimum procurement value threshold for including a supplier in the analysis. Suppliers with total values below this threshold are excluded to focus on significant relationships.\nmax_suppliers: Numeric, default 1,000. Limits the number of suppliers included in visualization outputs. If the number of suppliers exceeds this value, a stratified sample is selected to maintain representation across segments.\n\nSegmentation Controls\n\nrecency_quartiles: Numeric, default 4. Sets the number of segments for the recency dimension. Higher values create more granular segmentation.\nfrequency_quartiles: Numeric, default 4. Sets the number of segments for the frequency dimension. Higher values create more granular segmentation.\nmonetary_quartiles: Numeric, default 4. Sets the number of segments for the monetary value dimension. Higher values create more granular segmentation.\n\nAdditional Options\n\ninclude_financial_grade: Logical, default TRUE. When TRUE, incorporates supplier financial grade information into the RFM analysis, enabling correlation between financial capacity and procurement behavior.\nanalysis_date: Date or NULL. Specifies the reference date for calculating recency (days since last contract). When NULL (default), uses the most recent date in the dataset plus one day.\ndebug: Logical, default FALSE. When TRUE, outputs diagnostic messages during analysis to assist with troubleshooting.\n\nThe function outputs a comprehensive list containing: - rfm_data: Complete RFM analysis for all suppliers - rfm_sample: A sample of suppliers for visualization (limited by max_suppliers) - segment_summary: Aggregated metrics for each RFM segment - analysis_date: The reference date used for recency calculations\n\n\n\n7.2 Creating RFM Visualizations\nVarious visualization functions to represent RFM analysis:\n\ncreate_rfm_heatmap(): Visualizes the distribution of suppliers across RFM scores\ncreate_rfm_bubble_chart(): Maps suppliers across the RFM dimensions\ncreate_segment_distribution(): Compares supplier count vs. procurement value\ncreate_grade_segment_heatmap(): Shows the relationship between financial grades and RFM segments\ncreate_rfm_datatable(): Provides an interactive table of RFM analysis results\n\n\ncreate_rfm_heatmap &lt;- function(rfm_results, \n                               title = \"RFM Heatmap: Supplier Distribution\",\n                               subtitle = NULL,  # Make subtitle optional\n                               interactive = TRUE) {\n  \n  # Prepare data for heatmap\n  rfm_heatmap_data &lt;- rfm_results$rfm_data %&gt;%\n    count(r_score, f_score, m_score) %&gt;%\n    # Convert scores to factors for better visualization\n    mutate(\n      r_score = factor(r_score, levels = 4:1),  # Reversed to put higher scores at top\n      f_score = factor(f_score),\n      m_score = factor(m_score)\n    )\n  \n  # Create base plot\n  rfm_heatmap &lt;- ggplot(rfm_heatmap_data, aes(f_score, r_score, fill = n)) +\n    geom_tile(color = \"white\") +\n    geom_text(aes(label = n), color = \"white\", size = 3) +\n    scale_fill_viridis_c() +\n    labs(\n      title = title,\n      x = \"Frequency Score (1-4)\",\n      y = \"Recency Score (1-4)\",\n      fill = \"Number of\\nSuppliers\"\n    ) +\n    facet_wrap(~m_score, labeller = labeller(m_score = function(x) paste(\"Monetary Score:\", x))) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 10, b = 20)),\n      axis.title = element_text(face = \"bold\"),\n      legend.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\", size = 12),\n      strip.background = element_rect(fill = \"grey90\", color = NA),\n      plot.margin = margin(t = 20, r = 10, b = 10, l = 10),\n      panel.spacing = unit(1, \"lines\")\n    )\n  \n  # Add subtitle only if it's provided\n  if (!is.null(subtitle)) {\n    rfm_heatmap &lt;- rfm_heatmap +\n      labs(subtitle = subtitle) +\n      theme(plot.subtitle = element_text(size = 12, margin = margin(b = 15)))\n  }\n  \n  # For interactive plots, adjust the layout\n  if(interactive) {\n    p &lt;- ggplotly(rfm_heatmap, height = 600)\n    \n    # Handle title and subtitle for interactive plot\n    if (!is.null(subtitle)) {\n      p &lt;- p %&gt;% layout(\n        title = list(text = paste0(title, \"&lt;br&gt;\", \"&lt;sup&gt;\", subtitle, \"&lt;/sup&gt;\")),\n        margin = list(t = 100)\n      )\n    } else {\n      p &lt;- p %&gt;% layout(\n        title = list(text = title),\n        margin = list(t = 60)  # Less margin needed without subtitle\n      )\n    }\n    \n    return(p)\n  } else {\n    return(rfm_heatmap)\n  }\n}\n\n# 2. RFM Bubble Chart - visualizing suppliers across the RFM dimensions\ncreate_rfm_bubble_chart &lt;- function(rfm_results,\n                                   title = \"Supplier RFM Analysis\",\n                                   subtitle = \"Bubble size represents total contract value\",\n                                   x_log = TRUE,\n                                   size_range = c(2, 15),\n                                   color_palette = \"Set1\",\n                                   interactive = TRUE) {\n  \n  # Create RFM bubble chart\n  rfm_bubble &lt;- ggplot(rfm_results$rfm_sample, \n                      aes(frequency, recency, size = monetary, color = segment)) +\n    geom_point(alpha = 0.7) +\n    scale_size(range = size_range, name = \"Contract Value\") +\n    scale_color_brewer(palette = color_palette) +\n    labs(\n      title = title,\n      subtitle = subtitle,\n      x = \"Frequency (Number of Contracts)\",\n      y = \"Recency (Days Since Last Contract)\",\n      color = \"Segment\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 16),\n      plot.subtitle = element_text(size = 12),\n      axis.title = element_text(face = \"bold\"),\n      legend.title = element_text(face = \"bold\")\n    )\n  \n  # Apply log scale if requested\n  if(x_log) {\n    rfm_bubble &lt;- rfm_bubble + \n      scale_x_log10(labels = scales::comma) +\n      labs(x = \"Frequency (Number of Contracts, log scale)\")\n  } else {\n    rfm_bubble &lt;- rfm_bubble +\n      scale_x_continuous(labels = scales::comma)\n  }\n  \n  # Always reverse y-axis so lower recency (better) is at the top\n  rfm_bubble &lt;- rfm_bubble + scale_y_reverse(labels = scales::comma)\n  \n  # Return interactive or static plot based on parameter\n  if(interactive) {\n    return(ggplotly(rfm_bubble) %&gt;%\n             layout(title = list(text = paste0(title, \"&lt;br&gt;\", \"&lt;sup&gt;\", subtitle, \"&lt;/sup&gt;\"))))\n  } else {\n    return(rfm_bubble)\n  }\n}\n\n# 3. Segment Distribution Chart - comparing supplier count vs. procurement value\ncreate_segment_distribution &lt;- function(rfm_results,\n                                      title = \"Supplier Segments: Distribution\",\n                                      subtitle = \"Comparing percentage of suppliers to percentage of total procurement value\",\n                                      colors = c(\"#1f77b4\", \"#ff7f0e\"),\n                                      interactive = TRUE) {\n  \n  # Create a dual-bar chart using position_dodge instead of position_nudge\n  segment_dist_chart &lt;- ggplot(rfm_results$segment_summary, \n                              aes(x = reorder(segment, pct_spend))) +\n    geom_bar(aes(y = pct_suppliers, fill = \"% of Suppliers\"), stat = \"identity\", \n             alpha = 0.7, position = position_dodge(width = 0.8), width = 0.7) +\n    geom_bar(aes(y = pct_spend, fill = \"% of Spend\"), stat = \"identity\", \n             alpha = 0.7, position = position_dodge(width = 0.8), width = 0.7) +\n    scale_fill_manual(values = setNames(colors, c(\"% of Suppliers\", \"% of Spend\"))) +\n    labs(\n      title = title,\n      subtitle = subtitle,\n      x = \"Segment\",\n      y = \"Percentage (%)\",\n      fill = \"\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 16),\n      plot.subtitle = element_text(size = 12),\n      axis.title = element_text(face = \"bold\"),\n      axis.text.x = element_text(angle = 45, hjust = 1),\n      legend.position = \"top\"\n    ) +\n    coord_flip()\n  \n  # Return interactive or static plot based on parameter\n  if(interactive) {\n    return(ggplotly(segment_dist_chart) %&gt;%\n             layout(title = list(text = paste0(title, \"&lt;br&gt;\", \"&lt;sup&gt;\", subtitle, \"&lt;/sup&gt;\"))))\n  } else {\n    return(segment_dist_chart)\n  }\n}\n\n# 4. Financial Grade vs. RFM Segment Heatmap\ncreate_grade_segment_heatmap &lt;- function(rfm_results,\n                                       title = \"Supplier Distribution: Financial Grade vs. RFM Segment\",\n                                       interactive = TRUE) {\n  \n  # Check if financial grade data is available\n  if(!\"financial_category\" %in% colnames(rfm_results$rfm_data)) {\n    warning(\"Financial category data not available. Cannot create grade vs segment heatmap.\")\n    return(NULL)\n  }\n  \n  # Prepare data for cross-analysis\n  grade_rfm_data &lt;- rfm_results$rfm_data %&gt;%\n    filter(!is.na(financial_category)) %&gt;%\n    group_by(financial_category, segment) %&gt;%\n    summarize(\n      count = n(),\n      total_monetary = sum(monetary),\n      .groups = 'drop'\n    )\n  \n  # Create a heatmap showing the distribution of segments across financial categories\n  grade_segment_heatmap &lt;- ggplot(grade_rfm_data, \n                                aes(segment, financial_category, fill = count)) +\n    geom_tile() +\n    geom_text(aes(label = count), color = \"white\") +\n    scale_fill_viridis_c() +\n    labs(\n      title = title,\n      x = \"RFM Segment\",\n      y = \"Financial Category\",\n      fill = \"Count\"\n    ) +\n    theme_minimal() +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1),\n      plot.title = element_text(face = \"bold\")\n    )\n  \n  # Return interactive or static plot based on parameter\n  if(interactive) {\n    return(ggplotly(grade_segment_heatmap))\n  } else {\n    return(grade_segment_heatmap)\n  }\n}\n# 5. Interactive RFM Data Table - Fixed version\ncreate_rfm_datatable &lt;- function(rfm_results,\n                            title = \"Supplier RFM Analysis and Segmentation\",\n                            max_rows = 1000) {\n  # Get the data and convert to plain data.frame\n  rfm_data &lt;- as.data.frame(rfm_results$rfm_data) \n  \n  # Select important columns only\n  rfm_data &lt;- rfm_data[, c(\"supplier_name\", \"recency\", \"frequency\", \"monetary\", \n                          \"avg_contract\", \"r_score\", \"f_score\", \"m_score\",\n                          \"rfm_score\", \"segment\", \"financial_category\")]\n  \n  # Create the datatable\n  dt &lt;- datatable(\n    rfm_data,\n    caption = \"Supplier RFM Analysis and Segmentation\",\n    colnames = c(\n      \"Supplier\" = \"supplier_name\",\n      \"Recency (Days)\" = \"recency\",\n      \"Frequency\" = \"frequency\",\n      \"Value ($)\" = \"monetary\",\n      \"Avg Contract ($)\" = \"avg_contract\",\n      \"R\" = \"r_score\", \n      \"F\" = \"f_score\", \n      \"M\" = \"m_score\",\n      \"RFM Score\" = \"rfm_score\",\n      \"Segment\" = \"segment\",\n      \"Financial Category\" = \"financial_category\"\n    ),\n    options = list(\n      pageLength = 25,\n      scrollX = TRUE,\n      scrollY = 400\n    ),\n    escape = FALSE  # Try this to prevent escape-related errors\n  )\n  \n  # Add number formatting - Use the NEW column names after datatable() renames them\n  dt &lt;- dt %&gt;% \n    formatRound(columns = c(\"Recency (Days)\", \"Value ($)\", \"Avg Contract ($)\"), digits = 0)\n  \n  return(dt)\n}\n\n\n\n7.3 Demonstrating RFM Analysis with Different Parameters\nNow let’s demonstrate the RFM analysis with different parameter settings, starting with a baseline analysis of all suppliers:\n\nHeatmapBubble ChartSegment DistributionSegment SummaryInteractive Table\n\n\n\n# First create the RFM analysis results\nrfm_results_all &lt;- create_rfm_analysis(\n  data = procurement_enriched,\n  min_monetary_value = 100000,  # Focus on more significant contracts\n  max_suppliers = 50  # Limit for visualization\n)\n\n# Create the RFM heatmap\nrfm_heatmap &lt;- create_rfm_heatmap(\n  rfm_results_all,\n  title = \"RFM Heatmap: Supplier Distribution\"\n)\n\n# Display the heatmap\nrfm_heatmap\n\n\n\n\n\n\n\n\n# Create the bubble chart visualization\nrfm_bubble &lt;- create_rfm_bubble_chart(rfm_results_all)\n\n# Display the bubble chart\nrfm_bubble\n\n\n\n\n\n\n\n\n# Create the segment distribution visualization\nsegment_dist &lt;- create_segment_distribution(rfm_results_all)\n\n# Display the segment distribution\nsegment_dist\n\n\n\n\n\n\n\n\n# Display the RFM segment summary\nprint(rfm_results_all$segment_summary)\n\n# A tibble: 9 × 8\n  segment             count pct_suppliers avg_recency avg_frequency avg_monetary\n  &lt;chr&gt;               &lt;int&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Top Suppliers        1025          25.1         180           8.3     53512573\n2 High Value Regular    340           8.3         855           3.8     65480894\n3 Big Spenders          456          11.2        1162           1.2     38518663\n4 Big-Ticket Recent     221           5.4         245           1.4     35111131\n5 Average Contractors   772          18.9        1097           1.1       372960\n6 Active Contractors    400           9.8         211           4.2       435952\n7 Recent Contractors    396           9.7         253           1.2       361128\n8 Regular Contractors   277           6.8         923           2.8       434530\n9 Least Engaged         199           4.9        1469           1         187380\n# ℹ 2 more variables: total_monetary &lt;dbl&gt;, pct_spend &lt;dbl&gt;\n\n\n\n\n\n# Create the interactive datatable\nrfm_table &lt;- create_rfm_datatable(rfm_results_all)\n\n# Display the interactive table\nrfm_table\n\n\n\n\n\n\n\n\n\n\n7.4 Ministry Level Demonstration\nMinistry-specific RFM analysis examples.\n\nMOE HeatmapMOE Bubble ChartMOE Segment Distribution\n\n\n\n# Analyze suppliers working with Ministry of Education\nmoe_rfm_results &lt;- create_rfm_analysis(\n  data = procurement_enriched,\n  ministry_filter = \"MINISTRY OF EDUCATION\",\n  min_monetary_value = 50000,  # Lower threshold for ministry-specific analysis\n  debug = TRUE\n)\n# Create Ministry of Education heatmap with no subtitle\nmoe_rfm_heatmap &lt;- create_rfm_heatmap(\n  moe_rfm_results,\n  title = \"Ministry of Education: RFM Heatmap\"\n)\n# Display the MOE heatmap\nmoe_rfm_heatmap\n\n\n\n\n\n\n\n\n# Create the MOE bubble chart\nmoe_rfm_bubble &lt;- create_rfm_bubble_chart(\n  moe_rfm_results,\n  title = \"Ministry of Education: Supplier RFM Analysis\",\n  subtitle = \"Education supplier positioning by recency, frequency and value\",\n  color_palette = \"Dark2\"\n)\n# Display the MOE bubble chart\nmoe_rfm_bubble\n\n\n\n\n\n\n\n\n# Create the MOE segment distribution\nmoe_segment_dist &lt;- create_segment_distribution(\n  moe_rfm_results,\n  title = \"Ministry of Education: Supplier Segment Distribution\",\n  subtitle = \"Comparing education supplier count vs. procurement value by segment\"\n)\n# Display the MOE segment distribution\nmoe_segment_dist\n\n\n\n\n\n\n\n\n\n\n7.5 RFM Analysis: Financial Grade Integration\nDemonstrates integration of financial grade information with RFM analysis.\n\n# Run RFM analysis with financial grade focus\nfinancial_rfm_results &lt;- create_rfm_analysis(\n  data = procurement_enriched,\n  min_monetary_value = 100000,\n  include_financial_grade = TRUE\n)\n\n# Create financial grade vs. RFM segment heatmap with no subtitle\ngrade_segment_heatmap &lt;- create_grade_segment_heatmap(\n  financial_rfm_results,\n  title = \"Supplier Distribution: Financial Grade vs. RFM Segment\",\n)\n\n# Create a bubble chart with financial categories highlighted\nfinancial_bubble &lt;- create_rfm_bubble_chart(\n  financial_rfm_results,\n  title = \"Supplier RFM Analysis by Financial Category\",\n  subtitle = \"Showing relationship between financial grade and RFM position\"\n)\nfinancial_bubble\n\n\n\n\n# Create interactive datatable with financial grade information\nfinancial_rfm_table &lt;- create_rfm_datatable(\n  financial_rfm_results,\n  title = \"Supplier RFM Analysis with Financial Grades\"\n)\nfinancial_rfm_table"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3.html#ui-design-for-rfm-analysis-dashboard",
    "href": "Take-home_Ex/Take-home_Ex3.html#ui-design-for-rfm-analysis-dashboard",
    "title": "Take-home Exercise 3: GeBiz Procurement Data Visualization Prototype",
    "section": "8. UI Design for RFM Analysis Dashboard",
    "text": "8. UI Design for RFM Analysis Dashboard\nThe RFM Analysis Dashboard provides a comprehensive interface for analyzing supplier performance and segmentation through the lens of Recency, Frequency, and Monetary Value metrics. This powerful segmentation tool helps procurement officers identify key supplier groups and optimize engagement strategies.\n\n8.1 Dashboard Layout\nThe RFM Analysis Dashboard follows a structured layout optimized for both in-depth analysis and intuitive exploration:\n\nParameters & Control Panel (Left): Allows users to configure RFM analysis parameters and selection criteria\nVisualization Hub (Center): Houses multiple coordinated visualizations of RFM data\nSegment Insights Panel (Right): Provides detailed metrics and actionable insights for selected segments\n\nThis integrated layout ensures users can seamlessly move between parameter adjustment, visual exploration, and strategic insight extraction.\n\n\n8.2 Parameters & Control Panel\nThe left panel provides complete control over RFM analysis parameters:\n\nData Selection\n\nAnalysis Scope\n\nDate range with default to last complete fiscal year\nMinistry/Agency filter with multi-select capability\nSupplier category filter (All, Large, Medium, Small)\nMinimum contract value threshold slider\n\nRFM Calculation Parameters\n\nAnalysis reference date picker (defaults to current date)\nRecency weight adjustment (1-5)\nFrequency weight adjustment (1-5)\nMonetary weight adjustment (1-5)\nQuartile boundaries adjustment (3-5 segments per dimension)\n\nSegment Customization\n\nPredefined segment templates (Standard RFM, Value-focused, Engagement-focused)\nCustom segment definition interface\nSegment naming and color scheme selection\n\n\n\n\nOutput Configuration\n\nVisualization Controls\n\nChart type selector (Heatmap, Bubble, Distribution)\nDisplay density slider (data points per view)\nLabel density control\nInclude/exclude unclassified suppliers toggle\n\nExport Options\n\nSave configuration preset\nSchedule automated analysis\nExport format selection (PDF, Excel, Power BI)\n\n\n\n\n\n8.3 Visualization Hub\nThe central area presents coordinated, interactive visualizations:\n\nPrimary Visualization Area\n\nTabbed Interface with the following views:\n\nRFM Heatmap: Shows supplier distribution across RFM score combinations\nSegment Bubble Chart: Maps suppliers by recency, frequency with bubble size for monetary value\nDistribution Analysis: Bar charts comparing supplier counts vs. procurement value\nSegment Migration: Shows how suppliers move between segments over time\nFinancial Grade Correlation: Compares RFM segments with financial grades\n\nInteractive Features:\n\nDrill-down capabilities on all charts\nBrushing and linking across visualizations\nAnimation controls for temporal analysis\nHighlighting of selected segments across all views\n\n\n\n\nVisualization Toolbar\n\nView Controls: Zoom, pan, reset view\nSelection Tools: Rectangle select, lasso select\nComparative Tools: Segment comparison, benchmark overlay\nAnnotation Tools: Add notes, highlight insights\n\n\n\nKey Metrics Dashboard\n\nSummary Statistics Band:\n\nTotal suppliers analyzed\nTotal procurement value\nAverage RFM score\nSegment concentration index\nYear-over-year segment volatility\n\n\n\n\n\n8.4 Segment Insights Panel\nThe right panel delivers actionable insights about selected segments:\n\nSegment Profile\n\nSegment Header:\n\nSegment name and color coding\nSummary description of segment characteristics\nCount and percentage of total suppliers\nProcurement value and percentage of total\n\nRFM Profile:\n\nAverage recency (days since last contract)\nAverage frequency (contracts per year)\nAverage monetary value (per supplier)\nRFM score distribution within segment\n\n\n\n\nPerformance Metrics\n\nProcurement Pattern Analysis:\n\nContract size distribution\nTemporal pattern visualization\nAgency distribution\nContract type breakdown\n\nComparative Analysis:\n\nPerformance vs. other segments\nYear-over-year changes\nPerformance vs. financial grade expectations\nDistance from ideal supplier profile\n\n\n\n\nStrategic Recommendations\n\nEngagement Strategies:\n\nTailored engagement recommendations based on segment\nOpportunity identification\nRisk assessment\nPerformance improvement suggestions\n\nAction Planning:\n\nSuggested next steps for each segment\nPriority ranking of actions\nExpected impact metrics\nIntegration with procurement planning\n\n\n\n\n\n8.5 Advanced Interaction Features\nThe RFM Analysis Dashboard implements several advanced interaction features:\n\nSegment-Driven Filtering: Selections in segment visualizations automatically filter all other views\nParameter Impact Simulation: Interactive preview of how parameter changes affect segmentation\nThreshold Sensitivity Analysis: Shows stability of segments across different threshold settings\nTemporal Playback: Animated visualization of how supplier segments evolve over time\nPersonalized Insights: Context-aware recommendations based on selected segments and historical actions\n\n\n\n8.6 Integration with Other Modules\nThe RFM Analysis Dashboard seamlessly integrates with other system modules:\n\nSankey Flow Integration: Selected segments can be highlighted in procurement flow visualizations\nSupplier Profile Connection: Direct links to detailed supplier profiles for deeper analysis\nPredictive Analytics Bridge: Feed RFM segments into procurement forecasting and planning tools\nExport to Strategy: Direct export of segment insights to procurement strategy documents\nNotification System: Set up alerts for segment transitions or significant changes in supplier behavior"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1.html",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "",
    "text": "This dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n\nIn this exercise we will be:\n\nExamining Heart Attack Occurrence: Analyze the distribution and determinants of heart attack events across the dataset.\nConducting Demographic Analysis: Investigate how age, gender, and region contribute to heart attack risk, distinguishing between younger and older cohorts.\nExploring Health Metrics: Visualize relationships between clinical indicators (e.g., BMI, blood pressure, cholesterol) and heart attack occurrence.\nAssessing Lifestyle Factors: Evaluate the impact of lifestyle variables such as smoking history, physical activity, diet quality, alcohol consumption, and stress levels on heart health."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1.html#overview",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "",
    "text": "This dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n\nIn this exercise we will be:\n\nExamining Heart Attack Occurrence: Analyze the distribution and determinants of heart attack events across the dataset.\nConducting Demographic Analysis: Investigate how age, gender, and region contribute to heart attack risk, distinguishing between younger and older cohorts.\nExploring Health Metrics: Visualize relationships between clinical indicators (e.g., BMI, blood pressure, cholesterol) and heart attack occurrence.\nAssessing Lifestyle Factors: Evaluate the impact of lifestyle variables such as smoking history, physical activity, diet quality, alcohol consumption, and stress levels on heart health."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex1.html#getting-started",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nWe load the following R packages using the pacman::p_load() function:\n\ntidyverse: Core collection of R packages for data wrangling and visualization (e.g., dplyr, ggplot2)\n\nSmartEDA: For the ExpData() function used in exploratory data analysis\n\neasystats: Specifically for check_collinearity() to diagnose multicollinearity issues\n\nreshape2: Provides the melt() function for reshaping data from wide to long format\n\ncaret: Functions for data partitioning (createDataPartition) and model training workflows\n\nyardstick: Offers conf_mat() and other classification metrics\n\npROC: For ROC curves and AUC calculations (roc, auc)\n\nGGally: For the ggpairs() function to create pairwise scatterplot matrices\n\nggmosaic: To create mosaic plots via geom_mosaic()\n\npatchwork: For arranging multiple ggplot figures into a composite layout\n\nxgboost: Gradient boosting library for classification and regression tasks\n\n\npacman::p_load(tidyverse, SmartEDA, easystats, reshape2, caret, yardstick, pROC, GGally, ggmosaic, patchwork, xgboost)\n\nThis dataset contains information about heart attack occurrences in Japan, focusing on various demographic and health-related factors.\n\n\nImport data\n\nheart_data &lt;- read_csv(\"./data/japan_heart_attack_dataset.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#data-pre-processing",
    "href": "Take-home_Ex/Take-home_Ex1.html#data-pre-processing",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Data pre-processing",
    "text": "Data pre-processing\n\nGlimpse of data\nUsing the glimpse() function, we see that the dataset consists of 30,000 rows and 32 columns. The output displays the column names, their data types, and the first few entries for each variable. Additionally, there are 15 extra columns (Extra_Column_1 to Extra_Column_15) which are not clearly defined.\n\nglimpse(heart_data)\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\nThe following provides an overview of the Japan Heart Attack dataset using the ExpData() function, summarizing both overall and variable-level details.\n\nOverall data summaryVariable level summary\n\n\n\nsummary1 &lt;- heart_data %&gt;%\n  ExpData(type = 1)\n\n# Display the summary (further customization possible)\nsummary1\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)     30000\n2                              No. of variables (ncol)        32\n3                    No. of numeric/interger variables        22\n4                              No. of factor variables         0\n5                                No. of text variables        10\n6                             No. of logical variables         0\n7                          No. of identifier variables        20\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         0\n10               %. of variables having complete cases 100% (32)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\n\nsummary2 &lt;- heart_data %&gt;%\n  ExpData(type = 2)\n\n# Display the summary (further customization possible)\nsummary2\n\n   Index           Variable_Name Variable_Type Sample_n Missing_Count\n1      1                     Age       numeric    30000             0\n2      2                  Gender     character    30000             0\n3      3                  Region     character    30000             0\n4      4         Smoking_History     character    30000             0\n5      5        Diabetes_History     character    30000             0\n6      6    Hypertension_History     character    30000             0\n7      7       Cholesterol_Level       numeric    30000             0\n8      8       Physical_Activity     character    30000             0\n9      9            Diet_Quality     character    30000             0\n10    10     Alcohol_Consumption     character    30000             0\n11    11           Stress_Levels       numeric    30000             0\n12    12                     BMI       numeric    30000             0\n13    13              Heart_Rate       numeric    30000             0\n14    14             Systolic_BP       numeric    30000             0\n15    15            Diastolic_BP       numeric    30000             0\n16    16          Family_History     character    30000             0\n17    17 Heart_Attack_Occurrence     character    30000             0\n18    18          Extra_Column_1       numeric    30000             0\n19    19          Extra_Column_2       numeric    30000             0\n20    20          Extra_Column_3       numeric    30000             0\n21    21          Extra_Column_4       numeric    30000             0\n22    22          Extra_Column_5       numeric    30000             0\n23    23          Extra_Column_6       numeric    30000             0\n24    24          Extra_Column_7       numeric    30000             0\n25    25          Extra_Column_8       numeric    30000             0\n26    26          Extra_Column_9       numeric    30000             0\n27    27         Extra_Column_10       numeric    30000             0\n28    28         Extra_Column_11       numeric    30000             0\n29    29         Extra_Column_12       numeric    30000             0\n30    30         Extra_Column_13       numeric    30000             0\n31    31         Extra_Column_14       numeric    30000             0\n32    32         Extra_Column_15       numeric    30000             0\n   Per_of_Missing No_of_distinct_values\n1               0                    62\n2               0                     2\n3               0                     2\n4               0                     2\n5               0                     2\n6               0                     2\n7               0                 30000\n8               0                     3\n9               0                     3\n10              0                     4\n11              0                 29613\n12              0                 30000\n13              0                 30000\n14              0                 30000\n15              0                 30000\n16              0                     2\n17              0                     2\n18              0                 30000\n19              0                 30000\n20              0                 30000\n21              0                 30000\n22              0                 30000\n23              0                 30000\n24              0                 30000\n25              0                 30000\n26              0                 30000\n27              0                 30000\n28              0                 30000\n29              0                 30000\n30              0                 30000\n31              0                 30000\n32              0                 30000\n\n\n\n\n\n\n\nConvert categorical variables to factors\nFrom the overview above, we see that the dataset contains no missing values, and the categorical variables have a maximum of 4 unique values. Converting these variables into factors ensures they are correctly treated as categorical data during analysis and visualization.\n\n# Convert selected categorical variables into factors\nheart_data &lt;- heart_data %&gt;%\n  mutate(\n    Gender = as.factor(Gender),\n    Region = as.factor(Region),\n    Smoking_History = as.factor(Smoking_History),\n    Diabetes_History = as.factor(Diabetes_History),\n    Hypertension_History = as.factor(Hypertension_History),\n    Physical_Activity = as.factor(Physical_Activity),\n    Diet_Quality = as.factor(Diet_Quality),\n    Alcohol_Consumption = as.factor(Alcohol_Consumption),\n    Family_History = as.factor(Family_History),\n    Heart_Attack_Occurrence = as.factor(Heart_Attack_Occurrence)\n  )\n\n\n\nDrop extra columns\n\n# Select only the Extra_Columns and the outcome variable\nextra_data &lt;- heart_data %&gt;%\n  select(starts_with(\"Extra_Column_\"), Heart_Attack_Occurrence)\n\n# Reshape to long format\nextra_data_long &lt;- melt(extra_data, id.vars = \"Heart_Attack_Occurrence\")\n\n# Create boxplots comparing each Extra_Column by Heart_Attack_Occurrence\nggplot(extra_data_long, aes(x = Heart_Attack_Occurrence, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = \"free\") +\n  labs(\n    title = \"Distribution of Extra Columns by Heart Attack Occurrence\",\n    x = \"Heart Attack Occurrence\",\n    y = \"Value\"\n  )\n\n\n\n\n\n\n\n\nSince these variables do not appear to vary by heart attack status, they are unlikely to provide useful information for any downstream analysis (e.g., modeling, hypothesis testing). Dropping them will simplify the dataset and help focus on variables that do relate to heart attack risk.\nWe can drop them with the following code:\n\nheart_data &lt;- heart_data %&gt;%\n  select(-starts_with(\"Extra_Column_\"))\n\n\n\nCleaned dataset\n\nglimpse(heart_data)\n\nRows: 30,000\nColumns: 17\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;fct&gt; Male, Male, Male, Female, Female, Female, Male…\n$ Region                  &lt;fct&gt; Urban, Urban, Rural, Urban, Rural, Rural, Urba…\n$ Smoking_History         &lt;fct&gt; Yes, No, Yes, No, No, No, No, Yes, No, No, No,…\n$ Diabetes_History        &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, No, N…\n$ Hypertension_History    &lt;fct&gt; No, No, No, No, No, No, Yes, No, Yes, No, Yes,…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;fct&gt; Moderate, Low, Low, Moderate, High, Low, High,…\n$ Diet_Quality            &lt;fct&gt; Poor, Good, Average, Good, Good, Good, Poor, P…\n$ Alcohol_Consumption     &lt;fct&gt; Low, Low, Moderate, High, High, High, High, No…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, Yes,…\n$ Heart_Attack_Occurrence &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, No, N…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#exploratory-visuals",
    "href": "Take-home_Ex/Take-home_Ex1.html#exploratory-visuals",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Exploratory visuals",
    "text": "Exploratory visuals\n\nCreate new variables\nWe create a new variable, Age_Group, classifying individuals as “Over50” or “50OrBelow” to compare younger vs. older individuals.\n\nheart_data_eda &lt;- heart_data %&gt;%\n  mutate(Age_Group = ifelse(Age &gt; 50, \"Over50\", \"50OrBelow\") %&gt;% as.factor())\n\nWe create AgeGender by combining the Age_Group and gender. We also combine smoking status and physical activity into SmokeAct and reorder alcohol consumption levels.\n\n# Demographic variables\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    AgeGender = case_when(\n      Age_Group == \"Over50\" & Gender == \"Male\"   ~ \"Over 50 Male\",\n      Age_Group == \"Over50\" & Gender == \"Female\" ~ \"Over 50 Female\",\n      Age_Group == \"50OrBelow\" & Gender == \"Male\"   ~ \"≤50 Male\",\n      Age_Group == \"50OrBelow\" & Gender == \"Female\" ~ \"≤50 Female\"\n    ) %&gt;% factor(levels = c(\"≤50 Female\",\"≤50 Male\",\"Over 50 Female\",\"Over 50 Male\"))\n  )\n\n# Lifestyle variables\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    SmokeAct = case_when(\n      Smoking_History == \"Yes\" & Physical_Activity == \"Low\"      ~ \"Smoker, PA:Low\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"Moderate\" ~ \"Smoker, PA:Mod\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"High\"     ~ \"Smoker, PA:High\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Low\"      ~ \"Non-Smoker, PA:Low\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Moderate\" ~ \"Non-Smoker, PA:Mod\",\n      Smoking_History == \"No\"  & Physical_Activity == \"High\"     ~ \"Non-Smoker, PA:High\"\n    ) %&gt;% \n    # Order them in a sensible sequence:\n    factor(levels = c(\"Non-Smoker, PA:Low\",\"Non-Smoker, PA:Mod\",\"Non-Smoker, PA:High\",\n                      \"Smoker, PA:Low\",\"Smoker, PA:Mod\",\"Smoker, PA:High\"))\n  )\n\n\n\nMosaic Plot: Demographic Analysis\nWe plot a mosaic where AgeGender is on the x-axis, color indicates heart attack occurrence, and each facet represents a different region.\n\np_demo &lt;- ggplot(heart_data_eda) +\n  geom_mosaic(\n    aes(x = product(AgeGender),\n        fill = Heart_Attack_Occurrence,\n        text = paste0(\"Group: \", AgeGender,\n                      \"&lt;br&gt;Region: \", Region,\n                      \"&lt;br&gt;Heart Attack: \", Heart_Attack_Occurrence)\n    ),\n    alpha = 0.9\n  ) +\n  facet_wrap(~ Region) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Demographic Mosaic: Age & Gender by Region vs. Heart Attack\",\n    x     = \"Age & Gender\",\n    y     = \" \",\n    fill  = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\np_demo\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis mosaic plot illustrates heart attack occurrences across different age and gender groups within rural and urban regions. The width of each bar segment corresponds to the relative size of that demographic group, while the height indicates the proportion of individuals who experienced a heart attack.\nOverall, heart attack rates remain relatively consistent between rural and urban areas. However, males tend to have a higher probability of heart attack than females, regardless of age or region.\n\n\n\nMosaic plot: Lifestyle factors\nWe create a mosaic plot with SmokeAct on the x-axis, color by heart attack occurrence, and facet by the four alcohol consumption levels.\n\n# Reorder factor levels for Alcohol_Consumption\nheart_data_eda &lt;- heart_data_eda %&gt;%\n  mutate(\n    Alcohol_Consumption = factor(\n      Alcohol_Consumption,\n      levels = c(\"High\", \"Moderate\", \"Low\", \"None\")\n    )\n  )\n\nggplot(heart_data_eda) +\n  geom_mosaic(aes(\n    x    = product(SmokeAct),\n    fill = Heart_Attack_Occurrence\n  ), alpha = 0.9) +\n  facet_wrap(~ Alcohol_Consumption, ncol = 2) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Lifestyle Mosaic: Smoking, Activity, and Alcohol vs. Heart Attack\",\n    subtitle = \"PA = Physical Activity. Each facet represents a different Alcohol Consumption level.\",\n    x = \"Smoking & PA Group\",\n    y = \"\",\n    fill = \"Heart Attack\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 14, hjust=0.5),\n    plot.subtitle = element_text(size = 10, hjust=0.5),\n    strip.text    = element_text(face=\"bold\"),\n    axis.text.x   = element_text(angle=40, hjust=1, size=7),\n    panel.spacing = unit(2, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis mosaic plot explores how smoking, physical activity (PA), and alcohol consumption interact to influence heart attack occurrences. Each facet represents a different alcohol consumption level (High, Moderate, Low, None).\nInterestingly, non-smokers who report no alcohol consumption but high physical activity exhibit one of the highest heart attack rates. Additionally, smokers with moderate physical activity tend to have higher heart attack rates compared to smokers with low or high physical activity.\n\n\n\nPairwise numeric plot (Health metrics)\nThis code uses ggpairs() to create a matrix of pairwise plots for all numeric variables in heart_data. The mapping = aes(color = Heart_Attack_Occurrence) argument adds a color-coded grouping by heart attack status.\n\n# Automatically select all numeric columns from the dataset\nnumeric_cols &lt;- sapply(heart_data, is.numeric)\n\npairwise_plot &lt;- ggpairs(\n  data = heart_data,\n  columns = which(numeric_cols),\n  mapping = aes(color = Heart_Attack_Occurrence),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, size = 0.5)),\n  diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5)),\n  upper = list(continuous = wrap(\"cor\", size = 4))\n) +\n  ggtitle(\"Pairwise Correlations Among All Numeric Metrics\")\n\npairwise_plot\n\n\n\n\n\n\n\n\n\nExplanation of the plot\nThis grid compares health metrics like BMI, blood pressure, cholesterol, and stress. The diagonal panels show density curves for each variable, revealing, for instance, that Age has a broader distribution compared to the other variables.\nThe upper panels list correlation coefficients and their significance, most of which are near zero (e.g., Corr: 0.025, 0.048), indicating that these variables do not strongly co-vary. In the lower scatter plots, points are colored by heart attack occurrence; no tight clustering suggests no single numeric threshold exclusively separates “Yes” vs. “No.” For instance, Systolic_BP and Diastolic_BP show little correlation as high Systolic_BP often coexists with both high and low Diastolic_BP. Overall, no single numeric factor stands out as a strictly linear driver of heart attack, though there may be subtle nonlinear or interactive effects to explore later."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#train-test-split",
    "href": "Take-home_Ex/Take-home_Ex1.html#train-test-split",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Train test split",
    "text": "Train test split\nBefore building a predictive model, it is best practice to split the data into training and testing sets. The createDataPartition function ensures that the distribution of the target class is approximately the same in both sets. Here, we allocate 80% of the data for training and 20% for testing.\n\nset.seed(123)\n\ntrain_index &lt;- createDataPartition(heart_data$Heart_Attack_Occurrence, p = 0.8, list = FALSE)\n\ntrain_data &lt;- heart_data[train_index, ]\ntest_data  &lt;- heart_data[-train_index, ]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#naive-logistic-regression",
    "href": "Take-home_Ex/Take-home_Ex1.html#naive-logistic-regression",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Naive logistic regression",
    "text": "Naive logistic regression\nHere, we build an initial (“naive”) logistic regression model that includes all available predictors (except the 15 “Extra_Column” variables we dropped). This approach gives us a baseline.\n\nFit the model\nWe will fit a logistic regression using glm().\n\n# Use a standard glm with all predictors\nnaive_glm &lt;- glm(\n  Heart_Attack_Occurrence ~ .,\n  data   = train_data,\n  family = binomial\n)\n\n\n\nUnderstanding the model\nWe use check_collinearity() to see if any variables are highly correlated or cause near‐complete separation. A “good” logistic regression typically avoids extremely high VIFs or indefinite confidence intervals.\n\n# Capture the output\nresult &lt;- check_collinearity(naive_glm)\n\n# Coerce to a data frame\ndf &lt;- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n\n\nCheck for Multicollinearity\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nAge\n1.001325\n1.000000\n1.991615e+01\n1.000662\n0.9986771\n0.0502105\n0.9999999\n\n\nGender\n1.000943\n1.000000\n6.378053e+02\n1.000472\n0.9990575\n0.0015679\n1.0000000\n\n\nRegion\n1.000670\n1.000000\n1.062715e+05\n1.000335\n0.9993302\n0.0000094\n1.0000000\n\n\nSmoking_History\n1.000446\n1.000000\n9.589080e+08\n1.000223\n0.9995547\n0.0000000\n1.0000000\n\n\nDiabetes_History\n1.001228\n1.000000\n3.808479e+01\n1.000614\n0.9987733\n0.0262572\n1.0000000\n\n\nHypertension_History\n1.000834\n1.000000\n3.250209e+03\n1.000417\n0.9991664\n0.0003077\n1.0000000\n\n\nCholesterol_Level\n1.000587\n1.000000\n1.351909e+06\n1.000293\n0.9994134\n0.0000007\n1.0000000\n\n\nPhysical_Activity\n1.001852\n1.000002\n2.745436e+00\n1.000926\n0.9981510\n0.3642408\n0.9999980\n\n\nDiet_Quality\n1.001445\n1.000000\n1.030279e+01\n1.000722\n0.9985567\n0.0970611\n0.9999998\n\n\nAlcohol_Consumption\n1.002116\n1.000005\n1.852209e+00\n1.001057\n0.9978885\n0.5398958\n0.9999947\n\n\nStress_Levels\n1.000919\n1.000000\n8.893873e+02\n1.000459\n0.9990821\n0.0011244\n1.0000000\n\n\nBMI\n1.000849\n1.000000\n2.561245e+03\n1.000424\n0.9991522\n0.0003904\n1.0000000\n\n\nHeart_Rate\n1.000834\n1.000000\n3.252876e+03\n1.000417\n0.9991665\n0.0003074\n1.0000000\n\n\nSystolic_BP\n1.000939\n1.000000\n6.753992e+02\n1.000469\n0.9990618\n0.0014806\n1.0000000\n\n\nDiastolic_BP\n1.000920\n1.000000\n8.726101e+02\n1.000460\n0.9990807\n0.0011460\n1.0000000\n\n\nFamily_History\n1.000826\n1.000000\n3.716482e+03\n1.000413\n0.9991743\n0.0002691\n1.0000000\n\n\n\n\n\n\n\n\nInterpreting the collinearity results\n\nVIF ~1.0 but extremely large upper confidence bounds: This indicates the algorithm is unsure about the exact magnitude of possible collinearity. In simpler terms, the model’s variance–covariance matrix is nearly singular.\nThis often happens when:\n\nQuasi‐complete separation: Certain variables or combinations nearly “perfectly” predict the outcome.\nImbalance in the dataset (many more “No” than “Yes”) plus insufficient signal in some predictors.\nOver‐parametrization: Too many correlated predictors for the sample size.\n\n\n\n\n\nModel performance\n\n# 1) Collinearity plot\ncheck_c &lt;- check_collinearity(naive_glm)\np_collinearity &lt;- plot(check_c) +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\n# 2) Confusion matrix heatmap\npred_prob_naive &lt;- predict(naive_glm, newdata = test_data, type = \"response\")\n\npred_class_naive &lt;- ifelse(pred_prob_naive &gt;= 0.5, \"Yes\", \"No\") %&gt;%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\n\n# Evaluate\nnaive_results &lt;- data.frame(\n  obs   = test_data$Heart_Attack_Occurrence,\n  pred  = pred_class_naive,\n  prob  = pred_prob_naive\n)\n\nnaive_cm &lt;- naive_results %&gt;%\n  conf_mat(obs, pred)\n\np_confmat &lt;- autoplot(naive_cm, type = \"heatmap\") +\n  labs(title = \"Naive Logistic Regression: Confusion Matrix\")\n\n# 3) ROC curve as a ggplot object using ggroc()\nroc_naive &lt;- roc(\n  response  = as.numeric(naive_results$obs),\n  predictor = as.numeric(naive_results$prob)\n)\n\np_roc &lt;- ggroc(roc_naive, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Naïve Logistic Model\") +\n  theme_minimal()\n\n\ncombined_plot &lt;- p_collinearity / (p_confmat + p_roc)\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\nExplanation of the plot\n\nCollinearity Plot:\n\n\n\nThis bar chart displays VIF estimates, where the ideal values fall in the green region.\nIndividual VIF point estimates hover around 1.0, but their upper confidence intervals extend into the red, indicating extremely high values.\nThis suggests that the model’s parameter estimates are unstable due to quasi‐complete separation or an excess of correlated predictors relative to the sample size. In essence, the model cannot reliably discern each variable’s true contribution, leading to artificially low VIF point estimates paired with massive uncertainty bounds.\n\n\nConfusion Matrix:\n\n\n\nThe matrix shows 0 true positives.\nThis is typical when a logistic model either encounters near-complete separation or opts to disregard the minority class in imbalanced datasets.\n\n\nROC Curve:\n\n\n\nThe ROC curve lies near the diagonal reference line, confirming that the model lacks predictive power and is essentially guessing."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#improving-logistic-regression",
    "href": "Take-home_Ex/Take-home_Ex1.html#improving-logistic-regression",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Improving logistic regression",
    "text": "Improving logistic regression\n\nRationale\nOur naïve logistic regression suggested potential issues: - Very low sensitivity (predicting all “No”) - Large variance inflation factor (VIF) intervals\nTherefore, we refine the logistic model by:\n\nUse weighted logistic regression to handle class imbalance,\n\nIncorporate mild non-linear terms for BMI, Systolic_BP, and Diastolic_BP with polynomial expansions,\n\nRemove redundant variables.\n\n\nnew_train &lt;- train_data\nnew_test  &lt;- test_data\n\n\nnew_formula &lt;- as.formula(\n  \"Heart_Attack_Occurrence ~ Age + Gender + Family_History + poly(BMI, 2, raw=TRUE) + \n   Heart_Rate + poly(Systolic_BP, 2, raw=TRUE) + poly(Diastolic_BP, 2, raw=TRUE) + Cholesterol_Level + \n   Diabetes_History + Hypertension_History + Physical_Activity + Smoking_History + \n   Diet_Quality + Alcohol_Consumption + Stress_Levels\"\n)\n\n\nCreating observation weights\nWe create balanced weights to give more importance to the minority class. This ensures misclassifying a minority‐class “Yes” is penalized more strongly than misclassifying a “No.”\n\nn_yes &lt;- sum(new_train$Heart_Attack_Occurrence == \"Yes\")\nn_no  &lt;- sum(new_train$Heart_Attack_Occurrence == \"No\")\nN     &lt;- n_yes + n_no\n\nw_yes &lt;- N / (2 * n_yes)\nw_no  &lt;- N / (2 * n_no)\n\n# Assign weights in the new training dataset\nnew_train$weights_col &lt;- ifelse(\n  new_train$Heart_Attack_Occurrence == \"Yes\",\n  w_yes,\n  w_no\n)\n\n\n\n\nFit weighted logistic model\n\nmodel_glm_weighted &lt;- glm(\n  formula = new_formula,\n  data    = new_train,\n  family  = binomial(link = \"logit\"),\n  weights = weights_col\n)\n\n\n# Capture the output\nresult &lt;- check_collinearity(model_glm_weighted)\n\n# Coerce to a data frame\ndf &lt;- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n\n\nCheck for Multicollinearity\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nAge\n1.004556\n1.000277\n1.074857\n1.002275\n0.9954646\n0.9303566\n0.9997228\n\n\nGender\n1.003347\n1.000075\n1.149809\n1.001672\n0.9966644\n0.8697096\n0.9999252\n\n\nFamily_History\n1.001847\n1.000002\n2.774072\n1.000923\n0.9981564\n0.3604809\n0.9999981\n\n\npoly(BMI, 2, raw = TRUE)\n1.003982\n1.000162\n1.097600\n1.001989\n0.9960340\n0.9110785\n0.9998376\n\n\nHeart_Rate\n1.002266\n1.000008\n1.614256\n1.001132\n0.9977391\n0.6194804\n0.9999916\n\n\npoly(Systolic_BP, 2, raw = TRUE)\n1.005107\n1.000419\n1.062202\n1.002550\n0.9949186\n0.9414402\n0.9995808\n\n\npoly(Diastolic_BP, 2, raw = TRUE)\n1.004244\n1.000211\n1.085491\n1.002120\n0.9957736\n0.9212419\n0.9997893\n\n\nCholesterol_Level\n1.001808\n1.000002\n3.013365\n1.000904\n0.9981953\n0.3318549\n0.9999984\n\n\nDiabetes_History\n1.002857\n1.000033\n1.244352\n1.001427\n0.9971512\n0.8036315\n0.9999666\n\n\nHypertension_History\n1.002289\n1.000009\n1.586196\n1.001144\n0.9977158\n0.6304391\n0.9999911\n\n\nPhysical_Activity\n1.004614\n1.000291\n1.073216\n1.002304\n0.9954073\n0.9317786\n0.9997093\n\n\nSmoking_History\n1.002518\n1.000016\n1.390398\n1.001258\n0.9974880\n0.7192187\n0.9999838\n\n\nDiet_Quality\n1.003581\n1.000102\n1.125179\n1.001789\n0.9964314\n0.8887471\n0.9998975\n\n\nAlcohol_Consumption\n1.007005\n1.001124\n1.043643\n1.003496\n0.9930440\n0.9581821\n0.9988770\n\n\nStress_Levels\n1.002948\n1.000039\n1.219987\n1.001473\n0.9970609\n0.8196808\n0.9999605\n\n\n\n\n\n\n\n\n\nVisualizing the model\n\npred_prob_improved &lt;- predict(model_glm_weighted, newdata = test_data, type = \"response\")\n\n\nggplot(mapping = aes(x = pred_prob_improved)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Distribution of Predicted Probabilities (Naïve Logistic)\",\n       x = \"Predicted Probability of Heart Attack\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nFrom the histogram above, we can see that the model is overpredicting positive cases at 0.50 threshold. As the positive cases is approximately 10% of the dataset, we will take a threshold of 0.55 instead.\n\ncheck_c &lt;- check_collinearity(model_glm_weighted)\np_collinearity &lt;- plot(check_c) +\n  labs(title = \"Collinearity of Weighted Logistic Model\") +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\npred_prob_improved &lt;- predict(model_glm_weighted, newdata = test_data, type = \"response\")\npred_class_improved &lt;- ifelse(pred_prob_improved &gt;= 0.55, \"Yes\", \"No\") %&gt;%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\nimproved_cm &lt;- data.frame(\n  obs  = test_data$Heart_Attack_Occurrence,\n  pred = pred_class_improved\n) %&gt;% conf_mat(obs, pred)\n\np_confmat &lt;- autoplot(improved_cm, type = \"heatmap\") +\n  labs(title = \"Weighted Logistic: Confusion Matrix\")\n\nroc_improved &lt;- roc(\n  response  = as.numeric(test_data$Heart_Attack_Occurrence),\n  predictor = as.numeric(pred_prob_improved)\n)\np_roc &lt;- ggroc(roc_improved, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Weighted Logistic Model\") +\n  theme_minimal()\n\n## Combine the three plots\nlibrary(patchwork)\ncombined_plot &lt;- p_collinearity / (p_confmat + p_roc)\ncombined_plot\n\n\n\n\n\n\n\n\n\nWeighted Logistic Regression Results\nAfter applying class weights and mild polynomial terms to BMI and Blood Pressure, our weighted logistic model shows some improvements compared to the naïve model:\n\nSensitivity Improves: The model now predicts a small number of “Yes” cases rather than labeling everything “No.”\nCollinearity appears more stable\n\nThe VIF plot still shows point estimates around 1.0 for most variables, with moderate spikes in confidence intervals for a few. However, these are less extreme than in the naïve model, suggesting the parameter estimates are more stable overall.\n\nLittle change to overall accuracy (AUC)\n\nWhile the model does somewhat better at identifying positives, the ROC curve remains fairly close to the diagonal, reflecting an AUC only slightly better than 0.5.\nIn other words, the model is still not very accurate overall, indicating that the available predictors may not strongly discriminate between “Yes” and “No”—or that we need further refinements (e.g., more complex interactions, alternative transformations, or additional data)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#fitting-a-xgboost",
    "href": "Take-home_Ex/Take-home_Ex1.html#fitting-a-xgboost",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Fitting a xgboost",
    "text": "Fitting a xgboost\n\nData preparation\nWe first convert the outcome Heart_Attack_Occurrence to a 0/1 numeric variable. Then, we build model matrices using model.matrix() which transforms both categorical and numeric predictors into a suitable format for XGBoost.\n\n# Convert outcome to 0/1\ntrain_data_xgb &lt;- train_data %&gt;%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\nx_train &lt;- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=train_data_xgb)\ny_train &lt;- train_data_xgb$YesNo\n\ntest_data_xgb &lt;- test_data %&gt;%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\nx_test &lt;- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=test_data_xgb)\ny_test &lt;- test_data_xgb$YesNo\n\nn_yes &lt;- sum(y_train == 1)\nn_no  &lt;- sum(y_train == 0)\nscale_pos &lt;- n_no / n_yes\n\n\n\nXGBoost model training\nWe create DMatrix objects for both training and test sets, then specify key hyperparameters like max_depth, eta, and scale_pos_weight. The model is trained with early stopping if the test AUC does not improve after a certain number of rounds.\n\ndtrain &lt;- xgb.DMatrix(data = x_train, label = y_train)\ndtest  &lt;- xgb.DMatrix(data = x_test,  label = y_test)\n\nparam &lt;- list(\n  objective        = \"binary:logistic\",\n  eval_metric      = \"auc\",             # can also track \"error\" or \"logloss\"\n  max_depth        = 10,\n  eta              = 0.2,\n  scale_pos_weight = scale_pos          # imbalance correction\n)\n\n# Train with 100 rounds\nset.seed(123)\nxgb_model &lt;- xgb.train(\n  params   = param,\n  data     = dtrain,\n  nrounds  = 1000,\n  watchlist= list(train=dtrain, test=dtest),\n  early_stopping_rounds = 50,  # optional, for early stop\n  print_every_n          = 10\n)\n\n[1] train-auc:0.703267  test-auc:0.493556 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 50 rounds.\n\n[11]    train-auc:0.893593  test-auc:0.519498 \n[21]    train-auc:0.955144  test-auc:0.513634 \n[31]    train-auc:0.977267  test-auc:0.519925 \n[41]    train-auc:0.985509  test-auc:0.514492 \n[51]    train-auc:0.993414  test-auc:0.505698 \n[61]    train-auc:0.996438  test-auc:0.503955 \n[71]    train-auc:0.998506  test-auc:0.502356 \nStopping. Best iteration:\n[28]    train-auc:0.972693  test-auc:0.525233\n\n\nTraining AUC may reach near 1.0 (overfitting), but test AUC remains around 0.52, implying the model struggles to find a robust pattern. However, this may also indicate a difference in distribution of the training and testing dataset, and that no robust pattern is learnable from these features. There may also be insufficient predictive signal in the data.\n\n\nModel evaluation\n\npred_prob_xgb &lt;- predict(xgb_model, newdata = dtest, iteration_range = xgb_model$best_iteration)\n\npred_class_xgb &lt;- ifelse(pred_prob_xgb &gt;= 0.5, 1, 0)\n\nresults_xgb &lt;- data.frame(\n  obs  = factor(y_test, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  pred = factor(pred_class_xgb, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  prob = pred_prob_xgb\n)\n\nxgb_cm &lt;- conf_mat(results_xgb, truth=obs, estimate=pred)\nxgb_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.763 \n 2 kap                  binary        0.0124\n 3 sens                 binary        0.826 \n 4 spec                 binary        0.191 \n 5 ppv                  binary        0.903 \n 6 npv                  binary        0.107 \n 7 mcc                  binary        0.0131\n 8 j_index              binary        0.0167\n 9 bal_accuracy         binary        0.508 \n10 detection_prevalence binary        0.824 \n11 precision            binary        0.903 \n12 recall               binary        0.826 \n13 f_meas               binary        0.863 \n\n\n\n\nROC curve and confusion matrix\n\np_confmat &lt;- autoplot(xgb_cm, type=\"heatmap\") +\n  labs(title=\"XGBoost Confusion Matrix\", fill=\"Count\")\n\nroc_xgb &lt;- roc(response = as.numeric(results_xgb$obs), predictor = results_xgb$prob)\nauc_val &lt;- auc(roc_xgb)\n\np_roc &lt;- ggroc(roc_xgb, colour=\"#1c61b6\") +\n  labs(\n    title = paste0(\"XGBoost ROC Curve (AUC=\", round(auc_val,3), \")\"),\n    x     = \"1 - Specificity\",\n    y     = \"Sensitivity\"\n  ) +\n  theme_minimal()\n\n\n\ncombined_plot &lt;- p_confmat | p_roc\n\ncombined_plot\n\n\n\n\n\n\n\n\n\nPlot explanation:\n\nConfusion matrix: Shows how many “No” vs. “Yes” cases are classified correctly vs. incorrectly. Despite the class‐imbalance correction, the model still misclassifies most “Yes” events.\nROC curve: The curve hovers close to the diagonal, with an AUC near ~0.52. This is only marginally better than random guessing (AUC=0.5).\n\n\nimportance_xgb &lt;- xgb.importance(model = xgb_model)\nimportance_xgb  # see a data frame of feature importances\n\n                        Feature        Gain       Cover   Frequency\n                         &lt;char&gt;       &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n 1:           Cholesterol_Level 0.145242292 0.155522476 0.136697155\n 2:                 Systolic_BP 0.136727355 0.167277235 0.132375113\n 3:               Stress_Levels 0.134981157 0.185245402 0.134184340\n 4:                Diastolic_BP 0.134287572 0.160294404 0.126846919\n 5:                         BMI 0.131566444 0.126584570 0.123932053\n 6:                  Heart_Rate 0.123786314 0.121115391 0.121921801\n 7:                         Age 0.083851778 0.050432098 0.092371093\n 8:           Family_HistoryYes 0.010253500 0.002744379 0.011056388\n 9:   Physical_ActivityModerate 0.009907179 0.004210412 0.011659463\n10:                 RegionUrban 0.009613210 0.001768051 0.012262539\n11:          Smoking_HistoryYes 0.009402740 0.003311462 0.011357925\n12:     Hypertension_HistoryYes 0.009347006 0.001676643 0.010754850\n13:                  GenderMale 0.009129164 0.005765353 0.011357925\n14:         Diabetes_HistoryYes 0.008797618 0.001136887 0.009850236\n15:      Alcohol_ConsumptionLow 0.008275298 0.001408138 0.010553825\n16:            Diet_QualityGood 0.007770305 0.001091494 0.009146648\n17: Alcohol_ConsumptionModerate 0.007592931 0.001353723 0.009850236\n18:            Diet_QualityPoor 0.007177992 0.003669527 0.007940497\n19:     Alcohol_ConsumptionNone 0.006439186 0.004718671 0.007638959\n20:        Physical_ActivityLow 0.005850960 0.000673683 0.008242034\n                        Feature        Gain       Cover   Frequency\n\n# Plot\nxgb.plot.importance(importance_xgb, top_n = 15, \n                    main=\"XGBoost Feature Importance\")\n\n\n\n\n\n\n\n\n\n\nInterpretation:\nWhile certain features (e.g., Cholesterol_Level, Systolic_BP) rank highest in splitting power, the test AUC is still low. This suggests either insufficient predictive signal or potential overfitting to training noise."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1.html#final-thoughts",
    "href": "Take-home_Ex/Take-home_Ex1.html#final-thoughts",
    "title": "Take-home Exercise 1: Examining Heart Attack Risk in Japan",
    "section": "Final thoughts",
    "text": "Final thoughts\nAcross multiple approaches (naive logistic, weighted logistic, XGBoost), the models struggle to achieve predictive accuracy on the test set. This may indicate that the available features (after dropping the undefined extras) do not strongly distinguish between heart attack occurrences. Additional data, refined feature engineering, or domain expertise might be necessary to improve predictive performance. Nonetheless, the exploratory visualizations provide insights into demographic and lifestyle patterns associated with heart attack risk in Japan."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "This chapter explores techniques for modeling, analyzing and visualizing network data using R. Networks are powerful mathematical structures that represent relationships between entities, making them suitable for analyzing complex systems from social interactions to organizational communication patterns.\nCore learning objectives include:\n\nCreating graph object data frames with dplyr, lubridate, and tidygraph\nBuilding network visualizations using ggraph\nComputing network metrics to quantify structural properties\nCreating advanced graph visualizations incorporating network metrics\nDeveloping interactive network visualizations with visNetwork"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#overview",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#overview",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "This chapter explores techniques for modeling, analyzing and visualizing network data using R. Networks are powerful mathematical structures that represent relationships between entities, making them suitable for analyzing complex systems from social interactions to organizational communication patterns.\nCore learning objectives include:\n\nCreating graph object data frames with dplyr, lubridate, and tidygraph\nBuilding network visualizations using ggraph\nComputing network metrics to quantify structural properties\nCreating advanced graph visualizations incorporating network metrics\nDeveloping interactive network visualizations with visNetwork"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 Installing and launching R packages\nThe analysis requires several network-specific packages: igraph (for core network algorithms), tidygraph (for tidy network manipulation), ggraph (for network visualization), and visNetwork (for interactive visualization). The tidyverse and lubridate packages provide additional data manipulation capabilities.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#the-data",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#the-data",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "3. The Data",
    "text": "3. The Data\nThe dataset comes from an oil exploration and extraction company called GAStech. It contains email communication data between 55 employees over a two-week period.\n\n3.1 The edges data\n\nGAStech-email_edges.csv contains 9063 email correspondences between employees.\n\nEach record represents a single email sent from one employee to another, with attributes including the date, subject, and other metadata.\n\n\n3.2 The nodes data\n\nGAStech_email_nodes.csv contains information about the 55 employees, including their names, departments, and job titles.\n\nThese two datasets together form a complete network representation, where employees are nodes and emails are edges connecting them.\n\n\n3.3 Importing network data from files\nFirst, import the node and edge data files using the read_csv() function from the readr package:\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n3.4 Reviewing the imported data\nExamining the structure of the imported data reveals the attributes and data types:\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output reveals that the SentDate field is treated as a “Character” data type instead of a proper date data type. This needs to be corrected before proceeding with analysis.\n\n\n\n\n3.5 Wrangling time\nConverting the date string to a proper date format and extracting the day of the week:\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nKey date manipulation concepts\n\n\n\n\nThe dmy() function from lubridate transforms text dates to Date data type\nwday() extracts the day of the week as an ordered factor when label = TRUE\nSetting abbr = FALSE returns full day names like “Monday” instead of abbreviations\nThese functions create proper temporal dimensions for network analysis\n\n\n\n\n\n3.6 Reviewing the revised date fields\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n3.7 Wrangling attributes\nIndividual email records aren’t immediately useful for visualization. Aggregating them by relevant dimensions creates a more meaningful network representation:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\n4. Understanding the aggregation process\n\n\n\n\nFiltering for work-related emails focuses the analysis on professional communications\nGrouping by source, target, and weekday preserves temporal patterns\nThe Weight field counts the number of emails between each pair\nRemoving self-emails (where source = target) eliminates self-loops\nFiltering for Weight &gt; 1 focuses on repeated communications, revealing stronger relationships\n\n\n\n\n\n4.1 Reviewing the revised edges file\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-network-objects-using-tidygraph",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "5. Creating network objects using tidygraph",
    "text": "5. Creating network objects using tidygraph\nThe tidygraph package provides a tidy API for graph/network manipulation. It conceptualizes network data as two tidy tables - one for nodes and one for edges - and allows seamless switching between them while maintaining the relational structure.\n\n5.1 The tbl_graph object\nTwo key functions for creating network objects:\n\ntbl_graph() creates a network object from separate nodes and edges data frames\nas_tbl_graph() converts various existing network data formats into a tbl_graph object\n\n\n\n5.2 The dplyr verbs in tidygraph\nThe activate() function serves as a switch between the nodes and edges tables. All dplyr verbs applied to a tbl_graph object affect only the currently active table.\nSpecial accessor functions provide access to different parts of the graph: - .N() accesses node data while manipulating edges - .E() accesses edge data while manipulating nodes - .G() accesses the entire tbl_graph object\n\n\n5.3 Using tbl_graph() to build a tidygraph data model\nCreating the network graph object:\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n5.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\nThe output shows that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges. It displays the first rows of both node and edge data, and indicates that the Node Data is currently active.\n\n\n5.5 Graph Summary Statistics\nNetwork summary statistics provide a quantitative overview of the graph structure:\n\nGAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(degree = centrality_degree()) %&gt;%\n  as_tibble() %&gt;%\n  summarise(avg_degree = mean(degree),\n            min_degree = min(degree),\n            max_degree = max(degree),\n            sd_degree = sd(degree))\n\n# A tibble: 1 × 4\n  avg_degree min_degree max_degree sd_degree\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1       25.4          1        266      45.0\n\n\nThese statistics help understand the overall connectivity patterns in the network and identify potential outliers.\n\n\n5.6 Changing the active object\nSwitching between nodes and edges using activate():\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "6. Plotting Static Network Graphs with ggraph package",
    "text": "6. Plotting Static Network Graphs with ggraph package\nThe ggraph package extends ggplot2 for network visualization, making it easier to apply familiar ggplot skills to network graphs.\nThree main components of a ggraph network visualization: - nodes (the entities in the network) - edges (the connections between entities) - layouts (the spatial arrangement of nodes)\n\n6.1 Plotting a basic network graph\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCore ggraph functions\n\n\n\n\nggraph() initializes the plot with data and layout specifications\ngeom_edge_link() draws the connections between nodes\ngeom_node_point() visualizes the nodes themselves\nThese functions mirror the layered grammar of graphics from ggplot2\n\n\n\n\n\n6.2 Changing the default network graph theme\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThe theme_graph() function provides better defaults for network visualization by removing axes, grids, and borders that aren’t relevant for networks.\n\n\n6.3 Changing the coloring of the plot\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n6.4 Working with ggraph’s layouts\nggraph supports multiple layout algorithms that determine how nodes are positioned in the visualization. Options include: star, circle, nicely (default), dh, gem, graphopt, grid, mds, sphere, randomly, fr, kk, drl and lgl.\n\n\n6.5 Comparing Network Layouts\nDifferent layout algorithms reveal different aspects of network structure:\n\nlayout_options &lt;- c(\"fr\", \"kk\", \"drl\", \"lgl\")\nplots &lt;- list()\n\nfor (i in 1:length(layout_options)) {\n  plots[[i]] &lt;- ggraph(GAStech_graph, layout = layout_options[i]) +\n    geom_edge_link(alpha = 0.1) +\n    geom_node_point(aes(color = Department), size = 3) +\n    theme_graph() +\n    labs(title = paste(\"Layout:\", layout_options[i]))\n}\n\ngridExtra::grid.arrange(grobs = plots, ncol = 2)\n\n\n\n\n\n\n\n\nThis comparison helps select the most appropriate layout for revealing specific network patterns. Each algorithm has strengths: - FR (Fruchterman-Reingold): Good for revealing clusters - KK (Kamada-Kawai): Emphasizes overall structure - DRL: Handles large networks efficiently - LGL: Good for hierarchical structures\n\n\n6.6 Fruchterman and Reingold layout\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThe “fr” layout (Fruchterman-Reingold) uses a force-directed algorithm that positions nodes as if they repel each other while edges act as springs pulling connected nodes together.\n\n\n6.7 Modifying network nodes\nColoring nodes by department reveals organizational structure:\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization immediately reveals department-based clustering in communication patterns.\n\n\n6.8 Modifying edges\nMapping edge thickness to the Weight variable (number of emails exchanged):\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization now shows both organizational structure through node color and communication intensity through edge thickness.\n\n\n6.9 Node Degree Visualization\nNode degree (number of connections) is a fundamental centrality measure that identifies highly connected individuals:\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_degree(mode = \"total\"))) +\n  scale_size_continuous(range = c(2, 8), name = \"Degree\") +\n  labs(title = \"Network with Node Size Based on Total Degree\") +\n  theme_graph()\n\ng\n\n\n\n\n\n\n\n\nThis visualization immediately identifies communication hubs within the organization, regardless of department affiliation."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-facet-graphs",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-facet-graphs",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "7. Creating facet graphs",
    "text": "7. Creating facet graphs\nFaceting is a powerful technique to split visualization into subplots based on categorical variables. For networks, this helps reduce edge over-plotting and reveals patterns across different categories.\nThree faceting functions in ggraph: - facet_nodes() - draws edges only when both connected nodes appear in the panel - facet_edges() - always draws all nodes in all panels - facet_graph() - facets on two variables simultaneously\n\n7.1 Working with facet_edges()\nFaceting by the day of the week:\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\nThis visualization shows how communication patterns evolve throughout the week.\n\n\n7.2 Working with facet_edges() and theme adjustments\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n7.3 A framed facet graph\nAdding frames around each facet improves visual separation:\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n7.4 Department Communication Patterns\nAnalyzing which departments communicate most frequently with each other reveals organizational workflow:\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  left_join(\n    GAStech_graph %&gt;% \n      activate(nodes) %&gt;% \n      as_tibble() %&gt;% \n      select(id, Department),\n    by = c(\"from\" = \"id\")\n  ) %&gt;%\n  rename(from_dept = Department) %&gt;%\n  left_join(\n    GAStech_graph %&gt;% \n      activate(nodes) %&gt;% \n      as_tibble() %&gt;% \n      select(id, Department),\n    by = c(\"to\" = \"id\")\n  ) %&gt;%\n  rename(to_dept = Department) %&gt;%\n  group_by(from_dept, to_dept) %&gt;%\n  summarise(total_weight = sum(Weight)) %&gt;%\n  arrange(desc(total_weight)) %&gt;%\n  ungroup() %&gt;%\n  slice_head(n = 10) %&gt;%\n  ggplot(aes(x = reorder(paste(from_dept, \"→\", to_dept), total_weight), \n             y = total_weight)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Inter-Department Communication Flows\",\n       x = \"\",\n       y = \"Total Email Volume\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis analysis reveals which departments are most tightly coupled in their work processes.\n\n\n7.5 Working with facet_nodes()\nFaceting by department shows communication patterns within each organizational unit:\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#network-metrics-analysis",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#network-metrics-analysis",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "8. Network Metrics Analysis",
    "text": "8. Network Metrics Analysis\nNetwork metrics provide quantitative measurements of structural properties and help identify important nodes, clusters, and overall network characteristics.\n\n8.1 Computing centrality indices\nCentrality measures identify influential nodes in the network. Betweenness centrality measures how often a node lies on shortest paths between other nodes, identifying potential information brokers:\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization reveals individuals who control information flow between different parts of the network.\n\n\n8.2 Visualising network metrics\nFrom ggraph v2.0 onward, centrality measures can be computed directly within the plotting call:\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n8.3 Visualising Community\nCommunity detection algorithms identify clusters of densely connected nodes. tidygraph provides many algorithms for this purpose:\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization reveals natural clusters in the communication network that may not align perfectly with formal organizational structure.\n\n\n8.4 Community vs Department Alignment\nComparing algorithmic communities with formal departments reveals organizational structure effectiveness:\n\ncommunity_dept &lt;- GAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  as_tibble() %&gt;%\n  count(Department, community) %&gt;%\n  group_by(Department) %&gt;%\n  mutate(prop = n/sum(n))\n\nggplot(community_dept, aes(x = Department, y = community, size = prop, color = prop)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(title = \"Department vs Detected Community Alignment\",\n       size = \"Proportion\",\n       color = \"Proportion\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nPerfect alignment would show a one-to-one mapping; dispersed departments across multiple communities suggest informal communication structures that cross formal boundaries."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "9. Building Interactive Network Graph with visNetwork",
    "text": "9. Building Interactive Network Graph with visNetwork\nThe visNetwork package creates interactive network visualizations using the vis.js JavaScript library, enabling exploration through interaction.\n\n9.1 Data preparation\nPreparing data for visNetwork requires specific formatting:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n9.2 Plotting the first interactive network graph\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n9.3 Working with layout\nUsing the Fruchterman and Reingold layout for the interactive visualization:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n9.4 Working with visual attributes - Nodes\nvisNetwork uses a “group” field for node coloring. Renaming the Department field to match this convention:\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nAdding a legend to the visualization:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n9.5 Working with visual attributes - Edges\nCustomizing edge appearance with arrows and smooth curves:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n9.6 Interactivity\nAdding interactive features to enhance exploration:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nThe highlightNearest option highlights connected nodes when clicking on a node, while nodesIdSelection adds a dropdown menu for selecting specific nodes."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#reference",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#reference",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "10. Reference",
    "text": "10. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\n\n\n\nts_data &lt;- read_csv(\n  \"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)\n\n\nTip: make sure the format is correct (chr vs Date fields)\n\n19.2.3 Conventional base ts object versus tibble object\n\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\n\n\nDifferent dataframes… (explanation here) not a typical tibble dataframe &gt; class(ts_data_ts) [1] “mts” “ts” “matrix” “array” &gt; class(ts_data) [1] “spec_tbl_df” “tbl_df” “tbl” “data.frame”\nUse ts_data for data prep and conversion then convert to ts_data_ts for a timeseries object?\nsome explanation here!\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)\n\n\nclass(ts_tsibble) [1] “tbl_ts” “tbl_df” “tbl” “data.frame”\n\nsth that can be used by dpylr and tidyr and also timeseries time series manner?\nvisual time series.,,,\ntransform the series for you to work with first cause cannot see the header (numbers correspond to which country) - so transform to a long table. Prepare data by reordering data differently.\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nVisualising single time-series: ggplot2 methods\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\nor use facet you can define no of col –&gt; rows will be automatically calculated…\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE!!! intervals are not constant!!\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nUseful to start with\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\nshows you the distribution. e.g. Italians only come in at a big crowd in the month of Aug. Rest of the year hover around 5000.\nVietnams, higher in Jun and Jul months. Peak in jul. Sept to Dec relatively constant, Jan to May, gradual increase. Jun higher jump. Arrival count peak at jul\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)\n\n\n\n\n\n\n\n\n\n\n\nhow the time series correlated. only one variable - which is your time series variable\nauto correlation plots create 2 time lags… find the correlation between them - then you have multiple variables. - get the second variable by shifting lag=1 note the graph below starts from lag=1 (t vs t-1)\nChina - 6 mths period. Italy - 12 peak. weak correlation at 1st mth ~0.3 Note!!! 95% confidence level - should go above the blue line. China case, vietnam case, most/all the lags is statistically significant. China and vietnam, correlation decreases then increases again. The period different. Vietnam - 12 mths. China - 6 mths period UK lag t-1 significant… the rest of the year not sigificant, then 12 mths significant again - which means trend is not significant at all - drop down very fast - seasonal not important. (for UK and Italy)\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-draft.html#getting-started",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\n\n\n\nts_data &lt;- read_csv(\n  \"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)\n\n\nTip: make sure the format is correct (chr vs Date fields)\n\n19.2.3 Conventional base ts object versus tibble object\n\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\n\n\nDifferent dataframes… (explanation here) not a typical tibble dataframe &gt; class(ts_data_ts) [1] “mts” “ts” “matrix” “array” &gt; class(ts_data) [1] “spec_tbl_df” “tbl_df” “tbl” “data.frame”\nUse ts_data for data prep and conversion then convert to ts_data_ts for a timeseries object?\nsome explanation here!\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)\n\n\nclass(ts_tsibble) [1] “tbl_ts” “tbl_df” “tbl” “data.frame”\n\nsth that can be used by dpylr and tidyr and also timeseries time series manner?\nvisual time series.,,,\ntransform the series for you to work with first cause cannot see the header (numbers correspond to which country) - so transform to a long table. Prepare data by reordering data differently.\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nVisualising single time-series: ggplot2 methods\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\nor use facet you can define no of col –&gt; rows will be automatically calculated…\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE!!! intervals are not constant!!\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\nUseful to start with\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\nshows you the distribution. e.g. Italians only come in at a big crowd in the month of Aug. Rest of the year hover around 5000.\nVietnams, higher in Jun and Jul months. Peak in jul. Sept to Dec relatively constant, Jan to May, gradual increase. Jun higher jump. Arrival count peak at jul\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)\n\n\n\n\n\n\n\n\n\n\n\nhow the time series correlated. only one variable - which is your time series variable\nauto correlation plots create 2 time lags… find the correlation between them - then you have multiple variables. - get the second variable by shifting lag=1 note the graph below starts from lag=1 (t vs t-1)\nChina - 6 mths period. Italy - 12 peak. weak correlation at 1st mth ~0.3 Note!!! 95% confidence level - should go above the blue line. China case, vietnam case, most/all the lags is statistically significant. China and vietnam, correlation decreases then increases again. The period different. Vietnam - 12 mths. China - 6 mths period UK lag t-1 significant… the rest of the year not sigificant, then 12 mths significant again - which means trend is not significant at all - drop down very fast - seasonal not important. (for UK and Italy)\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\")\n\n\n\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\nOr with glimpse:\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#getting-started",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\")\n\n\n\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\nOr with glimpse:\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#data-overview",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-draft.html#data-overview",
    "title": "In-class Exercise 5",
    "section": "Data overview",
    "text": "Data overview\nType has 2 arguments: - Type = 1 - Type = 2…\n\nsummary1 &lt;- car_resale %&gt;%\n  ExpData(type=1)\n\nsummary1  # can be further customised (a table/df object)\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables        33\n4                              No. of factor variables         0\n5                                No. of text variables         5\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\nsummary2 &lt;- car_resale %&gt;%\n  ExpData(type=2)\n\nsummary2  # can be further customised (a table/df object)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\",\n           \"Cylinders\", \"Fuel_Type\", \"Color\",\n           \"Met_Color\", \"Automatic\", \"Mfr_Guarantee\", \"BOVAG_Guarantee\",\n           \"ABS\", \"Airbag_1\", \"Airbag_2\", \n           \"Airco\", \"Automatic_airco\", \"Boardcomputer\", \"CD_Player\",\n           \"Central_Lock\", \"Powered_Windows\", \"Power_Steering\",\"Radio\",\n           \"Mistlamps\", \"Sport_Model\", \"Backseat_Divider\", \"Metallic_Rim\",\n           \"Radio_cassette\", \"Tow_Bar\")\n\n\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each_(funs(factor(.)),cols)\n\n\nsummary(car_resale)\n\n      Id               Model               Price         Age_08_04    \n Length:1436        Length:1436        Min.   : 4350   Min.   : 1.00  \n Class :character   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Mode  :character   Mode  :character   Median : 9900   Median :61.00  \n                                       Mean   :10731   Mean   :55.95  \n                                       3rd Qu.:11950   3rd Qu.:70.00  \n                                       Max.   :32500   Max.   :80.00  \n                                                                      \n   Mfg_Month      Mfg_Year          KM         Quarterly_Tax        Weight    \n 1      :207   Min.   :1998   Min.   :     1   Min.   : 19.00   Min.   :1000  \n 4      :154   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00   1st Qu.:1040  \n 3      :138   Median :1999   Median : 63390   Median : 85.00   Median :1070  \n 2      :134   Mean   :2000   Mean   : 68533   Mean   : 87.12   Mean   :1072  \n 7      :133   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00   3rd Qu.:1085  \n 6      :120   Max.   :2004   Max.   :243000   Max.   :283.00   Max.   :1615  \n (Other):550                                                                  \n Guarantee_Period     HP_Bin      CC_bin    Doors   Gears    Cylinders\n Min.   : 3.000   &lt; 100  :560   &lt;1600:416   2:  2   3:   2   4:1436   \n 1st Qu.: 3.000   &gt; 120  : 11   &gt;1600:166   3:622   4:   1            \n Median : 3.000   100-120:865   1600 :854   4:138   5:1390            \n Mean   : 3.815                             5:674   6:  43            \n 3rd Qu.: 3.000                                                       \n Max.   :36.000                                                       \n                                                                      \n  Fuel_Type        Color     Met_Color Automatic Mfr_Guarantee BOVAG_Guarantee\n CNG   :  17   Grey   :301   0:467     0:1356    0:848         0: 150         \n Diesel: 155   Blue   :283   1:969     1:  80    1:588         1:1286         \n Petrol:1264   Red    :278                                                    \n               Green  :220                                                    \n               Black  :191                                                    \n               Silver :122                                                    \n               (Other): 41                                                    \n ABS      Airbag_1 Airbag_2 Airco   Automatic_airco Boardcomputer CD_Player\n 0: 268   0:  42   0: 398   0:706   0:1355          0:1013        0:1122   \n 1:1168   1:1394   1:1038   1:730   1:  81          1: 423        1: 314   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Central_Lock Powered_Windows Power_Steering Radio    Mistlamps Sport_Model\n 0:603        0:629           0:  32         0:1226   0:1067    0:1005     \n 1:833        1:807           1:1404         1: 210   1: 369    1: 431     \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Backseat_Divider Metallic_Rim Radio_cassette Tow_Bar \n 0: 330           0:1142       0:1227         0:1037  \n 1:1106           1: 294       1: 209         1: 399  \n                                                      \n                                                      \n                                                      \n                                                      \n                                                      \n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=NULL,\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=\"Price\",\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\nBar plots for all categorial variables\n\ncar_resale %&gt;%\n  ExpCatViz(target=NULL,\n            col=\"sky blue\",\n            clim=10,\n            margin=2,\n            Page=c(4,4),\n            sample=16)\n\n$`0`\n\n\n\n\n\n\n\n\n\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, data=car_resale)\n\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n# take out manufacturing year because of collinearlity \nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + Weight + Guarantee_Period, data=car_resale)\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\n\ncheck_heteroscedasticity(model1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n2 ways: 1. correlation matrix 2. vif\ngtsummary\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\ntbl_regression(model1, \n               intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-2,186\n-4,093, -278\n0.025\n\n\nAge_08_04\n-119\n-125, -114\n&lt;0.001\n\n\nKM\n-0.02\n-0.03, -0.02\n&lt;0.001\n\n\nWeight\n20\n18, 21\n&lt;0.001\n\n\nGuarantee_Period\n27\n2.1, 52\n0.034\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nBasic Regression Table\n\ntbl_regression(model1, \n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    # \"\\U03C3\" to extract the sigma value\n    label = list(sigma ~ \"\\U03C3\"),  # can ignore if you do not want the sigma\n    include = c(r.squared, adj.r.squared,\n                AIC, statistic,\n                p.value, sigma)\n  )\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10.html",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "This chapter explores information dashboard design techniques using R, focusing on creating effective visualizations for data communication. By the end of this chapter, students will be able to:\n\nCreate bullet charts using ggplot2\nDesign and implement sparklines with ggplot2\nBuild both static dashboards using gt/gtExtras\nDevelop interactive dashboards using reactable/reactablefmtr"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#overview",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "This chapter explores information dashboard design techniques using R, focusing on creating effective visualizations for data communication. By the end of this chapter, students will be able to:\n\nCreate bullet charts using ggplot2\nDesign and implement sparklines with ggplot2\nBuild both static dashboards using gt/gtExtras\nDevelop interactive dashboards using reactable/reactablefmtr"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#getting-started",
    "title": "Hands-on Exercise 10",
    "section": "2. Getting started",
    "text": "2. Getting started\nThe following R packages are essential for creating effective dashboard visualizations:\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse)\n\n\ntidyverse provides a collection of functions for performing data science tasks such as importing, tidying, wrangling data and visualizing data\nlubridate provides functions to work with dates and times more efficiently\nggthemes extends ggplot2 with additional themes beyond the basic options\ngtExtras provides helper functions for creating beautiful tables with gt\nreactable enables interactive data tables for R based on the React Table library\nreactablefmtr streamlines and enhances the styling of interactive reactable tables"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#importing-microsoft-access-database",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#importing-microsoft-access-database",
    "title": "Hands-on Exercise 10",
    "section": "3. Importing Microsoft Access database",
    "text": "3. Importing Microsoft Access database\n\n3.1 The data set\nFor this chapter, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\n3.2 Importing database into R\nThe RODBC package can import a database query table into R:\n\nlibrary(RODBC)\ncon &lt;- odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain &lt;- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\nNote: This requires a 32-bit version of R as odbcConnectAccess() is based on 32-bit architecture.\n\n\n3.3 Data Preparation\nImport the saved RDS file:\n\ncoffeechain &lt;- read_rds(\"data/rds/CoffeeChain.rds\")\n\nAggregate Sales and Budgeted Sales at the Product level:\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#bullet-charts-in-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#bullet-charts-in-ggplot2",
    "title": "Hands-on Exercise 10",
    "section": "4. Bullet Charts in ggplot2",
    "text": "4. Bullet Charts in ggplot2\nBullet charts are an excellent way to display performance metrics against targets. The visualization below creates bullet charts using ggplot2:\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n4.1 Enhancing Bullet Chart Readability with Custom Colors\nBullet charts can be enhanced by using meaningful color schemes that intuitively communicate performance. The following example uses a graduated color scheme to indicate performance levels:\n\n# Calculate performance ratio\nproduct &lt;- product %&gt;%\n  mutate(performance_ratio = current/target)\n\n# Create enhanced bullet chart with performance-based coloring\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey95\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey75\", width=0.85) +\n  geom_col(aes(Product, current, fill = performance_ratio), \n           width=0.35) + \n  scale_fill_gradient2(midpoint = 1, \n                      low = \"red\", \n                      mid = \"darkblue\",\n                      high = \"green\",\n                      name = \"Performance\") +\n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"black\",\n                size = 1) +\n  labs(title = \"Product Performance Against Targets\",\n       subtitle = \"Actual sales compared to budget targets\",\n       x = \"\",\n       y = \"Sales ($)\") +\n  theme_minimal() +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "title": "Hands-on Exercise 10",
    "section": "5. Plotting Sparklines Using ggplot2",
    "text": "5. Plotting Sparklines Using ggplot2\nSparklines provide compact, word-sized visualizations that show trends over time. They’re perfect for dashboard displays where space is limited.\n\n5.1 Preparing the data\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nCalculate minimum, maximum, and end-of-month sales:\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nCompute the 25th and 75th percentiles:\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n5.2 Creating sparklines with ggplot2\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n5.3 Enhancing Sparklines with Trend Analysis\nSparklines can be enhanced by incorporating trend analysis to help identify significant patterns across time periods:\n\n# Calculate three-month moving average to identify trends\ntrend_data &lt;- sales_report %&gt;%\n  arrange(Product, Month) %&gt;%\n  group_by(Product) %&gt;%\n  mutate(trend = zoo::rollmean(Sales, k = 3, fill = NA, align = \"right\"))\n\n# Plot the enhanced sparklines with trend line\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  # Add trend line in a distinct color\n  geom_line(data = trend_data, aes(y = trend), \n            color = \"darkgreen\", size = 0.6, linetype = \"dashed\") +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "title": "Hands-on Exercise 10",
    "section": "6. Static Information Dashboard Design: gt and gtExtras Methods",
    "text": "6. Static Information Dashboard Design: gt and gtExtras Methods\nThe gt and gtExtras packages allow for creating beautiful, static data tables that can incorporate visualizations.\n\n6.1 Plotting a Simple Bullet Chart\nThe gt_plt_bullet function creates bullet charts within a data table:\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\nperformance_ratio\n\n\n\n\nAmaretto\n\n\n\n   \n\n0.9657721\n\n\nCaffe Latte\n\n\n\n   \n\n1.1754748\n\n\nCaffe Mocha\n\n\n\n   \n\n1.0035934\n\n\nChamomile\n\n\n\n   \n\n1.1838659\n\n\nColombian\n\n\n\n   \n\n0.9548370\n\n\nDarjeeling\n\n\n\n   \n\n1.2752964\n\n\nDecaf Espresso\n\n\n\n   \n\n1.0322504\n\n\nDecaf Irish Cream\n\n\n\n   \n\n0.9285203\n\n\nEarl Grey\n\n\n\n   \n\n1.3118271\n\n\nGreen Tea\n\n\n\n   \n\n1.2963694\n\n\nLemon\n\n\n\n   \n\n1.2251086\n\n\nMint\n\n\n\n   \n\n1.2609463\n\n\nRegular Espresso\n\n\n\n   \n\n1.0623784"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#sparklines-gtextras-method",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#sparklines-gtextras-method",
    "title": "Hands-on Exercise 10",
    "section": "7. Sparklines: gtExtras Method",
    "text": "7. Sparklines: gtExtras Method\nFirst, prepare the data for monthly sales by product:\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate(Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nThe gtExtras functions require data.frames with list columns:\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n7.1 Plotting Coffee Chain Sales Report\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n7.2 Adding Statistics\nCalculate summary statistics:\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n7.3 Combining the Data Frames\nCombine sparkline data with statistics:\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\nsales_data = left_join(sales, spark)\n\n\n\n7.4 Plotting the Updated Data Table\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n7.5 Adding Conditional Formatting to Dashboard Tables\nEnhance the table with conditional formatting to visually highlight performance:\n\n# Create enhanced data table with conditional formatting\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE) %&gt;%\n  data_color(columns = \"Min\",\n             colors = scales::col_numeric(\n               palette = c(\"#ffebee\", \"#f44336\"),\n               domain = NULL)) %&gt;%\n  data_color(columns = \"Max\",\n             colors = scales::col_numeric(\n               palette = c(\"#e8f5e9\", \"#4caf50\"),\n               domain = NULL)) %&gt;%\n  tab_header(\n    title = \"Coffee Chain Sales Performance\",\n    subtitle = \"Monthly sales analysis by product (2013)\"\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Data represents aggregated monthly sales values\",\n    locations = cells_column_labels(columns = \"Monthly Sales\")\n  ) %&gt;%\n  cols_align(align = \"center\") %&gt;%\n  opt_all_caps() %&gt;%\n  opt_row_striping()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee Chain Sales Performance\n\n\nMonthly sales analysis by product (2013)\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales1\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n1 Data represents aggregated monthly sales values\n\n\n\n\n\n\n\n\n\n\n7.6 Combining Bullet Chart and Sparklines\nCreate a bullet chart data frame:\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\nJoin it with the sales data:\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\nCreate a comprehensive dashboard table:\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n7.7 Creating Hierarchical Dashboard Tables\nDashboard tables can be enhanced with hierarchical grouping to better organize information by categories:\n\n# First, add product category information\nproduct_categories &lt;- tibble(\n  Product = unique(coffeechain$Product),\n  Category = rep(c(\"Coffee\", \"Tea\", \"Espresso\"), length.out = length(unique(coffeechain$Product)))\n)\n\n# Join category information\nsales_data_with_categories &lt;- sales_data %&gt;%\n  left_join(product_categories, by = \"Product\")\n\n# Create hierarchical dashboard\nsales_data_with_categories %&gt;%\n  arrange(Category, Product) %&gt;%\n  gt(groupname_col = \"Category\") %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  tab_header(\n    title = \"Coffee Chain Performance by Product Category\",\n    subtitle = \"Monthly trends and performance against targets\"\n  ) %&gt;%\n  tab_options(\n    row_group.background.color = \"#f7f7f7\",\n    row_group.font.weight = \"bold\"\n  ) %&gt;%\n  opt_row_striping() %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee Chain Performance by Product Category\n\n\nMonthly trends and performance against targets\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nCoffee\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nEspresso\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n   \n\n\n\nTea\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "title": "Hands-on Exercise 10",
    "section": "8. Interactive Information Dashboard Design: reactable and reactablefmtr Methods",
    "text": "8. Interactive Information Dashboard Design: reactable and reactablefmtr Methods\nThe reactable and reactablefmtr packages enable interactive dashboards that allow users to explore the data.\nFirst, install the dataui package if needed:\n\nremotes::install_github(\"timelyportfolio/dataui\")\n\nLoad the package:\n\nlibrary(dataui)\n\n\n8.1 Plotting Interactive Sparklines\nPrepare data with list fields:\n\nreport &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\nCreate basic interactive sparklines:\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n8.2 Changing the Page Size\nSet a custom page size:\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n8.3 Adding Points and Labels\nHighlight points and add labels:\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n8.4 Adding Reference Line\nAdd a mean reference line:\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n8.5 Adding Bandline\nAdd quartile bands:\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n8.6 Adding Interactive Filtering to Dashboard Tables\nEnhance the dashboard with interactive filtering capabilities:\n\n# Create more comprehensive dataset with calculated metrics\ncomprehensive_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date, label = TRUE, abbr = TRUE),\n         Quarter = paste0(\"Q\", quarter(Date))) %&gt;%\n  group_by(Product, Month, Quarter) %&gt;%\n  summarise(Sales = sum(Sales),\n            Budget = sum(`Budget Sales`),\n            Variance = Sales - Budget,\n            Achievement = Sales/Budget) %&gt;%\n  ungroup()\n\n# Prepare for reactable\nproduct_monthly &lt;- comprehensive_report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales),\n            `Average Achievement` = mean(Achievement, na.rm = TRUE),\n            `Total Sales` = sum(Sales),\n            `Total Budget` = sum(Budget),\n            `Overall Variance` = sum(Variance))\n\n# Create interactive dashboard with filtering\nreactable(\n  product_monthly,\n  filterable = TRUE,\n  searchable = TRUE,\n  striped = TRUE,\n  highlight = TRUE,\n  showSortable = TRUE,\n  defaultSorted = \"Total Sales\",\n  defaultSortOrder = \"desc\",\n  columns = list(\n    Product = colDef(\n      minWidth = 150,\n      headerStyle = list(fontWeight = \"bold\")\n    ),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        product_monthly,\n        highlight_points = highlight_points(min = \"red\", max = \"blue\"),\n        line_width = 1.5,\n        bandline = \"innerquartiles\",\n        statline = \"mean\"\n      ),\n      minWidth = 200\n    ),\n    `Average Achievement` = colDef(\n      format = colFormat(percent = TRUE, digits = 1),\n      style = function(value) {\n        if (value &gt;= 1) {\n          list(color = \"#4caf50\", fontWeight = \"bold\")\n        } else if (value &gt;= 0.9) {\n          list(color = \"#ff9800\")\n        } else {\n          list(color = \"#f44336\")\n        }\n      },\n      minWidth = 120\n    ),\n    `Total Sales` = colDef(\n      format = colFormat(prefix = \"$\", separators = TRUE),\n      minWidth = 120\n    ),\n    `Total Budget` = colDef(\n      format = colFormat(prefix = \"$\", separators = TRUE),\n      minWidth = 120\n    ),\n    `Overall Variance` = colDef(\n      format = colFormat(prefix = \"$\", separators = TRUE),\n      style = function(value) {\n        if (value &gt;= 0) {\n          list(color = \"#4caf50\")\n        } else {\n          list(color = \"#f44336\")\n        }\n      },\n      minWidth = 120\n    )\n  )\n)\n\n\n\n\n\n\n\n8.7 Changing from Sparkline to Sparkbar\nDisplay data as sparkbars instead of sparklines:\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex10.html#reference",
    "title": "Hands-on Exercise 10",
    "section": "9. Reference",
    "text": "9. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "",
    "text": "pacman::p_load(tmap, tidyverse, sf, spdep)\n\n\n\n\nFor this exercise, a prepared dataset called NGA_wp.rds will be used. The dataset is a polygon feature data.frame providing information on water points of Nigeria at the LGA level. This can be found in the rds sub-directory of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#getting-started",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "",
    "text": "pacman::p_load(tmap, tidyverse, sf, spdep)\n\n\n\n\nFor this exercise, a prepared dataset called NGA_wp.rds will be used. The dataset is a polygon feature data.frame providing information on water points of Nigeria at the LGA level. This can be found in the rds sub-directory of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "2. Basic Choropleth Mapping",
    "text": "2. Basic Choropleth Mapping\n\n2.1 Visualising distribution of water points\n\n# Create map of total water points with proper tmap v4 syntax\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          fill.scale = tm_scale_intervals(n = 10, \n                                         style = \"equal\",\n                                         values = \"brewer.blues\"),\n          fill.legend = tm_legend(title = \"Total Water Points\")) +\n  tm_borders(lwd = 0.1) +\n  tm_title(text = \"Distribution of Total Water Points by LGAs\")\n\n# Create map of functional water points with proper tmap v4 syntax\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          fill.scale = tm_scale_intervals(n = 10, \n                                         style = \"equal\",\n                                         values = \"brewer.blues\"),\n          fill.legend = tm_legend(title = \"Functional Water Points\")) +\n  tm_borders(lwd = 0.1) +\n  tm_title(text = \"Distribution of Functional Water Points by LGAs\")\n\n# Arrange maps side by side\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n2.2 Impact of Classification Methods on Visualizations\nClassification methods significantly affect how patterns are perceived on choropleth maps. Below is a comparison of different classification approaches:\n\ntm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          fill.scale = tm_scale_intervals(\n            n = 5,\n            style = \"jenks\",\n            values = \"brewer.blues\"),\n          fill.legend = tm_legend(title = \"Jenks classification\")) +\n  tm_borders(lwd = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\")\n\n\n\n\n\n\n\n\n\ntm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          fill.scale = tm_scale_intervals(\n            n = 5,\n            style = \"quantile\",\n            values = \"brewer.blues\"),\n          fill.legend = tm_legend(title = \"Quantile classification\")) +\n  tm_borders(lwd = 0.1) + \n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\")\n\n\n\n\n\n\n\n\nThe Jenks classification minimizes within-class variance, highlighting natural breaks in the data, while quantile classification ensures an equal number of observations in each class. Note how this changes the visual patterns revealed in the maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#choropleth-maps-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#choropleth-maps-for-rates",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "3. Choropleth Maps for Rates",
    "text": "3. Choropleth Maps for Rates\nIn spatial analysis, mapping rates rather than raw counts is critical because water points are not equally distributed in space. Without accounting for the total number of water points in an area, maps end up visualizing size rather than the phenomenon of interest.\n\n3.1 Deriving Proportion of Functional and Non-Functional Water Points\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n3.2 Plotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          fill.scale = tm_scale_intervals(\n            n = 10,\n            style = \"equal\",\n            values = \"brewer.blues\"),\n          fill.legend = tm_legend(\n            title = \"% Functional\",\n            hist = TRUE)) +\n  tm_borders(lwd = 0.1) +\n  tm_title(text = \"Rate map of functional water point by LGAs\") +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n3.3 Visualizing Uncertainty in Rate Maps\nWhen working with rates, areas with small populations (or in this case, few water points) can produce unstable rates. Small changes in counts can lead to large swings in percentages.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(reliability = case_when(\n    total_wp &lt; 10 ~ \"Low\",\n    total_wp &lt; 50 ~ \"Medium\",\n    TRUE ~ \"High\"\n  ))\n\n# Create faceted map with tmap v4 syntax\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          style = \"quantile\",\n          n = 10,\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_facets(by = \"reliability\", \n            ncol = 3) +\n  tm_layout(legend.outside = TRUE,\n            panel.labels = c(\"Low reliability\", \"Medium reliability\", \"High reliability\"))\n\n\n\n\n\n\n\n\nThis faceted approach helps identify where results might be less reliable due to small sample sizes, an important consideration when interpreting spatial patterns."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#extreme-value-maps",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "4. Extreme Value Maps",
    "text": "4. Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n4.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%, 1-10%, 10-50%, 50-90%, 90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\nData Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geometry field.\n\n\nWhy writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\nCreating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\nA percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\nTest drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n4.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nCreating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  if (lofence &lt; qv[1]) {  \n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { \n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\nTest drive the newly created function\nLet’s test the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\nBoxmap function\nThe code chunk below is an R function to create a box map.\n\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nreturns:\n\na tmap-element (plots a map)\n\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n\n\n\n\n\n\n\n4.3 Spatial Autocorrelation Visualization\nUnderstanding whether spatial patterns show clustering is crucial. A simple approach to visualizing potential spatial autocorrelation is to map both the variable of interest and its spatial lag (neighborhood average):\n\n# Ensure geometry column is properly set\nNGA_wp$geometry &lt;- st_geometry(NGA_wp)\n# Create neighborhood weights\nNGA_nb &lt;- poly2nb(NGA_wp, queen = TRUE)\nNGA_wt &lt;- nb2listw(NGA_nb, style = \"W\", zero.policy = TRUE)\n# Calculate spatial lag of functional water point percentages\nNGA_wp$lag_pct_functional &lt;- lag.listw(NGA_wt, NGA_wp$pct_functional)\n\n# Create original values map\nmap1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\", \n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"Functional water points (%)\") +\n  tm_borders() +\n  tm_layout(title = \"Original Values\",\n            frame = TRUE)\n\n# Create spatial lag map\nmap2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"lag_pct_functional\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Neighborhood average (%)\") +\n  tm_borders() +\n  tm_layout(title = \"Spatial Lag Values\",\n            frame = TRUE)\n\n# Arrange both maps side-by-side\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\nThis side-by-side comparison helps identify potential spatial clustering or dispersion patterns, a crucial first step before formal spatial autocorrelation analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#advanced-techniques-for-analytical-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#advanced-techniques-for-analytical-mapping",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "5. Advanced Techniques for Analytical Mapping",
    "text": "5. Advanced Techniques for Analytical Mapping\n\n5.1 Visualizing Multiple Variables Simultaneously\nMultivariate mapping can reveal complex relationships between different aspects of water point data. The bivariate choropleth technique below combines functional and non-functional percentages:\n\n# Handle NA values properly with a different approach\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(\n    # First ensure no NA values by replacing with 0\n    pct_functional_clean = if_else(is.na(pct_functional), 0, pct_functional),\n    pct_nonfunctional_clean = if_else(is.na(pct_nonfunctional), 0, pct_nonfunctional),\n    \n    # Then create factor categories\n    func_cat = cut(pct_functional_clean, \n                   breaks = c(-0.001, 0.33, 0.66, 1),\n                   labels = c(\"Low\", \"Medium\", \"High\")),\n    nonfunc_cat = cut(pct_nonfunctional_clean, \n                      breaks = c(-0.001, 0.33, 0.66, 1),\n                      labels = c(\"Low\", \"Medium\", \"High\")),\n    \n    # Create combined category\n    bivariate_cat = paste(func_cat, nonfunc_cat, sep = \"-\")\n  )\n\n# Create a color palette for bivariate map\nbivariate_colors &lt;- c(\n  \"High-Low\" = \"#1a9641\",     # High functional, Low non-functional (good)\n  \"High-Medium\" = \"#a6d96a\", \n  \"High-High\" = \"#ffffbf\",\n  \"Medium-Low\" = \"#66bd63\",\n  \"Medium-Medium\" = \"#fee08b\",\n  \"Medium-High\" = \"#fdae61\",\n  \"Low-Low\" = \"#d9ef8b\",\n  \"Low-Medium\" = \"#f46d43\",\n  \"Low-High\" = \"#d73027\"      # Low functional, High non-functional (poor)\n)\n\n# Create the bivariate map with corrected tmap v4 syntax\ntm_shape(NGA_wp) +\n  tm_fill(\"bivariate_cat\",\n         fill.scale = tm_scale_categorical(values = bivariate_colors),\n         fill.legend = tm_legend(title = \"Functional vs Non-functional\")) +\n  tm_borders(lwd = 0.1) +\n  tm_layout(title = \"Bivariate Analysis of Water Point Status\",\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\nThis approach reveals LGAs facing compound challenges (low functionality and high non-functionality) as well as those performing well across both metrics.\n\n\n5.2 Spatial Pattern Analysis\nFor deeper understanding of water point patterns, hotspot analysis can reveal statistically significant clusters:\n\nset.seed(42)  # For reproducibility\nNGA_localMoran &lt;- localmoran(NGA_wp$pct_nonfunctional, \n                             NGA_wt, \n                             zero.policy = TRUE, \n                             na.action = na.omit)\n\n# Bind results to spatial dataframe\nNGA_wp$localI &lt;- NGA_localMoran[,1]\nNGA_wp$p_value &lt;- NGA_localMoran[,5]\n\n# Create hotspot map\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(hotspot_type = case_when(\n    p_value &gt; 0.05 ~ \"Not Significant\",\n    localI &gt; 0 & p_value &lt;= 0.05 ~ \"High-High Cluster\",\n    localI &lt; 0 & p_value &lt;= 0.05 ~ \"Low-Low Cluster\",\n    TRUE ~ \"Spatial Outlier\"\n  ))\n\ntm_shape(NGA_wp) +\n  tm_fill(\"hotspot_type\",\n          palette = c(\"High-High Cluster\" = \"red\", \n                      \"Low-Low Cluster\" = \"blue\",\n                      \"Spatial Outlier\" = \"purple\",\n                      \"Not Significant\" = \"grey90\"),\n          title = \"Cluster Analysis\") +\n  tm_borders(lwd = 0.1, alpha = 0.5) +\n  tm_layout(main.title = \"Hotspot Analysis of Non-functional Water Points\",\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\nThis analysis identifies statistically significant spatial clusters of high non-functional water point rates (hotspots) and low rates (coldspots), providing evidence-based targets for intervention programs."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#combining-visualization-techniques",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#combining-visualization-techniques",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "6. Combining Visualization Techniques",
    "text": "6. Combining Visualization Techniques\nEffective spatial analysis often requires combining different visualization techniques to gain comprehensive insights. The following example integrates box map classification with hotspot analysis:\n\n# Combine box classification with hotspot analysis\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(box_cat = cut(wp_nonfunctional,\n                       breaks = boxbreaks(var),\n                       labels = c(\"Lower outlier\", \n                                  \"&lt; 25%\", \n                                  \"25% - 50%\", \n                                  \"50% - 75%\",\n                                  \"&gt; 75%\", \n                                  \"Upper outlier\"),\n                       include.lowest = TRUE))\n\n# Create integrated visualization\ntm_shape(NGA_wp) +\n  tm_fill(\"box_cat\",\n          palette = c(\"Lower outlier\" = \"#d1e5f0\", \n                      \"&lt; 25%\" = \"#92c5de\",\n                      \"25% - 50%\" = \"#4393c3\", \n                      \"50% - 75%\" = \"#2166ac\",\n                      \"&gt; 75%\" = \"#053061\", \n                      \"Upper outlier\" = \"#67001f\"),\n          title = \"Box Map Classification\") +\n  tm_borders(lwd = 0.1, alpha = 0.5) +\n  tm_symbols(size = 0.2,\n             col = \"hotspot_type\",\n             palette = c(\"High-High Cluster\" = \"red\", \n                         \"Low-Low Cluster\" = \"blue\",\n                         \"Spatial Outlier\" = \"purple\",\n                         \"Not Significant\" = \"grey90\"),\n             title = \"Cluster Type\",\n             alpha = 0.7) +\n  tm_layout(main.title = \"Integrated Box Map and Hotspot Analysis\",\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\nThis integrated approach reveals not only areas with extreme values but also whether these extremes form statistically significant spatial patterns, providing deeper analytical insights."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08c.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08c.html#reference",
    "title": "Hands-on Exercise 8c: Analytical Mapping",
    "section": "7. Reference",
    "text": "7. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a.html",
    "href": "Hands-on_Ex/Hands-on_Ex08a.html",
    "title": "Hands-on Exercise 8a: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolization of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nThis chapter explores how to plot functional and truthful choropleth maps by using the tmap package in R.\nIt is advisable to read the functional description of each function before using them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08a.html#overview",
    "title": "Hands-on Exercise 8a: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolization of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nThis chapter explores how to plot functional and truthful choropleth maps by using the tmap package in R.\nIt is advisable to read the functional description of each function before using them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08a.html#getting-started",
    "title": "Hands-on Exercise 8a: Choropleth Mapping with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\nThe key R package used is tmap package in R. Beside tmap package, four other R packages will be used:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages:\n\npacman::p_load(sf, tmap, tidyverse, classInt)\n\n\n2.1 The Data\nTwo data sets will be used to create the choropleth map:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg. This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data file. It can be downloaded at Department of Statistics, Singapore. Although it does not contain any coordinates values, its PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n2.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\simpl\\Desktop\\sindy-556\\ISSS608\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo examine the content of mpsz:\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n2.3 Importing Attribute Data into R\nNext, we import respopagsex2011to2020.csv file and save it into an R dataframe called popdata using the read_csv() function of readr package:\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08a.html#data-preparation",
    "title": "Hands-on Exercise 8a: Choropleth Mapping with R",
    "section": "3. Data Preparation",
    "text": "3. Data Preparation\nBefore a thematic map can be prepared, we need to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, and DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n3.2 Understanding Data Normalization in Choropleth Maps\nWhen creating choropleth maps, it’s critical to normalize data properly. The DEPENDENCY ratio calculated above is a good example of normalization - showing the proportion of dependent population (young and elderly) relative to the working population rather than raw counts. This approach reduces the visual bias that can occur when larger areas naturally contain more people.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(DEPENDENCY_CLASS = case_when(\n    DEPENDENCY &lt; 0.4 ~ \"Low\",\n    DEPENDENCY &lt; 0.7 ~ \"Moderate\",\n    TRUE ~ \"High\"\n  ))\n\nsummary(popdata2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7137  0.7862  0.8579  0.8767 19.0000      98 \n\n\n\n\n3.3 Joining the attribute data and geospatial data\nBefore performing the georelational join, we need to convert the values in PA and SZ fields to uppercase because the values in SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThe left_join() function with mpsz simple feature data frame as the left data table ensures that the output will be a simple features data frame.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 8a: Choropleth Mapping with R",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic maps using tmap:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customizable thematic maps by using tmap elements.\n\n\n4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest way to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualization in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNote that: - tmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used. - fill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n4.2 Interactive vs. Static Mapping\nThe tmap package offers both static (“plot”) and interactive (“view”) modes. Interactive maps are particularly useful for exploratory analysis and presentations, allowing users to zoom, pan, and click features for more information.\n\ntmap_mode(\"view\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe interactive mode is excellent for initial data exploration, while static maps are better for formal reports and publications where consistent display across devices is important.\n\n\n4.3 Creating a choropleth map by using tmap’s elements\nDespite the usefulness of qtm() for drawing a choropleth map quickly and easily, the disadvantage is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n4.4 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nKey points about tm_polygons(): - The default interval binning used is called “pretty” - The default color scheme is YlOrRd of ColorBrewer - Missing values are shaded in grey by default\n\n\n4.5 Drawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is actually a wrapper of tm_fill() and tm_border(). tm_fill() shades the polygons using the default color scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nTo add the boundary of the planning subzones:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\n4.6 Handling Missing Data in Choropleth Maps\nMissing data is a common issue in spatial datasets. It’s important to represent it appropriately in choropleth maps to avoid misleading visualizations. By default, tmap shows missing values in grey, but we can customize this representation:\n\n# Create a copy with some missing values for demonstration\nmpsz_pop2020_demo &lt;- mpsz_pop2020\nmpsz_pop2020_demo$DEPENDENCY[1:5] &lt;- NA\n\ntm_shape(mpsz_pop2020_demo)+\n  tm_fill(\"DEPENDENCY\",\n          title = \"Dependency Ratio\",\n          textNA = \"Missing Data\",\n          colorNA = \"white\",\n          showNA = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nProper handling of missing data enhances the credibility of visualizations and prevents misinterpretation. Always specify clear visual cues for missing values and include explanations in legends.\n\n\n4.7 Data classification methods of tmap\nMost choropleth maps employ some method of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total of ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() is used.\n\nPlotting choropleth maps with built-in classification methods\nThe code below shows a quantile data classification that uses 5 classes:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nHere’s an example using the equal data classification method:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the distribution in the quantile data classification method is more evenly distributed than in the equal data classification method.\n\n\n\n4.8 Statistical Considerations in Classification Methods\nThe choice of classification method can significantly impact the story your map tells. Here’s a statistical comparison of different classification methods using summary statistics:\n\n# Calculate class breaks using different methods\njenks_breaks &lt;- classIntervals(mpsz_pop2020$DEPENDENCY, n = 5, style = \"jenks\")$brks\nequal_breaks &lt;- classIntervals(mpsz_pop2020$DEPENDENCY, n = 5, style = \"equal\")$brks\nquantile_breaks &lt;- classIntervals(mpsz_pop2020$DEPENDENCY, n = 5, style = \"quantile\")$brks\n\n# Display the breaks for comparison\ndata.frame(\n  Method = c(\"Jenks\", \"Equal\", \"Quantile\"),\n  Break1 = c(jenks_breaks[2], equal_breaks[2], quantile_breaks[2]),\n  Break2 = c(jenks_breaks[3], equal_breaks[3], quantile_breaks[3]),\n  Break3 = c(jenks_breaks[4], equal_breaks[4], quantile_breaks[4]),\n  Break4 = c(jenks_breaks[5], equal_breaks[5], quantile_breaks[5])\n)\n\n    Method    Break1    Break2     Break3     Break4\n1    Jenks 0.4886364 0.7913043  1.0436893  1.5000000\n2    Equal 3.8888889 7.6666667 11.4444444 15.2222222\n3 Quantile 0.6919431 0.7655602  0.8208279  0.8988166\n\n\nThe Jenks method minimizes within-class variance and maximizes between-class differences, making it ideal for identifying natural clusters in data. Equal interval is best for evenly distributed data, while quantile ensures each class has equal number of observations but may group dissimilar values together.\n\nPlotting choropleth map with custom breaks\nFor all the built-in styles, the category breaks are computed internally. To override these defaults, breakpoints can be set explicitly using the breaks argument of tm_fill(). In tmap, breaks include minimum and maximum values, so to create n categories, n+1 elements must be specified (values must be in increasing order).\nLet’s get some descriptive statistics on the DEPENDENCY field before setting break points:\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break points at 0.60, 0.70, 0.80, and 0.90, plus a minimum of 0 and maximum of 1.00:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n4.9 Color Scheme\ntmap supports color ramps either defined by the user or a set of predefined color ramps from the RColorBrewer package.\n\nUsing ColourBrewer palette\nTo change the color, assign the preferred color to the palette argument of tm_fill():\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nTo reverse the color shading, add a “-” prefix:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n4.10 Color Theory and Accessibility in Cartography\nChoosing appropriate colors for maps is crucial for both effectiveness and accessibility. Different types of data require different color schemes:\n\n# Sequential data (like our dependency ratio)\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"Sequential (Blues)\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n# Diverging data (example: deviation from mean)\nmpsz_pop2020$DEVIATION &lt;- mpsz_pop2020$DEPENDENCY - mean(mpsz_pop2020$DEPENDENCY, na.rm=TRUE)\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEVIATION\",\n          style = \"quantile\",\n          palette = \"RdBu\",\n          title = \"Diverging (RdBu)\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n# Colorblind-friendly palette\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"viridis\",\n          title = \"Colorblind-friendly\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n4.11 Map Layouts\nMap layout refers to the combination of all map elements into a cohesive map. Map elements include the objects to be mapped, title, scale bar, compass, margins, and aspect ratios.\n\nMap Legend\nIn tmap, several legend options are provided to change the placement, format, and appearance of the legend:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nMap style\ntmap allows a wide variety of layout settings to be changed using tmap_style():\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\nCartographic Furniture\ntmap provides arguments to draw other map furniture such as compass, scale bar, and grid lines:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          fill.scale = tm_scale_intervals(style = \"quantile\", \n                                         values = \"brewer.blues\"),\n          fill.legend = tm_legend(title = \"No. of persons\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tm_layout(frame = TRUE,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            main.title = \"Distribution of Dependency Ratio\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            main.title.fontface = \"bold\",\n            sub.title = \"by planning subzone\",\n            sub.title.position = \"center\",\n            sub.title.size = 1.0)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\", \n          fill.scale = tm_scale(values = \"brewer.blues\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid() +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA and Population data from DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nReset to the default style:\n\ntmap_style(\"white\")\n\n\n\n\n4.12 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arranged side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualization of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the aesthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nBy assigning multiple values to at least one of the aesthetic arguments\nCreating small multiple choropleth maps by defining ncols in tm_fill():\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nCreating small multiple choropleth maps by assigning multiple values to aesthetic arguments:\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\"))\n\n\n\n\n\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\nCreating multiple small choropleth maps using tm_facets():\n\ndependency_breaks &lt;- seq(min(mpsz_pop2020$DEPENDENCY, na.rm = TRUE),\n                        max(mpsz_pop2020$DEPENDENCY, na.rm = TRUE), \n                        length.out = 6)\n\nregions &lt;- unique(mpsz_pop2020$REGION_N)\nregion_maps &lt;- lapply(regions, function(region) {\n  region_data &lt;- mpsz_pop2020[mpsz_pop2020$REGION_N == region,]\n  tm_shape(region_data) +\n    tm_fill(\"DEPENDENCY\",\n            style = \"fixed\",\n            breaks = dependency_breaks,\n            palette = \"Blues\",\n            legend.show = FALSE) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(title = region,\n              frame = TRUE) +\n    tmap_style(\"white\")\n})\n\nlegend_map &lt;- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"fixed\",\n          breaks = dependency_breaks,\n          palette = \"Blues\") +\n  tm_layout(legend.only = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE)\n\ntmap_arrange(c(region_maps, list(legend_map)), \n             ncol = 3,\n             heights = c(1, 1, 0.1)) \n\n\n\n\n\n\n\n\n\n\n\n4.13 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth maps, you can also use selection functions to map spatial objects meeting specific criteria:\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            legend.title.size = 0.8,\n            legend.title.position = c(\"center\", \"top\"),\n            legend.title.space = 1.5,\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08a.html#reference",
    "title": "Hands-on Exercise 8a: Choropleth Mapping with R",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "",
    "text": "Treemaps are a visualization technique used to represent hierarchical data through nested rectangles, where the size and color of each rectangle convey different attributes. They are particularly effective for displaying proportions within categories, making it easy to compare parts of a whole at a glance."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#overview",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "",
    "text": "Treemaps are a visualization technique used to represent hierarchical data through nested rectangles, where the size and color of each rectangle convey different attributes. They are particularly effective for displaying proportions within categories, making it easy to compare parts of a whole at a glance."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "1. Installing and Launching R Packages",
    "text": "1. Installing and Launching R Packages\nYou will need the following packages. Here, we use pacman::p_load() for convenience:\n\npacman::p_load(treemap, treemapify, tidyverse, d3treeR)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#data-wrangling",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "2. Data Wrangling",
    "text": "2. Data Wrangling\nFor this exercise, the dataset REALIS2018.csv is used, containing private property transaction records in 2018 from Singapore’s Urban Redevelopment Authority (URA).\n\n2.1 Importing the Dataset\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\n\n2.2 Transforming the Data\nTreemap visualizations often require aggregated information (e.g., by project, region). Here we:\n\nGroup by Project Name, Planning Region, Planning Area, Property Type, and Type of Sale.\nCompute:\n\nTotal Units Sold (sum of No. of Units),\nTotal Area (sum of Area (sqm)),\nMedian Unit Price (median of Unit Price ($ psm)),\nMedian Transacted Price (median of Transacted Price ($)).\n\n\nWe will use two key dplyr verbs: group_by() and summarise().\n\nWithout using the pipe:\n\n# Group the data\nrealis2018_grouped &lt;- group_by(\n  realis2018, \n  `Project Name`,\n  `Planning Region`, \n  `Planning Area`, \n  `Property Type`, \n  `Type of Sale`\n)\n\n# Summarise the grouped data\nrealis2018_summarised &lt;- summarise(\n  realis2018_grouped, \n  `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n  `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n  `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n  `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE)\n)\n\n\n\nWith the pipe:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(\n    `Project Name`,\n    `Planning Region`,\n    `Planning Area`,\n    `Property Type`,\n    `Type of Sale`\n  ) %&gt;%\n  summarise(\n    `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n    `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n    `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n    `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE)\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemap-package",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "3. Designing Treemaps with treemap Package",
    "text": "3. Designing Treemaps with treemap Package\n\n3.1 Subset the Data (Resale Condominium)\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(\n    `Property Type` == \"Condominium\", \n    `Type of Sale` == \"Resale\"\n  )\n\n\n\n3.2 Basic Arguments in treemap()\n\nindex: defines the hierarchical structure (e.g., Region → Area → Project).\nvSize: a numeric column that defines the rectangle sizes.\nvColor: a numeric column that defines the rectangle colors.\ntype: how color values will be interpreted.\n\n\nFirst Attempt\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\nNotice that without specifying type, treemap assumes type = \"index\", which produces unexpected colors. We fix this below.\n\n\n\n\n3.3 Correct Color Mapping with type = \"value\"\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"value\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\n\n3.4 Color Palettes\nWhen type = \"value\", the default color palette is a diverging palette. You can manually choose a palette using the palette argument.\n\nExample: “RdYlBu”\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"value\",\n  palette = \"RdYlBu\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\nBecause our values are all positive, you may not see the full gradient (e.g., no reds if all prices are above zero).\n\n\n\n3.5 “manual” Type\nWith type = \"manual\", the data is linearly mapped onto the palette. A single-hue palette like “Blues” is more intuitive if all values are positive.\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"manual\",\n  palette = \"Blues\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\n\n3.6 Treemap Layout Algorithms\n\n“pivotSize” (default) respects the order of the data but is slightly less balanced visually.\n“squarified” often produces more balanced aspect ratios.\n\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"manual\",\n  palette = \"Blues\",\n  algorithm = \"squarified\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\n\nUsing sortID\nWhen using \"pivotSize\", you can specify sortID to control the rectangle order:\n\ntreemap(\n  realis2018_selected,\n  index = c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"manual\",\n  palette = \"Blues\",\n  algorithm = \"pivotSize\",\n  sortID = \"Median Transacted Price\",\n  title = \"Resale Condominium by Planning Region and Area, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemapify",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#designing-treemaps-with-treemapify",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "4. Designing Treemaps with treemapify",
    "text": "4. Designing Treemaps with treemapify\nThe treemapify package uses the ggplot2 framework for treemaps. It provides geom_treemap() and additional layers like geom_treemap_subgroup_border() for hierarchical boundaries.\n\n4.1 Basic Treemap\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`\n  )\n) + \n  geom_treemap() +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  labs(title = \"Basic Treemap with treemapify (2018 Data)\",\n       fill = \"Median Unit Price ($ psm)\")\n\n\n\n\n\n\n\n\n\n\n4.2 Defining a Hierarchy\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    subgroup = `Planning Region`\n  )\n) + \n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"grey20\", size = 2) +\n  labs(\n    title = \"Treemap Grouped by Planning Region\",\n    fill = \"Median Unit Price ($ psm)\"\n  )\n\n\n\n\n\n\n\n\nTo add another hierarchy level (e.g., Planning Area):\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    subgroup = `Planning Region`,\n    subgroup2 = `Planning Area`\n  )\n) + \n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"grey20\") +\n  geom_treemap_subgroup2_border(colour = \"grey40\", size = 2) +\n  labs(\n    title = \"Treemap Grouped by Planning Region & Area\",\n    fill = \"Median Unit Price ($ psm)\"\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#designing-interactive-treemaps-with-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#designing-interactive-treemaps-with-d3treer",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "5. Designing Interactive Treemaps with d3treeR",
    "text": "5. Designing Interactive Treemaps with d3treeR\n\n5.1 Building the Interactive Treemap\nFirst, create a treemap object with treemap:\n\ntm &lt;- treemap(\n  realis2018_summarised,\n  index = c(\"Planning Region\", \"Planning Area\"),\n  vSize = \"Total Unit Sold\",\n  vColor = \"Median Unit Price ($ psm)\",\n  type = \"value\",\n  title = \"Private Residential Property Sold, 2018\",\n  title.legend = \"Median Unit Price (S$ per sq. m)\"\n)\n\n\n\n\n\n\n\n\nThen convert it to an interactive visualization with d3tree():\n\nd3tree(\n  tm,\n  rootname = \"Singapore\"\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#more-exploration",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#more-exploration",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "6. More exploration",
    "text": "6. More exploration\n\n6.1 Adding Labels Inside Treemap (treemapify)\nYou can add text labels to each rectangle using geom_treemap_text():\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    label = `Planning Area`\n  )\n) + \n  geom_treemap() +\n  geom_treemap_text(\n    colour = \"white\",\n    place = \"centre\",\n    grow = TRUE\n  ) +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  labs(\n    title = \"Treemap with Labels (Planning Area)\",\n    fill = \"Median Unit Price ($ psm)\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.2 Filtering for Top N Areas\nSuppose you only want to visualize the top 10 Planning Areas by Total Unit Sold:\n\ntop10_areas &lt;- realis2018_selected %&gt;%\n  group_by(`Planning Area`) %&gt;%\n  summarise(total_unit_sold = sum(`Total Unit Sold`)) %&gt;%\n  slice_max(order_by = total_unit_sold, n = 10) %&gt;%\n  pull(`Planning Area`)\n\ntop10_data &lt;- realis2018_selected %&gt;%\n  filter(`Planning Area` %in% top10_areas)\n\nggplot(\n  data = top10_data,\n  aes(\n    area = `Total Unit Sold`,\n    fill = `Median Unit Price ($ psm)`,\n    label = `Planning Area`\n  )\n) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\", place = \"centre\", grow = TRUE) +\n  labs(title = \"Top 10 Planning Areas by Total Unit Sold (2018)\")\n\n\n\n\n\n\n\n\n\n\n6.3 Comparing Two Property Types Side by Side\nYou can create two treemaps and place them side by side using patchwork or cowplot. For instance, comparing Condominium vs. Executive Condominium:\n\npacman::p_load(patchwork)\n\ncondo_data &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\")\n\nec_data &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Executive Condominium\")\n\nplot_condo &lt;- ggplot(\n  data = condo_data,\n  aes(area = `Total Unit Sold`, fill = `Median Unit Price ($ psm)`)\n) + \n  geom_treemap() +\n  labs(title = \"Condominium\")\n\nplot_ec &lt;- ggplot(\n  data = ec_data,\n  aes(area = `Total Unit Sold`, fill = `Median Unit Price ($ psm)`)\n) + \n  geom_treemap() +\n  labs(title = \"Executive Condominium\")\n\nplot_condo\n\n\n\n\n\n\n\n\n\nplot_ec\n\n\n\n\n\n\n\n\n\n\n6.4 Highlighting Specific Regions (Conditional Coloring)\nYou might want to color certain regions differently if they meet certain criteria, e.g., a Median Unit Price above $20,000 psm:\n\nrealis2018_selected &lt;- realis2018_selected %&gt;%\n  mutate(\n    PriceCategory = if_else(\n      `Median Unit Price ($ psm)` &gt; 20000, \n      \"High Price\", \n      \"Others\"\n    )\n  )\n\nggplot(\n  data = realis2018_selected,\n  aes(\n    area = `Total Unit Sold`,\n    fill = PriceCategory\n  )\n) + \n  geom_treemap() +\n  scale_fill_manual(values = c(\"High Price\" = \"red\", \"Others\" = \"grey70\")) +\n  labs(\n    title = \"Conditional Coloring: High vs. Other Price Levels\",\n    fill = \"Price Category\"\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#reference",
    "title": "Hands-on Exercise 5e: Treemap Visualization with R",
    "section": "7. Reference",
    "text": "7. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "",
    "text": "Heatmaps are a powerful tool to visualise multivariate data. They help reveal patterns, clusters, and correlations among variables by mapping numerical values to colors. In this document, we demonstrate how to create both static and interactive heatmaps in R using data from the World Happiness Report (2018). We also provide an extra analysis—a correlation matrix heatmap—to offer additional insights into the relationships between the happiness indicators.\n\n\nWe load the necessary R packages: seriation, dendextend, heatmaply, tidyverse, and RColorBrewer.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, RColorBrewer)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names, and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#overview",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "",
    "text": "Heatmaps are a powerful tool to visualise multivariate data. They help reveal patterns, clusters, and correlations among variables by mapping numerical values to colors. In this document, we demonstrate how to create both static and interactive heatmaps in R using data from the World Happiness Report (2018). We also provide an extra analysis—a correlation matrix heatmap—to offer additional insights into the relationships between the happiness indicators.\n\n\nWe load the necessary R packages: seriation, dendextend, heatmaply, tidyverse, and RColorBrewer.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, RColorBrewer)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names, and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#static-heatmap-with-base-r",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#static-heatmap-with-base-r",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "2. Static Heatmap with Base R",
    "text": "2. Static Heatmap with Base R\nFirst, we create static heatmaps using the base R heatmap() function. We illustrate two examples: one without clustering dendrograms and one with the default clustering.\n\n# Static heatmap without dendrograms\nheatmap(wh_matrix, Rowv = NA, Colv = NA, \n        main = \"Static Heatmap (No Clustering)\")\n\n\n\n\n\n\n\n# Static heatmap with default hierarchical clustering\nheatmap(wh_matrix, \n        main = \"Static Heatmap with Clustering\")\n\n\n\n\n\n\n\n\nTo enhance interpretability—especially when variables have different scales—we scale the matrix by columns.\n\nheatmap(wh_matrix,\n        scale = \"column\",\n        cexRow = 0.6, \n        cexCol = 0.8,\n        margins = c(10, 4),\n        main = \"Column-Scaled Static Heatmap\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#interactive-heatmap-with-heatmaply",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#interactive-heatmap-with-heatmaply",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "3. Interactive Heatmap with Heatmaply",
    "text": "3. Interactive Heatmap with Heatmaply\nThe heatmaply package enables interactive heatmaps. Below are several examples that include data transformation and clustering options.\n\n3.1 Basic Interactive Heatmap\n\nheatmaply(wh_matrix,\n          main = \"Interactive Heatmap of World Happiness Data\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\n3.2 Data Transformation Methods\nTransforming the data can make variables on different scales comparable. Here are examples of scaling (column-wise), normalising, and percentising.\n\nColumn Scaling\n\nheatmaply(wh_matrix,\n          scale = \"column\",\n          main = \"Interactive Heatmap with Column Scaling\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\nNormalisation\n\nheatmaply(normalize(wh_matrix),\n          main = \"Interactive Heatmap with Normalisation\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\nPercentising\n\nheatmaply(percentize(wh_matrix),\n          main = \"Interactive Heatmap with Percentising\",\n          fontsize_row = 5)\n\n\n\n\n\n\n\n\n3.3 Clustering and Optimal Ordering\nWe can further improve the heatmap by clustering the rows (countries) using hierarchical clustering (with the “average” method) and applying an optimal leaf ordering via seriation.\n\nheatmaply(normalize(wh_matrix),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          seriate = \"OLO\",\n          k_row = 3,\n          main = \"Interactive Heatmap with Clustering and Optimal Leaf Ordering\",\n          fontsize_row = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#working-with-colour-palettes",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#working-with-colour-palettes",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "4. Working with Colour Palettes",
    "text": "4. Working with Colour Palettes\nTo enhance the visual appeal, we can change the color palette. In the example below, we use the “Blues” palette from RColorBrewer.\n\nheatmaply(normalize(wh_matrix),\n          seriate = \"none\",\n          colors = RColorBrewer::brewer.pal(9, \"Blues\"),\n          k_row = 5,\n          margins = c(NA, 200, 60, NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main = \"World Happiness Data (Normalised)\",\n          xlab = \"Happiness Indicators\",\n          ylab = \"Countries\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#correlation-matrix-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#correlation-matrix-heatmap",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "5. Correlation Matrix Heatmap",
    "text": "5. Correlation Matrix Heatmap\nTo further explore the relationships among the variables, we calculate a correlation matrix and display it as an interactive heatmap. This visualization highlights which indicators are strongly correlated.\n\n# Compute the correlation matrix\ncorr_matrix &lt;- cor(wh_matrix, use = \"complete.obs\")\n\n# Plot an interactive correlation heatmap using a yellow–orange–red palette\nheatmaply(corr_matrix,\n          colors = RColorBrewer::brewer.pal(9, \"YlOrRd\"),\n          main = \"Correlation Matrix of Happiness Indicators\",\n          xlab = \"Indicators\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#reference",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data: Key Concepts and Insights",
    "section": "6. Reference",
    "text": "6. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will explore ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#overview",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will explore ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "2. Plotting Ternary Diagram with R",
    "text": "2. Plotting Ternary Diagram with R\n\n2.1 Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2018\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n2.2 Plotting an interative ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\",\n  # tooltips: displaying details on the underlying data for each planning area/subzone\n  text = ~paste(\"Total Population:\", TOTAL,\n                 \"&lt;br&gt;Young:\", YOUNG,\n                 \"&lt;br&gt;Active:\", ACTIVE,\n                 \"&lt;br&gt;Old:\", OLD),\n  hoverinfo = \"text\"\n) %&gt;%\n  layout(\n    annotations = label(\"Population Composition\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#reference",
    "title": "Hands-on Exercise 5a: Creating Ternary Plot with R",
    "section": "3. Reference",
    "text": "3. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#overview",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "2. Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "2. Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\n\n2.1 Plot error bars of maths scores by race\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n2.2 Plotting standard error bars of point estimates\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n2.3 Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\n\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\n\n\n2.4 Visualizing the uncertainty of point estimates with interactive error bars\nPlot interactive error bars for the 99% confidence interval of mean maths score by race with the code below.\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "3. Visualising Uncertainty: ggdist package",
    "text": "3. Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n3.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n3.2 Set confidence interval\nIn the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n3.3 Setting multiple confidence intervals\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(\n    .width = c(0.95, 0.99),  # Includes both 95% and 99% CIs\n    .point = median,\n    .interval = qi\n  ) +\n  labs(\n    title = \"Visualising Confidence Intervals of Median Math Score\",\n    subtitle = \"Median Point + 95% and 99% Confidence Intervals\"\n  )\n\n\n\n\n\n\n\n\n\n\n3.4 Colouring the confidence intervals\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "4. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "4. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nLaunch the ungeviz package in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#reference",
    "title": "Hands-on Exercise 4c: Visualising Uncertainty",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "",
    "text": "The following R packages will be used:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a ggplot2 extension spacially desgin for visualising distribution and uncertainty\n\nThe code chunk below will be used load these R packages.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#getting-started",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "",
    "text": "The following R packages will be used:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a ggplot2 extension spacially desgin for visualising distribution and uncertainty\n\nThe code chunk below will be used load these R packages.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "2. Visualising Distribution with Ridgeline Plot",
    "text": "2. Visualising Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n2.1 Plotting ridgeline graph: ggridges method\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nggplot(exam_data, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n2.2 Varying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nVarying fill coloursTransparency\n\n\n\nggplot(exam_data, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"English score\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    fill = \"blue\",      # Choose a constant fill color\n    alpha = 0.5         # Set transparency level (0 = fully transparent, 1 = opaque)\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Mapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n2.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  # Use a brewer scale with pastel colors and custom labels for the quartiles\n  scale_fill_brewer(name = \"Quartile Range\", \n                    palette = \"Pastel1\", \n                    labels = c(\"0–25%\", \"25–50%\", \"50–75%\", \"75–100%\")) +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"white\", color = NA),\n        panel.grid.major = element_line(color = \"grey85\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  # Use more visually appealing muted colors with transparency\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#D55E00A0\",  # Muted orange\n               \"#999999A0\",  # Soft gray\n               \"#0072B2A0\"), # Muted blue\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  # Softer theme with a light background for better contrast\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"white\", color = NA),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.key = element_rect(fill = \"white\", color = NA)\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "3. Visualising Distribution with Raincloud Plot",
    "text": "3. Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n3.1 Plotting a Half Eye graph\nirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n3.2 Adding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\n\n3.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n3.4 Finishing touch\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#reference",
    "title": "Hands-on Exercise 4a: Visualising Distribution",
    "section": "4. Reference",
    "text": "4. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "Beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#getting-started",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "Beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "2. Interactive Data Visualisation - ggiraph methods",
    "text": "2. Interactive Data Visualisation - ggiraph methods\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n2.1 Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package.\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nNotice that two steps are involved.\n\nFirst, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph.\nThen, girafe() will be used to generate an svg (scalable vector graphics) object to be displayed on an html page.\n\n\n\n2.2 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\n2.3 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold. Refer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n2.4 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n2.5 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n2.6 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n2.7 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n2.8 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#more-interactive-plots",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#more-interactive-plots",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3. More interactive plots",
    "text": "3. More interactive plots\n\n3.1 Coordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\n\n3.2 Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n\n3.3 Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n3.4 Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\nInteractive: Click on the colour symbol at the legend.\n\n\n3.5 Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n3.6 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\nThings to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\n\nVisit this link to learn more about crosstalk."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "4. Interactive Data Visualisation - crosstalk methods!",
    "text": "4. Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n4.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n4.2 Linked brushing: crosstalk method\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#reference",
    "title": "Hands-on Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "2. R Graphics VS ggplot",
    "text": "2. R Graphics VS ggplot\nFirst, let us compare how R Graphics, the core graphical functions of Base R and ggplot plot a simple histogram.\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "3. Grammar of Graphics",
    "text": "3. Grammar of Graphics\nGrammar of Graphics defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\nThere are two principles in Grammar of Graphics, they are:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mapping\n\nA good grammar of graphics will allow us to:\n\nGain insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox 1978).\nProvide a strong foundation for understanding a diverse range of graphics.\nGuide us on what a well-formed or correct graphic looks like.\n\nNote: there will still be many grammatically correct but nonsensical graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "4. Essential Grammatical Elements in ggplot2: data",
    "text": "4. Essential Grammatical Elements in ggplot2: data\n\nggplot(data=exam_data) # ggplot() initializes a ggplot object.\n# output:A blank canvas\n\n\n# Aesthetic mappings\nggplot(data=exam_data, \n       aes(x= MATHS))\n# output: ggplot that includes the x-axis and the axis’s label."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-geom",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5. Essential Grammatical Elements in ggplot2: geom",
    "text": "5. Essential Grammatical Elements in ggplot2: geom\nGeometric objects are the actual marks we put on a plot. Examples include:\n\n\nA plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator.\nFor complete list, please refer to here.\n\n\n5.1 Geometric Objects: geom_bar\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n5.2 Geometric Objects: geom_dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\n\n\n\n\n\n\n\n\nUnderstanding the parameters:\n\nbinwidth: The default binwidth is 1.\n\nEach bin will cover an interval of 1 unit on the x-axis.\nFor example, if your x-axis represents test scores ranging from 0 to 100, the bins will be [0-1), [1-2), [2-3), …, [99-100).\nbinwidth = 2.5 means that test scores are grouped into intervals of 2.5 units e.g. [0-2.5), [2.5, 5)…\n\nscale_y_continuous() is used to turn off the y-axis.\n\nThe range of y-axis is 0-1 which can can potentially distort the interpretation of the data.\n\n\n\n\n\n5.3 Geometric Objects: geom_histogram\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 25)\n\n\n\n\n\n\n\n\nThe default bin is 30. By setting bins = 25, the number of bins is consistent with the test scores along the x-axis.\n\n\n5.4 Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 25,\nfill argument is used to shade the histogram with light blue color, and\ncolor argument is used to change the outline colour of the bars in black\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\n\n\n\n\n\n\n\n5.5 Modifying a geometric object by changing aes()\nThe code chunk below changes the interior colour of the histogram (i.e. fill) by using sub-group of aesthetic().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric.\n\n\n5.6 Geometric Objects: geom-density()\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of MATHS scores in a kernel density estimate plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density() \n\n\n\n\n\n\n\n\nThe code chunk below plots two kernel density lines by using colour or fill arguments of aes()\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\n5.7 Geometric Objects: geom_boxplot()\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\nThe code chunk below plots boxplots by using geom_boxplot().\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()\n\n\n\n\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of MATHS scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\n\n5.8 Geometric Objects: geom_violin()\ngeom_violin() is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of Maths score by gender in violin plot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n5.9 Geometric Objects: geom_point()\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\n\n\n\n\n5.10 geom objects can be combined\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "6. Essential Grammatical Elements in ggplot2: stat",
    "text": "6. Essential Grammatical Elements in ggplot2: stat\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n6.1 Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n6.2 Working with stat - the stat_summary() method\nThe code chunk below adds mean values by using stat_summary() function and overriding the default geom.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour =\"red\",        \n               size=4)               \n\n\n\n\n\n\n\n\n\n\n6.3 Working with stat - the geom() method\nThe code chunk below adding mean values by using geom_() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour=\"red\",          \n             size=4)  \n\n\n\n\n\n\n\n\nSections 6.2 and 6.3 produce the same output. Both approaches add red points representing the mean values to the boxplots, but these are achieved through slightly different syntax.\n\n\n6.4 Adding a best fit curve on a scatterplot?\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve. In the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\n\n\n\n\n\n\n\nNote: The default method used is loess.\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "7. Essential Grammatical Elements in ggplot2: facets",
    "text": "7. Essential Grammatical Elements in ggplot2: facets\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_grid() and facet_wrap().\n\n7.1 Working with facet_wrap()\nfacet_wrap() wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nThe code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n7.2 facet_grid() function\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\nThe code chunk below plots a trellis plot using facet_grid()\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "8. Essential Grammatical Elements in ggplot2: Coordinates",
    "text": "8. Essential Grammatical Elements in ggplot2: Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use. They are:\n\ncoord_cartesian(): The default cartesian coordinate system, where you specify x and y values (e.g., allows you to zoom in or out).\ncoord_flip(): A cartesian system with the x and y axes flipped.\ncoord_fixed(): A cartesian system with a “fixed” aspect ratio (e.g., 1.78 for a “widescreen” plot).\ncoord_quickmap(): A coordinate system that approximates a good aspect ratio for maps.\n\n\n8.1 Working with Coordinate\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe code chunk below flips the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n8.2 Changing the y- and x-axis range\nThe scatterplot on the below is slightly misleading because the y-aixs and x-axis range are not equal.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\n\n\n\n\n\nThis is better\nThe code chunk below fixed both the y-axis and x-axis range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "9. Essential Grammatical Elements in ggplot2: themes",
    "text": "9. Essential Grammatical Elements in ggplot2: themes\nThemes control elements of the graph not related to the data. For example:\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\nBuilt-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\nDefault themeClassic themeMinimal theme\n\n\nThe code chunk below plot a horizontal bar chart using theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\nA horizontal bar chart plotted using theme_classic().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nA horizontal bar chart plotted using theme_minimal().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#reference",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "10. Reference",
    "text": "10. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608 VAA 🎨",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! I’m Sindy! 👋\nSo I just wrapped up ISSS608 Visual Analytics with Professor Kam Tin Seong, and this site is basically my digital scrapbook from the whole experience.\nStarted as a total R newbie (seriously, what’s a tibble?) and somehow managed to create some visualizations I’m actually proud of! This course took me from R basics to building interactive dashboards that actually work.\nYou’ll find my projects, exercises, and probably a few happy accidents throughout this site. Each one represents a small victory in my data visualisation journey."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html",
    "href": "Hands-on_Ex/Hands-on_Ex00.html",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#getting-started",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "",
    "text": "Loading tidyverse into r environment by using the code chunk below.\n\npacman::p_load(tidyverse, psych)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#importing-data",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Importing data",
    "text": "Importing data\nThis is an outdated version.\n\nrealis_csv &lt;- read.csv(\"data/REALIS2019.csv\")\n\nCode chunk below uses uses read_csv() of readr to import REALIS2019.csv into r environment as a tibble data.frame.\nUse _ instead of . functions to prevent changes made to column names\n\nrealis2019 &lt;- read_csv(\"data/REALIS2019.csv\")\n\n\npopdata_fat &lt;- read_csv(\"data/PopData2019_fat.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#pivoting-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#pivoting-data",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Pivoting data",
    "text": "Pivoting data\n\n#! eval: FALSE\npopdata_long &lt;- popdata_fat %&gt;%\n  pivot_longer(c(3:21),\n               names_to = \"Age Group\",\n               values_to = \"Population\")\n\n\nMore pipes:\n\n#popdata_long &lt;- popdata_fat %&gt;%\n#  pivot_longer(c(3:21),\n#               names_to = \"Age Group\",\n#               values_to = \"Population\") %&gt;%\n#  select(\"Age Group\" == \"5_to_9\")\n\n\n# Filter the dataset for a specific Age Group, e.g., \"0_to_4\"\npopdata_filtered &lt;- popdata_long %&gt;%\n  filter(`Age Group` == \"0_to_4\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#save-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#save-data",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Save data",
    "text": "Save data\nrds: r native file format - always good to save as rds format\n\nwrite_rds(popdata_fat, \"data/rds/popdata_fat.rds\")\nwrite_rds(popdata_long, \"data/rds/popdata_long.rds\")\n\n{r, eval=FALSE}: only display code, does not run the code\n{r, echo=FALSE}: you run the code in the background, without displaying code"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#working-with-dplyr",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#working-with-dplyr",
    "title": "Hands-on Exercise 0: Working with tidyverse",
    "section": "Working with dplyr",
    "text": "Working with dplyr\n\nSelecting columns\n\nrealis2019_selected &lt;- realis2019 %&gt;%\n  select(`Project Name`,\n         `Transacted Price ($)`,\n         `Property Type`,\n         `Type of Sale`,\n         `Unit Price ($ psm)`)\n\n\n\nFiltering columns\n\nrealis2019_filtered &lt;- realis2019_selected %&gt;%\n  filter(`Property Type` == \"Condominium\" |\n           `Property Type` == \"Apartment\") %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  filter(`Unit Price ($ psm)` &lt;= 13000)\n\n\n\nCombining select and filter with pipe\nWe can also combine the above two operations into a single call.\n\nrealis2019_end &lt;- realis2019 %&gt;%\n  select(`Project Name`,\n         `Transacted Price ($)`,\n         `Property Type`,\n         `Type of Sale`,\n         `Unit Price ($ psm)`) %&gt;%\n  filter(`Property Type` == \"Condominium\" |\n           `Property Type` == \"Apartment\") %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  filter(`Unit Price ($ psm)` &lt;= 13000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "2. Beyond ggplot2 Annotation: ggrepel",
    "text": "2. Beyond ggplot2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nDefaultggrepel\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID),\n                   hjust = .5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\nWe simply replace geom_label() by geom_label_repel()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "3. Beyond ggplot2 Themes",
    "text": "3. Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n3.1 Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, The Economist theme is used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\nIt also provides some extra geoms and scales for ‘ggplot2’.\n\nScatter plotBar chart\n\n\n\nggplot(data = exam_data, aes(x = MATHS, y = ENGLISH, color = GENDER)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_economist() +\n  ggtitle(\"English vs. Maths Scores by Gender\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, aes(x = MATHS, fill = GENDER)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_wsj() +\n  ggtitle(\"Student Count by Maths and Gender\") +\n  theme_wsj() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Working with hrbthems package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 25)) + # Set x-axis breaks at 25-unit intervals\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "4. Beyond Single Graph",
    "text": "4. Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, you will learn how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\n\nGraph 1Graph 2Graph 3\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np1\n\n\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=25, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np2\n\n\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\np3\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 Creating Composite Graphics: pathwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, I am going to shared with you an ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n4.2 Combining two ggplot2 graphs\nFigure below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n4.3 Combining three ggplot2 graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n/ operator to stack two ggplot2 graphs,\n| operator to place the plots beside each other,\n() operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\n\n\n4.4 Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n\n\n4.5 Creating figure with inset\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n4.6 Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#reference",
    "title": "Hands-on Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics.\n\n\n\nLearn how to create animated data visualizations using gganimate and plotly in R.\nGain hands-on experience in reshaping data with the tidyr package and processing, wrangling, and transforming data with the dplyr package.\n\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#overview",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics.\n\n\n\nLearn how to create animated data visualizations using gganimate and plotly in R.\nGain hands-on experience in reshaping data with the tidyr package and processing, wrangling, and transforming data with the dplyr package.\n\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#getting-started",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 Loading the R packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n2.2 Importing the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nCode chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_at() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\nInstead of using mutate_at(), across() can be used to derive the same outputs."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "3. Animated Data Visualisation: gganimate methods",
    "text": "3. Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n3.1 Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n3.2 Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "4. Animated Data Visualisation: plotly",
    "text": "4. Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n4.1 Building an animated bubble plot: ggplotly() method\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component for controlling the animation\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk below.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n4,2 Building an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#reference",
    "title": "Hands-on Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\n\n\n\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\ngghistostats(): This function creates a histogram with statistical annotations.\n\nx = ENGLISH: Specifies the variable (English scores) to be analyzed.\ntype = \"bayes\": Indicates a Bayesian one-sample test is conducted.\ntest.value = 60: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.\nxlab = \"English scores\": Labels the x-axis as “English scores.”\n\n\n\n\nHistogram: Displays the distribution of English scores with gray bars.\nY-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.\nDashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, \\(\\hat{\\mu}_{MAP}\\)), which is approximately 74.74.\nStatistical Annotations:\n\n\\(\\log_e(BF_{01}) = -31.45\\): The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).\n\\(\\delta_{\\text{difference}}^{\\text{posterior}} = 7.16\\): The estimated mean difference between the sample mean and 60.\n\\(CI^{ETI}_{95\\%} [5.54, 8.75]\\): The 95% credible interval (Highest Density Interval) for the mean difference.\n\\(r^{JZS}_{Cauchy} = 0.71\\): The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.\n\n\n\n\n\nThe English scores are right-skewed and centered around 74.74, which is significantly higher than the test value of 60.\nThe negative log Bayes factor (-31.45) provides overwhelming evidence against the null hypothesis.\nThe credible interval [5.54, 8.75] indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above 60."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#getting-started",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\n\n\n\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\ngghistostats(): This function creates a histogram with statistical annotations.\n\nx = ENGLISH: Specifies the variable (English scores) to be analyzed.\ntype = \"bayes\": Indicates a Bayesian one-sample test is conducted.\ntest.value = 60: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.\nxlab = \"English scores\": Labels the x-axis as “English scores.”\n\n\n\n\nHistogram: Displays the distribution of English scores with gray bars.\nY-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.\nDashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, \\(\\hat{\\mu}_{MAP}\\)), which is approximately 74.74.\nStatistical Annotations:\n\n\\(\\log_e(BF_{01}) = -31.45\\): The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).\n\\(\\delta_{\\text{difference}}^{\\text{posterior}} = 7.16\\): The estimated mean difference between the sample mean and 60.\n\\(CI^{ETI}_{95\\%} [5.54, 8.75]\\): The 95% credible interval (Highest Density Interval) for the mean difference.\n\\(r^{JZS}_{Cauchy} = 0.71\\): The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.\n\n\n\n\n\nThe English scores are right-skewed and centered around 74.74, which is significantly higher than the test value of 60.\nThe negative log Bayes factor (-31.45) provides overwhelming evidence against the null hypothesis.\nThe credible interval [5.54, 8.75] indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above 60."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#bayes-factor",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#bayes-factor",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "2. Bayes Factor",
    "text": "2. Bayes Factor\n\n2.1 Unpacking the Bayes Factor\n\nA Bayes Factor (BF) is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThe Bayes Factor provides a way to evaluate data in favor of a null hypothesis and to incorporate external information in doing so. It quantifies the weight of the evidence in favor of a given hypothesis.\nWhen comparing two hypotheses, \\(H_1\\) (the alternative hypothesis) and \\(H_0\\) (the null hypothesis), the Bayes Factor is often written as \\(BF_{10}\\). Mathematically, it is defined as:\n\\[\nBF_{10} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)}\n\\]\n\nwhere:\n\n\\(P(D \\mid H_1)\\) is the probability of the observed data given that the alternative hypothesis is true.\n\\(P(D \\mid H_0)\\) is the probability of the observed data given that the null hypothesis is true.\nA Bayes Factor greater than 1 indicates evidence in favor of \\(H_1\\), while a Bayes Factor less than 1 supports \\(H_0\\).\n\nThe Schwarz criterion (Bayesian Information Criterion, BIC) is one of the simplest ways to approximate the Bayes Factor.\n\n\n2.2 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#hypothesis-testing",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#hypothesis-testing",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "3. Hypothesis Testing",
    "text": "3. Hypothesis Testing\n\n3.1 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\n\nx = GENDER → Categorical variable (independent variable) representing gender groups (Male & Female).\ny = MATHS → Numeric variable (dependent variable) representing Maths scores.\ntype = \"np\" → Specifies a nonparametric test (Mann-Whitney U test, also known as the Wilcoxon rank-sum test) instead of a parametric t-test.\n\n\n\n\nViolin Plots:\n\nShow the distribution of Maths scores for Female (left, teal) and Male (right, orange).\nThe width of the violin represents the density of data points.\n\nBoxplots Inside Violin Plots:\n\nThe black box within each violin represents the interquartile range (IQR) (middle 50% of data).\nThe black horizontal line inside the box represents the median.\nThe whiskers extend to the smallest and largest values within 1.5 times the IQR.\n\nIndividual Data Points:\n\nEach dot represents an individual student’s Maths score.\nProvides insight into the spread and density of scores.\n\nMann-Whitney U Test Results (Top Annotation):\n\n\\(W_{Mann-Whitney} = 13011.00\\) → The Mann-Whitney U test statistic.\n\\(p = 0.91\\) → High p-value suggests no significant difference between the two groups.\n\\(\\hat{r}_{biserial}^{rank} = 7.04e-03\\) → Rank-biserial correlation effect size (very small effect).\n\\(CI_{95\\%} [-0.12, 0.13]\\) → 95% confidence interval for the effect size.\n\\(n_{obs} = 322\\) → Total number of observations (170 females, 152 males).\n\n\n\n\n\nThe p-value (0.91) is very high, suggesting no statistically significant difference in Maths scores between genders.\nThe confidence interval [-0.12, 0.13] includes zero, reinforcing the lack of a meaningful effect.\nThe effect size is nearly zero, further indicating no meaningful difference in Maths performance based on gender.\n\n\n\n\n\n\n3.2 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode breakdownExplanation of outputKey Interpretation\n\n\n\ntype = \"p\" → Performs a parametric test (Welch’s ANOVA for unequal variances).\nmean.ci = TRUE → Displays mean and confidence intervals for each group.\npairwise.comparisons = TRUE → Conducts post-hoc pairwise comparisons (e.g., Games-Howell test for unequal variances).\npairwise.display = \"s\"\n\n\"ns\" → Shows only non-significant comparisons.\n\"s\" → Shows only significant comparisons (used in this case).\n\"all\" → Shows all comparisons.\n\np.adjust.method = \"fdr\" → Adjusts p-values for multiple comparisons using the False Discovery Rate (FDR) correction.\nmessages = FALSE → Suppresses console messages.\n\n\n\n\n1. One-Way Welch ANOVA Results\n\n\\(F_{\\text{Welch}}(3, 23.8) = 10.15\\) → The Welch’s ANOVA test statistic\n\n\\(p = 1.71 \\times 10^{-4}\\) → The p-value, indicating a statistically significant difference among groups\n\n\\(\\hat{\\omega}^2_p = 0.50\\) → Effect size (moderate to large effect)\n\n\\(CI_{95\\%} [0.21, 1.00]\\) → Confidence interval for the effect size\n\n\\(n_{\\text{obs}} = 322\\) → Number of observations\n\nThis suggests that there is a significant difference in English scores across racial groups.\n\n\n2. Violin & Boxplots\n\nThe violin plot shows the distribution of scores.\n\nThe boxplot (inside the violin plot) summarizes:\n\nThe median (middle line in the box)\nThe interquartile range (box)\nThe whiskers (spread of data)\nThe mean (red dot with label)\n\n\n\n\n3. Post-Hoc Pairwise Comparisons\n\nThe Games-Howell test is used for post-hoc analysis (adjusted for multiple comparisons).\n\nOnly significant comparisons are displayed.\n\nThe p-value adjustment method used is False Discovery Rate (FDR).\n\n\n\n4. Bayesian Statistics\nAt the bottom:\n\n\\(\\log_e(BF_{01}) = -11.63\\) → Bayesian evidence against the null hypothesis\n\n\\(R^2_{\\text{posterior}} = 0.09\\) → Bayesian effect size\n\n\\(CI_{95\\%} [0.04, 0.15]\\) → Bayesian credible interval\n\n\\(r_{\\text{Cauchy}} = 0.71\\) → Cauchy prior width used\n\nThe negative log Bayes Factor \\(BF_{01}\\) suggests strong evidence against the null hypothesis, meaning there is a significant difference between the groups.\n\n\n\n\nThe one-way Welch ANOVA confirms a significant difference in English scores across racial groups.\nThe Chinese and “Other” groups have higher mean scores than the Indian and Malay groups.\nPost-hoc Games-Howell comparisons highlight significant differences.\nBayesian analysis supports the findings from the frequentist approach.\n\n\n\n\n\n\n\n3.3 Significance Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visualization for the Significance Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n3.4 Significance Test of Association (Dependence): ggbarstats() Method\nIn the code chunk below, the Maths scores are binned into a 4-class variable using the cut() function.\n\nexam1 &lt;- exam_data %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0, 60, 75, 85, 100))\n  )\n\n\nggbarstats(\n  data = exam1, \n  x = MATHS_bins, \n  y = GENDER\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#visualising-models",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "4. Visualising Models",
    "text": "4. Visualising Models\nIn this section, Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n4.1 Visualising Models\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n4.2 Importing Excel File: readxl Methods\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNote that the output object car_resale is a tibble data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#building-and-diagnosing-a-multiple-regression-model",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#building-and-diagnosing-a-multiple-regression-model",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "5. Building and Diagnosing a Multiple Regression Model",
    "text": "5. Building and Diagnosing a Multiple Regression Model\n\n5.1 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n5.2 Multiple Regression Model using lm()\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n5.3 Model Diagnostic: checking normality assumption\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n5.4 Model Diagnostic: Check model for homogeneity of variances\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n5.5 Model Diagnostic: Complete check\n\ncheck_model(model1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#visualizing-regression-results",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#visualizing-regression-results",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "6. Visualizing Regression Results",
    "text": "6. Visualizing Regression Results\n\n6.1 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n6.2 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#reference",
    "title": "Hands-on Exercise 4b: Visual Statistical Analysis",
    "section": "7. Reference",
    "text": "7. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities.\n\n\nIn this exercise, four R packages will be used.\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#overview",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities.\n\n\nIn this exercise, four R packages will be used.\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "2. FunnelPlotR methods",
    "text": "2. FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n2.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`, \n  data_type = \"SR\",       \n  limit = 95              \n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_type argument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n2.2 FunnelPlotR methods: Makeover 1\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),\n  yrange = c(0, 0.05) \n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\nxrange and yrange are used to set the range of x-axis and y-axis.\n\n\n\n2.3 FunnelPlotR methods: Makeover 2\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "3. Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "3. Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, we will build funnel plots step-by-step by using ggplot2.\n\n3.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n3.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n3.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n3.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#reference",
    "title": "Hands-on Exercise 4d: Funnel Plots for Fair Comparisons",
    "section": "4. Reference",
    "text": "4. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\n\n\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#getting-started",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\n\n\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "2. Building Correlation Matrix: pairs() method",
    "text": "2. Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\n\n2.1 Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n2.2 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\n\nLower halfUpper half\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "3. Visualising Correlation Matrix: ggcormat()",
    "text": "3. Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\n\n3.1 The basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#building-multiple-plots",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "4.Building multiple plots",
    "text": "4.Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. This feature is available in grouped_ggcorrmat().\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "5. Visualising Correlation Matrix using corrplot Package",
    "text": "5. Visualising Correlation Matrix using corrplot Package\n\n5.1 Getting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n5.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. This default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n5.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n5.4 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n5.5 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n5.6 Reorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n5.7 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         lower = \"ellipse\", \n         upper = \"number\",\n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#reference",
    "title": "Hands-on Exercise 5b: Visual Correlation Analysis",
    "section": "6. Reference",
    "text": "6. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "",
    "text": "Parallel coordinates plots are an effective way to visualize and analyze multivariate numerical data. They allow you to compare multiple variables simultaneously and reveal relationships among them—for example, how various indicators contribute to the World Happiness Index. In this exercise, we will create both static and interactive parallel coordinates plots using the World Happiness 2018 dataset. We also include a couple of additional insights through custom graphs.\n\n\nWe load the necessary R packages: GGally, parallelPlot, and tidyverse.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names (if needed), and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n# (Optional) Set country names as row identifiers if needed:\nrow.names(wh) &lt;- wh$Country\n\n# Select relevant columns for analysis (e.g., columns 7 to 12 contain numerical indicators)\nwh_selected &lt;- dplyr::select(wh, c(7:12))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#overview",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "",
    "text": "Parallel coordinates plots are an effective way to visualize and analyze multivariate numerical data. They allow you to compare multiple variables simultaneously and reveal relationships among them—for example, how various indicators contribute to the World Happiness Index. In this exercise, we will create both static and interactive parallel coordinates plots using the World Happiness 2018 dataset. We also include a couple of additional insights through custom graphs.\n\n\nWe load the necessary R packages: GGally, parallelPlot, and tidyverse.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nWe import the World Happiness 2018 dataset (saved as WHData-2018.csv), set the country names as row names (if needed), and select the relevant columns. (Adjust column indices as needed.)\n\n# Import the data\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n# (Optional) Set country names as row identifiers if needed:\nrow.names(wh) &lt;- wh$Country\n\n# Select relevant columns for analysis (e.g., columns 7 to 12 contain numerical indicators)\nwh_selected &lt;- dplyr::select(wh, c(7:12))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-ggally",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-ggally",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "2. Static Parallel Coordinates Plot with GGally",
    "text": "2. Static Parallel Coordinates Plot with GGally\nUsing the GGally package’s ggparcoord() function, we first create static parallel coordinates plots.\n\n2.1 Basic Parallel Coordinates Plot\nA simple parallel coordinates plot showing the selected numerical variables:\n\nggparcoord(data = wh, \n           columns = c(7:12)) +\n  labs(title = \"Basic Parallel Coordinates Plot\")\n\n\n\n\n\n\n\n\n\n\n2.2 Enhanced Parallel Coordinates Plot with Grouping\nHere we group observations by a variable (e.g., Region in column 2), scale variables using the uniminmax method, lower line opacity, and overlay boxplots to better reveal the distribution.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happiness Variables\") +\n  labs(x = \"Indicators\", y = \"Scaled Value\")\n\n\n\n\n\n\n\n\n\n\n2.3 Parallel Coordinates Plot with Facets\nWe can create small multiples by faceting the plot by region. This approach helps compare patterns across geographical areas.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plots by Region\") +\n  facet_wrap(~ Region) +\n  labs(x = \"Indicators\", y = \"Scaled Value\")\n\n\n\n\n\n\n\n\n\n\n2.4 Rotating and Adjusting x-axis Labels\nWhen variable names overlap on the x-axis, we can rotate and adjust their positioning using theme() from ggplot2.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plots by Region with Rotated x-axis Labels\") +\n  facet_wrap(~ Region) +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(x = \"Indicators\", y = \"Scaled Value\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#interactive-parallel-coordinates-plot-with-parallelplot",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#interactive-parallel-coordinates-plot-with-parallelplot",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "3. Interactive Parallel Coordinates Plot with parallelPlot",
    "text": "3. Interactive Parallel Coordinates Plot with parallelPlot\nThe parallelPlot package provides an interactive version of parallel coordinates plots built on htmlwidgets and d3.js.\n\n3.1 Basic Interactive Plot\nWe first create a basic interactive plot. For clarity, we select a subset of variables including the “Happiness score” (if available) along with the numerical indicators.\n\n# Adjust the data: select \"Happiness score\" (if present) and columns 7:12\nwh_interactive &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\n\n# Basic interactive parallel coordinates plot\nparallelPlot(wh_interactive,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n3.2 Rotate Axis Labels\nTo avoid overlapping axis labels, use the rotateTitle argument.\n\nparallelPlot(wh_interactive,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n3.3 Changing the Colour Scheme\nChange the default blue colour scheme to a yellow–orange–red palette using the continuousCS argument.\n\nparallelPlot(wh_interactive,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n3.4 Interactive Plot with Histograms\nDisplay histograms along each variable’s axis by setting the histoVisibility argument.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh_interactive))\nparallelPlot(wh_interactive,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-custom-theme",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#static-parallel-coordinates-plot-with-custom-theme",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "4. Static Parallel Coordinates Plot with Custom Theme",
    "text": "4. Static Parallel Coordinates Plot with Custom Theme\nUsing GGally together with additional ggplot2 theme modifications, we create a static parallel coordinates plot with a refined aesthetic.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.3,\n           boxplot = TRUE, \n           title = \"Enhanced Static Parallel Coordinates Plot\") +\n  labs(x = \"Happiness Indicators\", y = \"Scaled Value\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#reference",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot: Key Concepts and Insights",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "",
    "text": "Install and load the necessary packages:\n\npacman::p_load(scales, viridis, lubridate, ggthemes, \n               gridExtra, readxl, knitr, data.table, \n               CGPfunctions, ggHoriPlot, tidyverse, plotly)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#getting-started",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "",
    "text": "Install and load the necessary packages:\n\npacman::p_load(scales, viridis, lubridate, ggthemes, \n               gridExtra, readxl, knitr, data.table, \n               CGPfunctions, ggHoriPlot, tidyverse, plotly)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-calendar-heatmap",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "2. Plotting a Calendar Heatmap",
    "text": "2. Plotting a Calendar Heatmap\nA calendar heatmap can provide a quick visual cue of how data points (e.g., attacks, sales, events) distribute across days and hours of the week.\n\n2.1 The Data\nWe will use a CSV file (eventlog.csv) containing time-series data of cyber attacks by country.\n\n\n2.2 Import and Examine the Data\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n# Quick peek at the data\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n2.3 Data Preparation\nWe want to extract weekday and hour from the timestamp.\n\nStep 1: Write a function\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, tz = tz[1], quiet = TRUE)\n  dt &lt;- data.table(\n    source_country = sc,\n    wkday = weekdays(real_times),\n    hour  = hour(real_times)\n  )\n  return(dt)\n}\n\n\n\nStep 2: Use the function and re-factor\n\nwkday_levels &lt;- c('Saturday','Friday','Thursday','Wednesday','Tuesday','Monday','Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, .$source_country, .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    wkday = factor(wkday, levels = wkday_levels),\n    hour  = factor(hour, levels = 0:23)\n  )\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n2.4 Building the Calendar Heatmap\nAggregate the data and visualize:\n\ngrouped &lt;- attacks %&gt;%\n  count(wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, aes(hour, wkday, fill = n)) +\n  geom_tile(color = \"white\", size = 0.1) +\n  theme_tufte(base_family = \"Helvetica\") +\n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\", \n                      low = \"sky blue\", high = \"dark blue\") +\n  labs(x = NULL, y = NULL,\n       title = \"Attacks by weekday and time of day\") +\n  theme(\n    axis.ticks = element_blank(),\n    plot.title = element_text(hjust = 0.5),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 6)\n  )\n\n\n\n\n\n\n\n\n\n\n2.5 Multiple Calendar Heatmaps (Facets)\n\nStep 1: Find top 4 countries\n\nattacks_by_country &lt;- count(attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\n\nStep 2: Extract only these top 4 countries\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\nStep 3: Plot\n\nggplot(top4_attacks, aes(hour, wkday, fill = n)) +\n  geom_tile(color = \"white\", size = 0.1) +\n  theme_tufte(base_family = \"Helvetica\") +\n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                      low = \"sky blue\", \n                      high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(\n    x = NULL, \n    y = NULL, \n    title = \"Attacks on top 4 countries by weekday and time of day\"\n  ) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text.x = element_text(size = 7),\n    plot.title = element_text(hjust = 0.5),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 6)\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-cycle-plot",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "3. Plotting a Cycle Plot",
    "text": "3. Plotting a Cycle Plot\nA cycle plot highlights intra-year (or intra-cycle) patterns and helps compare across years (cycles).\n\n3.1 Data Import\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n3.2 Derive Month and Year\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels = 1:12, \n                    labels = month.abb, \n                    ordered = TRUE)\nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n3.3 Extract Target Country\n\nVietnam &lt;- air %&gt;%\n  select(Vietnam, month, year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n3.4 Compute Averages\n\nhline.data &lt;- Vietnam %&gt;%\n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(Vietnam))\n\n\n\n3.5 Plot the Cycle Plot\n\nggplot(Vietnam, aes(x = year, y = Vietnam, group = month)) + \n  geom_line(color = \"black\") +\n  geom_hline(\n    data     = hline.data, \n    aes(yintercept = avgvalue),\n    linetype = 6, \n    colour   = \"red\",\n    size     = 0.5\n  ) + \n  facet_wrap(~month, nrow = 1) +\n  scale_x_continuous(breaks = seq(2010, 2019,3)) +\n  labs(\n    x     = \"Year\",\n    y     = \"No. of Visitors\",\n    title = \"Visitor arrivals from Vietnam by air, Jan 2010–Dec 2019\"\n  ) +\n  theme_gray(base_size = 8)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#plotting-a-slopegraph",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "4. Plotting a Slopegraph",
    "text": "4. Plotting a Slopegraph\nA slopegraph compares changes between two or more time points across different categories. We’ll use newggslopegraph() from CGPfunctions.\n\n4.1 Data Import\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n4.2 Plot the Slopegraph\n\nrice %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(\n    Year, \n    Yield, \n    Country,\n    Title = \"Rice Yield of Top 11 Asian Countries\",\n    SubTitle = \"1961-1980\",\n  )\n\n\n\n\n\n\n\n\n\nTip: Converting Year to a factor helps emphasize discrete time points."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#more-plots",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#more-plots",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "5. More plots",
    "text": "5. More plots\n\n5.1 Highlighting the Maximum/Minimum in a Cycle Plot\nTo highlight extremes in the cycle plot, we can try:\n\nVietnam_extremes &lt;- Vietnam %&gt;%\n  group_by(month) %&gt;%\n  mutate(\n    max_val = max(Vietnam),\n    min_val = min(Vietnam)\n  )\n\nggplot(Vietnam_extremes, aes(x = year, y = Vietnam, group = month)) +\n  geom_line() +\n  geom_point(data = subset(Vietnam_extremes, Vietnam == max_val),\n             aes(color = \"Max\"), size = 2) +\n  geom_point(data = subset(Vietnam_extremes, Vietnam == min_val),\n             aes(color = \"Min\"), size = 2) +\n  facet_wrap(~ month) +\n  scale_color_manual(values = c(\"Max\" = \"red\", \"Min\" = \"blue\")) +\n  labs(\n    title = \"Visitor Arrivals - Highlighting Max and Min by Month\",\n    color = \"\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n5.2 Adding a Trendline to the Slopegraph\nWhile slopegraphs usually connect discrete points, we can experiment with a small data set over more than two years by adding intermediate lines:\n\nrice %&gt;%\n  filter(Year %in% c(1961,1970,1980)) %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  newggslopegraph(\n    Year, \n    Yield, \n    Country,\n    Title = \"Rice Yield Over Multiple Time Points\",\n    SubTitle = \"Using Slopegraph with 1961, 1970, 1980\",\n    Caption = \"Extended example\"\n  )\n\n\n\n\n\n\n\n\n\n\n5.3 Plotting a Horizon Chart\nA horizon chart can compactly show how a time series changes relative to a baseline. The ggHoriPlot package can help.\nExample (using the arrivals_by_air.xlsx data for Vietnam):\n\nhorizon_data &lt;- Vietnam %&gt;%\n  mutate(Date = ymd(paste(year, match(month, month.abb), \"01\", sep=\"-\"))) %&gt;%\n  select(Date, Visitors = Vietnam)\n\nggplot(horizon_data, aes(x = Date, y = Visitors)) +\n  geom_horizon() +\n  theme_minimal() +\n  facet_wrap(~ \"Visitor Arrivals Horizon Chart\", ncol = 1)\n\n\n\n\n\n\n\n\n\nNote: Horizon charts are especially useful if you have many time series to stack vertically.\n\n\n\n5.4 Making an Interactive Time-Series Plot\nWe could also convert any of these ggplot objects to an interactive plot using plotly or ggiraph:\n\np &lt;- ggplot(horizon_data, aes(x = Date, y = Visitors)) +\n  geom_line() + theme_minimal()\n\nggplotly(p)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#reference",
    "title": "Hands-on Exercise 7: Visualising and Analysing Time-Oriented Data",
    "section": "6. Reference",
    "text": "6. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b.html",
    "href": "Hands-on_Ex/Hands-on_Ex08b.html",
    "title": "Hands-on Exercise 8b: Visualising Geospatial Point Data",
    "section": "",
    "text": "This chapter explores proportional symbol maps, a powerful technique for visualizing discrete spatial phenomena through the visual variable of size. These maps, also known as graduate symbol maps, come in two variants: range-graded (classed) and proportional symbols (unclassed). The focus will be on creating proportional symbol maps showing Singapore Pools’ outlets using the tmap package in R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08b.html#overview",
    "title": "Hands-on Exercise 8b: Visualising Geospatial Point Data",
    "section": "",
    "text": "This chapter explores proportional symbol maps, a powerful technique for visualizing discrete spatial phenomena through the visual variable of size. These maps, also known as graduate symbol maps, come in two variants: range-graded (classed) and proportional symbols (unclassed). The focus will be on creating proportional symbol maps showing Singapore Pools’ outlets using the tmap package in R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08b.html#getting-started",
    "title": "Hands-on Exercise 8b: Visualising Geospatial Point Data",
    "section": "2. Getting Started",
    "text": "2. Getting Started\nThe first step involves loading the necessary R packages for geospatial data handling and visualization.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex08b.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 8b: Visualising Geospatial Point Data",
    "section": "3. Geospatial Data Wrangling",
    "text": "3. Geospatial Data Wrangling\n\n3.1 The Data\nThe dataset used for this exercise is SGPools_svy21.csv, which contains information about Singapore Pools outlets. The dataset consists of seven columns, including XCOORD and YCOORD which represent the x-coordinates and y-coordinates in Singapore SVY21 Projected Coordinate System.\n\n\n3.2 Data Import and Preparation\nThe following code uses the read_csv() function from the readr package to import the data as a tibble.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nExamining the imported data structure ensures proper data loading:\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that sgpools is stored as a tibble data frame rather than a standard R data frame.\n\n\n3.3 Creating a sf Data Frame from an Aspatial Data Frame\nConverting the tabular data into a spatial format requires the st_as_sf() function from the sf package:\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThis transformation requires: - The coords argument specifying the column names for x-coordinates followed by y-coordinates - The crs argument indicating the coordinate reference system in EPSG format (3414 corresponds to Singapore SVY21 Projected Coordinate System)\nInspecting the new spatial object:\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output confirms that sgpools_sf is a point feature class with EPSG:3414 projection. The bounding box (bbox) indicates the geographical extent of the data.\n\n\n3.4 Exploring Spatial Properties of Point Data\nUnderstanding the spatial distribution of the data helps inform mapping decisions. The following code summarizes key spatial characteristics:\n\nst_bbox(sgpools_sf)\n\n     xmin      ymin      xmax      ymax \n 7844.194 26525.699 45176.575 47987.133 \n\n\nCalculating the distance between points can reveal potential clustering:\n\nnearest_neighbor &lt;- st_distance(sgpools_sf, sgpools_sf)\ndiag(nearest_neighbor) &lt;- NA\nmin_distances &lt;- apply(nearest_neighbor, 1, min, na.rm=TRUE)\nsummary(as.numeric(min_distances))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   238.2   439.5   470.3   622.8  5121.6 \n\n\nThis analysis shows the minimum, maximum, and average distances between outlets, helping determine appropriate symbol sizes for the map."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b.html#drawing-proportional-symbol-maps",
    "href": "Hands-on_Ex/Hands-on_Ex08b.html#drawing-proportional-symbol-maps",
    "title": "Hands-on Exercise 8b: Visualising Geospatial Point Data",
    "section": "4. Drawing Proportional Symbol Maps",
    "text": "4. Drawing Proportional Symbol Maps\n\n4.1 Setting Up Interactive Mode\nTo create interactive maps, the view mode of tmap is activated:\n\ntmap_mode(\"view\")\n\n\n\n4.2 Basic Interactive Point Symbol Map\nThe first step in creating a proportional symbol map is plotting simple point symbols:\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n4.3 Creating Proportional Symbols\nTo create a true proportional symbol map, a numerical variable must be assigned to the size visual attribute:\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n4.4 Handling Symbol Overlap with Transparency\nWhen mapping densely clustered points, symbol overlap can obscure patterns. Adding transparency helps mitigate this issue:\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1,\n           alpha = 0.7,\n           scale = 0.8)\n\n\n\n\n\n\nThe alpha parameter controls transparency (0 = fully transparent, 1 = fully opaque), while scale adjusts the overall size of symbols. This combination maintains proportionality while reducing visual clutter.\n\n\n4.5 Adding Categorical Color Dimension\nThe map can be enhanced by using color to represent categorical data:\n\ntm_shape(sgpools_sf)+\ntm_bubbles(fill = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n4.6 Optimizing Color Palettes for Categorical Data\nColor choice significantly impacts map readability. The palette parameter allows selection of appropriate color schemes:\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1,\n          palette = \"Dark2\") +\ntm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\nThe “Dark2” palette from ColorBrewer provides distinct colors that work well for categorical data. The legend.outside parameter prevents the legend from obscuring map data.\n\n\n4.7 Creating Faceted Maps\nFaceted maps allow comparison of subsets of data:\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sync parameter ensures all facets maintain synchronized zoom and pan settings for easier comparison.\n\n\n4.8 Adding Essential Map Elements\nProfessional maps include contextual elements that improve interpretation:\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_layout(title = \"Singapore Pools Outlets and Winnings\",\n            title.position = c(\"center\", \"top\"),\n            title.size = 1.2,\n            legend.outside = TRUE,\n            legend.title.size = 0.9,\n            legend.text.size = 0.7)\n\n\n\n\n\n\n\nThis code adds a clear title and configures legend positioning and sizing for better readability.\n\n\n4.9 Incorporating Basemaps for Context\nAdding a basemap provides valuable geographic context:\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_basemap(\"OpenStreetMap\") +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\nOpenStreetMap provides street-level detail that helps viewers locate familiar landmarks and understand the distribution of outlets in relation to urban features.\n\n\n4.10 Returning to Plot Mode\nBefore ending the session, switch tmap back to plot mode:\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08b.html#reference",
    "title": "Hands-on Exercise 8b: Visualising Geospatial Point Data",
    "section": "5. Reference",
    "text": "5. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Hands-on_Ex09.html",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "This chapter explores techniques for modeling, analyzing and visualizing network data using R. Networks are powerful mathematical structures that represent relationships between entities, making them suitable for analyzing complex systems from social interactions to organizational communication patterns.\nCore learning objectives include:\n\nCreating graph object data frames with dplyr, lubridate, and tidygraph\nBuilding network visualizations using ggraph\nComputing network metrics to quantify structural properties\nCreating advanced graph visualizations incorporating network metrics\nDeveloping interactive network visualizations with visNetwork"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#overview",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "This chapter explores techniques for modeling, analyzing and visualizing network data using R. Networks are powerful mathematical structures that represent relationships between entities, making them suitable for analyzing complex systems from social interactions to organizational communication patterns.\nCore learning objectives include:\n\nCreating graph object data frames with dplyr, lubridate, and tidygraph\nBuilding network visualizations using ggraph\nComputing network metrics to quantify structural properties\nCreating advanced graph visualizations incorporating network metrics\nDeveloping interactive network visualizations with visNetwork"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#getting-started",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 Installing and launching R packages\nThe analysis requires several network-specific packages: igraph (for core network algorithms), tidygraph (for tidy network manipulation), ggraph (for network visualization), and visNetwork (for interactive visualization). The tidyverse and lubridate packages provide additional data manipulation capabilities.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#the-data",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "3. The Data",
    "text": "3. The Data\nThe dataset comes from an oil exploration and extraction company called GAStech. It contains email communication data between 55 employees over a two-week period.\n\n3.1 The edges data\n\nGAStech-email_edges.csv contains 9063 email correspondences between employees.\n\nEach record represents a single email sent from one employee to another, with attributes including the date, subject, and other metadata.\n\n\n3.2 The nodes data\n\nGAStech_email_nodes.csv contains information about the 55 employees, including their names, departments, and job titles.\n\nThese two datasets together form a complete network representation, where employees are nodes and emails are edges connecting them.\n\n\n3.3 Importing network data from files\nFirst, import the node and edge data files using the read_csv() function from the readr package:\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n3.4 Reviewing the imported data\nExamining the structure of the imported data reveals the attributes and data types:\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output reveals that the SentDate field is treated as a “Character” data type instead of a proper date data type. This needs to be corrected before proceeding with analysis.\n\n\n\n\n3.5 Wrangling time\nConverting the date string to a proper date format and extracting the day of the week:\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nKey date manipulation concepts\n\n\n\n\nThe dmy() function from lubridate transforms text dates to Date data type\nwday() extracts the day of the week as an ordered factor when label = TRUE\nSetting abbr = FALSE returns full day names like “Monday” instead of abbreviations\nThese functions create proper temporal dimensions for network analysis\n\n\n\n\n\n3.6 Reviewing the revised date fields\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n3.7 Wrangling attributes\nIndividual email records aren’t immediately useful for visualization. Aggregating them by relevant dimensions creates a more meaningful network representation:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nUnderstanding the aggregation process\n\n\n\n\nFiltering for work-related emails focuses the analysis on professional communications\nGrouping by source, target, and weekday preserves temporal patterns\nThe Weight field counts the number of emails between each pair\nRemoving self-emails (where source = target) eliminates self-loops\nFiltering for Weight &gt; 1 focuses on repeated communications, revealing stronger relationships\n\n\n\n\n\n3.8 Reviewing the revised edges file\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "4. Creating network objects using tidygraph",
    "text": "4. Creating network objects using tidygraph\nThe tidygraph package provides a tidy API for graph/network manipulation. It conceptualizes network data as two tidy tables - one for nodes and one for edges - and allows seamless switching between them while maintaining the relational structure.\n\n4.1 The tbl_graph object\nTwo key functions for creating network objects:\n\ntbl_graph() creates a network object from separate nodes and edges data frames\nas_tbl_graph() converts various existing network data formats into a tbl_graph object\n\n\n\n4.2 The dplyr verbs in tidygraph\nThe activate() function serves as a switch between the nodes and edges tables. All dplyr verbs applied to a tbl_graph object affect only the currently active table.\nSpecial accessor functions provide access to different parts of the graph: - .N() accesses node data while manipulating edges - .E() accesses edge data while manipulating nodes - .G() accesses the entire tbl_graph object\n\n\n4.3 Using tbl_graph() to build a tidygraph data model\nCreating the network graph object:\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n4.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\nThe output shows that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges. It displays the first rows of both node and edge data, and indicates that the Node Data is currently active.\n\n\n4.5 Graph Summary Statistics\nNetwork summary statistics provide a quantitative overview of the graph structure:\n\nGAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(degree = centrality_degree()) %&gt;%\n  as_tibble() %&gt;%\n  summarise(avg_degree = mean(degree),\n            min_degree = min(degree),\n            max_degree = max(degree),\n            sd_degree = sd(degree))\n\n# A tibble: 1 × 4\n  avg_degree min_degree max_degree sd_degree\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1       25.4          1        266      45.0\n\n\nThese statistics help understand the overall connectivity patterns in the network and identify potential outliers.\n\n\n4.6 Changing the active object\nSwitching between nodes and edges using activate():\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "5. Plotting Static Network Graphs with ggraph package",
    "text": "5. Plotting Static Network Graphs with ggraph package\nThe ggraph package extends ggplot2 for network visualization, making it easier to apply familiar ggplot skills to network graphs.\nThree main components of a ggraph network visualization: - nodes (the entities in the network) - edges (the connections between entities) - layouts (the spatial arrangement of nodes)\n\n5.1 Plotting a basic network graph\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCore ggraph functions\n\n\n\n\nggraph() initializes the plot with data and layout specifications\ngeom_edge_link() draws the connections between nodes\ngeom_node_point() visualizes the nodes themselves\nThese functions mirror the layered grammar of graphics from ggplot2\n\n\n\n\n\n5.2 Changing the default network graph theme\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThe theme_graph() function provides better defaults for network visualization by removing axes, grids, and borders that aren’t relevant for networks.\n\n\n5.3 Changing the coloring of the plot\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n5.4 Working with ggraph’s layouts\nggraph supports multiple layout algorithms that determine how nodes are positioned in the visualization. Options include: star, circle, nicely (default), dh, gem, graphopt, grid, mds, sphere, randomly, fr, kk, drl and lgl.\n\n\n5.5 Comparing Network Layouts\nDifferent layout algorithms reveal different aspects of network structure:\n\nlayout_options &lt;- c(\"fr\", \"kk\", \"drl\", \"lgl\")\nplots &lt;- list()\n\nfor (i in 1:length(layout_options)) {\n  plots[[i]] &lt;- ggraph(GAStech_graph, layout = layout_options[i]) +\n    geom_edge_link(alpha = 0.1) +\n    geom_node_point(aes(color = Department), size = 3) +\n    theme_graph() +\n    labs(title = paste(\"Layout:\", layout_options[i]))\n}\n\ngridExtra::grid.arrange(grobs = plots, ncol = 2)\n\n\n\n\n\n\n\n\nThis comparison helps select the most appropriate layout for revealing specific network patterns. Each algorithm has strengths: - FR (Fruchterman-Reingold): Good for revealing clusters - KK (Kamada-Kawai): Emphasizes overall structure - DRL: Handles large networks efficiently - LGL: Good for hierarchical structures\n\n\n5.6 Fruchterman and Reingold layout\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThe “fr” layout (Fruchterman-Reingold) uses a force-directed algorithm that positions nodes as if they repel each other while edges act as springs pulling connected nodes together.\n\n\n5.7 Modifying network nodes\nColoring nodes by department reveals organizational structure:\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization immediately reveals department-based clustering in communication patterns.\n\n\n5.8 Modifying edges\nMapping edge thickness to the Weight variable (number of emails exchanged):\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization now shows both organizational structure through node color and communication intensity through edge thickness.\n\n\n5.9 Node Degree Visualization\nNode degree (number of connections) is a fundamental centrality measure that identifies highly connected individuals:\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_degree(mode = \"total\"))) +\n  scale_size_continuous(range = c(2, 8), name = \"Degree\") +\n  labs(title = \"Network with Node Size Based on Total Degree\") +\n  theme_graph()\n\ng\n\n\n\n\n\n\n\n\nThis visualization immediately identifies communication hubs within the organization, regardless of department affiliation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#creating-facet-graphs",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "6. Creating facet graphs",
    "text": "6. Creating facet graphs\nFaceting is a powerful technique to split visualization into subplots based on categorical variables. For networks, this helps reduce edge over-plotting and reveals patterns across different categories.\nThree faceting functions in ggraph: - facet_nodes() - draws edges only when both connected nodes appear in the panel - facet_edges() - always draws all nodes in all panels - facet_graph() - facets on two variables simultaneously\n\n6.1 Working with facet_edges()\nFaceting by the day of the week:\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\nThis visualization shows how communication patterns evolve throughout the week.\n\n\n6.2 Working with facet_edges() and theme adjustments\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n6.3 A framed facet graph\nAdding frames around each facet improves visual separation:\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n6.4 Department Communication Patterns\nAnalyzing which departments communicate most frequently with each other reveals organizational workflow:\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  left_join(\n    GAStech_graph %&gt;% \n      activate(nodes) %&gt;% \n      as_tibble() %&gt;% \n      select(id, Department),\n    by = c(\"from\" = \"id\")\n  ) %&gt;%\n  rename(from_dept = Department) %&gt;%\n  left_join(\n    GAStech_graph %&gt;% \n      activate(nodes) %&gt;% \n      as_tibble() %&gt;% \n      select(id, Department),\n    by = c(\"to\" = \"id\")\n  ) %&gt;%\n  rename(to_dept = Department) %&gt;%\n  group_by(from_dept, to_dept) %&gt;%\n  summarise(total_weight = sum(Weight)) %&gt;%\n  arrange(desc(total_weight)) %&gt;%\n  ungroup() %&gt;%\n  slice_head(n = 10) %&gt;%\n  ggplot(aes(x = reorder(paste(from_dept, \"→\", to_dept), total_weight), \n             y = total_weight)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Inter-Department Communication Flows\",\n       x = \"\",\n       y = \"Total Email Volume\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis analysis reveals which departments are most tightly coupled in their work processes.\n\n\n6.5 Working with facet_nodes()\nFaceting by department shows communication patterns within each organizational unit:\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#network-metrics-analysis",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "7. Network Metrics Analysis",
    "text": "7. Network Metrics Analysis\nNetwork metrics provide quantitative measurements of structural properties and help identify important nodes, clusters, and overall network characteristics.\n\n7.1 Computing centrality indices\nCentrality measures identify influential nodes in the network. Betweenness centrality measures how often a node lies on shortest paths between other nodes, identifying potential information brokers:\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization reveals individuals who control information flow between different parts of the network.\n\n\n7.2 Visualising network metrics\nFrom ggraph v2.0 onward, centrality measures can be computed directly within the plotting call:\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n7.3 Visualising Community\nCommunity detection algorithms identify clusters of densely connected nodes. tidygraph provides many algorithms for this purpose:\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThis visualization reveals natural clusters in the communication network that may not align perfectly with formal organizational structure.\n\n\n7.4 Community vs Department Alignment\nComparing algorithmic communities with formal departments reveals organizational structure effectiveness:\n\ncommunity_dept &lt;- GAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  as_tibble() %&gt;%\n  count(Department, community) %&gt;%\n  group_by(Department) %&gt;%\n  mutate(prop = n/sum(n))\n\nggplot(community_dept, aes(x = Department, y = community, size = prop, color = prop)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(title = \"Department vs Detected Community Alignment\",\n       size = \"Proportion\",\n       color = \"Proportion\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nPerfect alignment would show a one-to-one mapping; dispersed departments across multiple communities suggest informal communication structures that cross formal boundaries."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "8. Building Interactive Network Graph with visNetwork",
    "text": "8. Building Interactive Network Graph with visNetwork\nThe visNetwork package creates interactive network visualizations using the vis.js JavaScript library, enabling exploration through interaction.\n\n8.1 Data preparation\nPreparing data for visNetwork requires specific formatting:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n8.2 Plotting the first interactive network graph\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n8.3 Working with layout\nUsing the Fruchterman and Reingold layout for the interactive visualization:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n8.4 Working with visual attributes - Nodes\nvisNetwork uses a “group” field for node coloring. Renaming the Department field to match this convention:\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nAdding a legend to the visualization:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n8.5 Working with visual attributes - Edges\nCustomizing edge appearance with arrows and smooth curves:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n8.6 Interactivity\nAdding interactive features to enhance exploration:\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nThe highlightNearest option highlights connected nodes when clicking on a node, while nodesIdSelection adds a dropdown menu for selecting specific nodes."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex09.html#reference",
    "title": "Hands-on Exercise 9: Modelling, Visualising and Analysing Network Data with R",
    "section": "9. Reference",
    "text": "9. Reference\nCredits to Prof Kam."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(haven, SmartEDA, tidyverse, tidymodels, ggridges)\n\n\nexam_data &lt;- read_csv(\"./data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(haven, SmartEDA, tidyverse, tidymodels, ggridges)\n\n\nexam_data &lt;- read_csv(\"./data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#box-plot",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#box-plot",
    "title": "In-class Exercise 4",
    "section": "Box plot",
    "text": "Box plot\n\nggplot(data = exam_data,\n       aes(x = ENGLISH,\n           y = CLASS)) + \n  geom_boxplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#ridgeline-and-boxplot",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#ridgeline-and-boxplot",
    "title": "In-class Exercise 4",
    "section": "Ridgeline and Boxplot",
    "text": "Ridgeline and Boxplot\nBoxplots provide essential summary statistics—medians, quartiles, and whiskers—but they are incomplete, they don’t reveal the full distribution of the data.\nIn contrast, ridgeline plots illustrate the underlying density, highlighting features like multiple peaks (as seen in classes 3G and 3H) and outliers (evident in class 3F).\nCombining both provides a more complete view by showing both the summary metrics and the detailed distribution.\n\n# Create the combined plot\nggplot(data = exam_data, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 1.1,        # Adjusts the vertical scaling of the ridgelines\n    alpha = 0.5,    \n    fill = \"lightblue\"\n  ) +\n  geom_boxplot(\n    width = 0.15,\n    position = position_nudge(y = -0.2),  \n    outlier.colour = \"red\",              \n    alpha = 0.7                       \n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Combined Plot of English Scores by Class\",\n    x = \"English Score\",\n    y = \"Class\"\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "",
    "text": "We begin by loading all necessary libraries.\n\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "",
    "text": "We begin by loading all necessary libraries.\n\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-loading-and-preparation",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-loading-and-preparation",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "2. Data Loading and Preparation",
    "text": "2. Data Loading and Preparation\nWe load the dataset from an Excel file and convert selected columns to factors for further analysis.\nConverting columns to factors helps in treating categorical variables appropriately during analysis.\n\n# Load the Toyota Corolla dataset from the 'data' sheet\ncar_resale &lt;- read_xls(\"./data/ToyotaCorolla.xls\", \"data\")\n\n# Specify columns to be converted to factors\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\",\n          \"Cylinders\", \"Fuel_Type\", \"Color\", \"Met_Color\", \"Automatic\",\n          \"Mfr_Guarantee\", \"BOVAG_Guarantee\", \"ABS\", \"Airbag_1\",\n          \"Airbag_2\", \"Airco\", \"Automatic_airco\", \"Boardcomputer\",\n          \"CD_Player\", \"Central_Lock\", \"Powered_Windows\",\n          \"Power_Steering\", \"Radio\", \"Mistlamps\", \"Sport_Model\",\n          \"Backseat_Divider\", \"Metallic_Rim\", \"Radio_cassette\",\n          \"Tow_Bar\")\n\n# Convert the 'Id' column to character and specified columns to factors\ncar_resale &lt;- car_resale %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_at(vars(one_of(cols)), as.factor)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-overview",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-overview",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "3. Data Overview",
    "text": "3. Data Overview\n\n3.1 Structural Overview\nWe can inspect the overall structure of the dataset using a summary and glimpse.\n\nSummaryGlimpse\n\n\n\n# Display a summary of the dataset\nsummary(car_resale)\n\n      Id               Model               Price         Age_08_04    \n Length:1436        Length:1436        Min.   : 4350   Min.   : 1.00  \n Class :character   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Mode  :character   Mode  :character   Median : 9900   Median :61.00  \n                                       Mean   :10731   Mean   :55.95  \n                                       3rd Qu.:11950   3rd Qu.:70.00  \n                                       Max.   :32500   Max.   :80.00  \n                                                                      \n   Mfg_Month      Mfg_Year          KM         Quarterly_Tax        Weight    \n 1      :207   Min.   :1998   Min.   :     1   Min.   : 19.00   Min.   :1000  \n 4      :154   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00   1st Qu.:1040  \n 3      :138   Median :1999   Median : 63390   Median : 85.00   Median :1070  \n 2      :134   Mean   :2000   Mean   : 68533   Mean   : 87.12   Mean   :1072  \n 7      :133   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00   3rd Qu.:1085  \n 6      :120   Max.   :2004   Max.   :243000   Max.   :283.00   Max.   :1615  \n (Other):550                                                                  \n Guarantee_Period     HP_Bin      CC_bin    Doors   Gears    Cylinders\n Min.   : 3.000   &lt; 100  :560   &lt;1600:416   2:  2   3:   2   4:1436   \n 1st Qu.: 3.000   &gt; 120  : 11   &gt;1600:166   3:622   4:   1            \n Median : 3.000   100-120:865   1600 :854   4:138   5:1390            \n Mean   : 3.815                             5:674   6:  43            \n 3rd Qu.: 3.000                                                       \n Max.   :36.000                                                       \n                                                                      \n  Fuel_Type        Color     Met_Color Automatic Mfr_Guarantee BOVAG_Guarantee\n CNG   :  17   Grey   :301   0:467     0:1356    0:848         0: 150         \n Diesel: 155   Blue   :283   1:969     1:  80    1:588         1:1286         \n Petrol:1264   Red    :278                                                    \n               Green  :220                                                    \n               Black  :191                                                    \n               Silver :122                                                    \n               (Other): 41                                                    \n ABS      Airbag_1 Airbag_2 Airco   Automatic_airco Boardcomputer CD_Player\n 0: 268   0:  42   0: 398   0:706   0:1355          0:1013        0:1122   \n 1:1168   1:1394   1:1038   1:730   1:  81          1: 423        1: 314   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Central_Lock Powered_Windows Power_Steering Radio    Mistlamps Sport_Model\n 0:603        0:629           0:  32         0:1226   0:1067    0:1005     \n 1:833        1:807           1:1404         1: 210   1: 369    1: 431     \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n Backseat_Divider Metallic_Rim Radio_cassette Tow_Bar \n 0: 330           0:1142       0:1227         0:1037  \n 1:1106           1: 294       1: 209         1: 399  \n                                                      \n                                                      \n                                                      \n                                                      \n                                                      \n\n\n\n\n\n# Show a concise structure overview\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;chr&gt; \"81\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"44\", \"…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;fct&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;fct&gt; 100-120, &lt; 100, &lt; 100, &lt; 100, &lt; 100, &lt; 100, &lt; 100, &lt; …\n$ CC_bin           &lt;fct&gt; 1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600, &gt;1600…\n$ Doors            &lt;fct&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;fct&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;fct&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;fct&gt; Petrol, Diesel, Diesel, Diesel, Diesel, Diesel, Diese…\n$ Color            &lt;fct&gt; Blue, Blue, Silver, Blue, Black, Black, White, Grey, …\n$ Met_Color        &lt;fct&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;fct&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;fct&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;fct&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;fct&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;fct&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;fct&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;fct&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;fct&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\n\n3.2 List records\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n   Id    Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1 81    TOYOTA … 18950        25 8             2002  20019           100   1180\n 2 1     TOYOTA … 13500        23 10            2002  46986           210   1165\n 3 2     TOYOTA … 13750        23 10            2002  72937           210   1165\n 4 3      TOYOTA… 13950        24 9             2002  41711           210   1165\n 5 4     TOYOTA … 14950        26 7             2002  48000           210   1165\n 6 5     TOYOTA … 13750        30 3             2002  38500           210   1170\n 7 6     TOYOTA … 12950        32 1             2002  61000           210   1170\n 8 7      TOYOTA… 16900        27 6             2002  94612           210   1245\n 9 8     TOYOTA … 18600        30 3             2002  75889           210   1245\n10 44    TOYOTA … 16950        27 6             2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;fct&gt;, CC_bin &lt;fct&gt;,\n#   Doors &lt;fct&gt;, Gears &lt;fct&gt;, Cylinders &lt;fct&gt;, Fuel_Type &lt;fct&gt;, Color &lt;fct&gt;,\n#   Met_Color &lt;fct&gt;, Automatic &lt;fct&gt;, Mfr_Guarantee &lt;fct&gt;,\n#   BOVAG_Guarantee &lt;fct&gt;, ABS &lt;fct&gt;, Airbag_1 &lt;fct&gt;, Airbag_2 &lt;fct&gt;,\n#   Airco &lt;fct&gt;, Automatic_airco &lt;fct&gt;, Boardcomputer &lt;fct&gt;, CD_Player &lt;fct&gt;,\n#   Central_Lock &lt;fct&gt;, Powered_Windows &lt;fct&gt;, Power_Steering &lt;fct&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-summaries",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#data-summaries",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "4. Data Summaries",
    "text": "4. Data Summaries\n\nType 1Type 2\n\n\nType 1 provides an overall data summary.\n\nsummary1 &lt;- car_resale %&gt;%\n  ExpData(type = 1)\n\n# Display the summary (further customization possible)\nsummary1\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables         7\n4                              No. of factor variables        29\n5                                No. of text variables         2\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\nType 2 provides a variable level summary.\n\nsummary2 &lt;- car_resale %&gt;%\n  ExpData(type = 2)\n\n# Display the detailed summary\nsummary2\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id     character     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month        factor     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin        factor     1436             0              0\n12    12           CC_bin        factor     1436             0              0\n13    13            Doors        factor     1436             0              0\n14    14            Gears        factor     1436             0              0\n15    15        Cylinders        factor     1436             0              0\n16    16        Fuel_Type        factor     1436             0              0\n17    17            Color        factor     1436             0              0\n18    18        Met_Color        factor     1436             0              0\n19    19        Automatic        factor     1436             0              0\n20    20    Mfr_Guarantee        factor     1436             0              0\n21    21  BOVAG_Guarantee        factor     1436             0              0\n22    22              ABS        factor     1436             0              0\n23    23         Airbag_1        factor     1436             0              0\n24    24         Airbag_2        factor     1436             0              0\n25    25            Airco        factor     1436             0              0\n26    26  Automatic_airco        factor     1436             0              0\n27    27    Boardcomputer        factor     1436             0              0\n28    28        CD_Player        factor     1436             0              0\n29    29     Central_Lock        factor     1436             0              0\n30    30  Powered_Windows        factor     1436             0              0\n31    31   Power_Steering        factor     1436             0              0\n32    32            Radio        factor     1436             0              0\n33    33        Mistlamps        factor     1436             0              0\n34    34      Sport_Model        factor     1436             0              0\n35    35 Backseat_Divider        factor     1436             0              0\n36    36     Metallic_Rim        factor     1436             0              0\n37    37   Radio_cassette        factor     1436             0              0\n38    38          Tow_Bar        factor     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualizations",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualizations",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "5. Visualizations",
    "text": "5. Visualizations\n\n5.1 Numerical Data Visualizations\nWe create visualizations for numerical variables.\nTwo versions are provided:\n\nWithout specifying a target variable.\nFocusing on the “Price” variable.\n\n\nWithout Target Variable\n\ncar_resale %&gt;%\n  ExpNumViz(target = NULL,\n            nlim = 10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Price Distribution\n\ncar_resale %&gt;%\n  ExpNumViz(target = \"Price\",\n            nlim = 10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 Categorical Data Visualizations\nThe following plot displays bar charts for categorical variables to visualize the distribution of categories.\n\ncar_resale %&gt;%\n  ExpCatViz(target = NULL,\n            col = \"sky blue\",\n            clim = 10,\n            margin = 2,\n            Page = c(4,4),\n            sample = 16)\n\n$`0`"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#checking-multicollinearity",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#checking-multicollinearity",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "6. Checking Multicollinearity",
    "text": "6. Checking Multicollinearity\nIn this section, we assess multicollinearity using two approaches: the correlation matrix and the Variance Inflation Factor (VIF). We start by fitting a regression model, then examine multicollinearity diagnostics using the check_collinearity() function, which provides both correlation and VIF information.\n\n# Fit an initial model including potential multicollinear predictors\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, data = car_resale)\n\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n# Plot the multicollinearity diagnostics for a visual overview\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\nSince the variable “Mfg_Year” shows multicollinearity issues, we remove it and fit an updated model.\n\n# Fit an updated model excluding manufacturing year\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + Weight + Guarantee_Period, data = car_resale)\n\n\n# Check the normality of residuals and heteroscedasticity for the updated model\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\ncheck_heteroscedasticity(model1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nWe can also generate a comprehensive set of diagnostic plots for our fitted model using the check_model() function.\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\nApproaches to Assess Multicollinearity:\n\nCorrelation Matrix: Examines pairwise correlations between predictors to identify highly correlated variables.\nVariance Inflation Factor (VIF): Quantifies how much the variance of a regression coefficient is inflated due to multicollinearity.\n\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#model-summary",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#model-summary",
    "title": "In-class Exercise 5: Toyota Corolla Data Analysis",
    "section": "7. Model Summary",
    "text": "7. Model Summary\n\n7.1 gtsummary\nFinally, we can summarize the updated model using gtsummary.\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n7.2 tbl_regression\nWe can also generate regression tables with tbl_regression\n\nBasic regression table\n\ntbl_regression(model1, \n               intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-2,186\n-4,093, -278\n0.025\n\n\nAge_08_04\n-119\n-125, -114\n&lt;0.001\n\n\nKM\n-0.02\n-0.03, -0.02\n&lt;0.001\n\n\nWeight\n20\n18, 21\n&lt;0.001\n\n\nGuarantee_Period\n27\n2.1, 52\n0.034\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nRegression table with additional statistics\n\ntbl_regression(model1, \n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    # \"\\U03C3\" to extract the sigma value\n    label = list(sigma ~ \"\\U03C3\"),  # can ignore if you do not want the sigma\n    include = c(r.squared, adj.r.squared,\n                AIC, statistic,\n                p.value, sigma)\n  )\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\nThe packages provide the following functionality: - tidyverse: Collection of packages for data manipulation and visualization - tsibble: Provides a data infrastructure for tidy temporal data - feasts: Feature Extraction And Statistics for Time Series analysis - fable: Forecasting models including exponential smoothing and ARIMA - seasonal: Seasonal decomposition of time series\n\n\n\n\nts_data &lt;- read_csv(\"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(ts_data$`Month-Year`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used.\n\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\nThe packages provide the following functionality: - tidyverse: Collection of packages for data manipulation and visualization - tsibble: Provides a data infrastructure for tidy temporal data - feasts: Feature Extraction And Statistics for Time Series analysis - fable: Forecasting models including exponential smoothing and ARIMA - seasonal: Seasonal decomposition of time series\n\n\n\n\nts_data &lt;- read_csv(\"data/visitor_arrivals_by_air.csv\")\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\nts_data$`Month-Year` &lt;- dmy(ts_data$`Month-Year`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-data-formats-for-time-series-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-data-formats-for-time-series-analysis",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "2. Converting Data Formats for Time Series Analysis",
    "text": "2. Converting Data Formats for Time Series Analysis\n\n2.1 Base ts object versus tibble object\n\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\n\n\nBase ts objects differ from tibble dataframes in several ways. A ts object is specifically designed for time series analysis and has class types “mts”, “ts”, “matrix”, and “array”, whereas a tibble dataframe has class types “spec_tbl_df”, “tbl_df”, “tbl”, and “data.frame”.\n\n\n2.2 Converting to tsibble format\nThe tsibble format allows us to work with time series data in a tidy framework, making it compatible with both dplyr/tidyr functions and time series analysis functions.\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = Month)\n\nThis creates a tbl_ts object which can be used in both dplyr/tidyr operations and time series analysis."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-transformation-for-visualization",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-transformation-for-visualization",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "3. Data Transformation for Visualization",
    "text": "3. Data Transformation for Visualization\nTo visualize time series data effectively, we need to transform the data from wide to long format.\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualizing-time-series-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualizing-time-series-data",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "4. Visualizing Time Series Data",
    "text": "4. Visualizing Time Series Data\n\n4.1 Single Time Series Visualization\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\n\n4.2 Multiple Time Series Visualization\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n4.3 Using Facets for Better Comparison\nUsing facet_wrap allows us to compare multiple time series more effectively by giving each country its own panel.\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt’s important to note that intervals are not constant across these visualizations."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-tsibble-for-further-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-tsibble-for-further-analysis",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "5. Converting tsibble for Further Analysis",
    "text": "5. Converting tsibble for Further Analysis\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#analyzing-time-series-patterns",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#analyzing-time-series-patterns",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "6. Analyzing Time Series Patterns",
    "text": "6. Analyzing Time Series Patterns\n\n6.1 Comparing Country-Specific Patterns\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\nThis visualization shows distinct patterns for different countries: - Italy shows a strong seasonal pattern with a large spike in August and relatively consistent lower values during the rest of the year - Vietnam shows higher arrivals in June and July with a peak in July, relatively stable numbers from September to December, and a gradual increase from January to May\n\n\n6.2 Seasonal Subseries Analysis\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)\n\n\n\n\n\n\n\n\nThe subseries plot helps visualize seasonal patterns by month across years, making it easier to identify consistent seasonal behaviors."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#autocorrelation-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#autocorrelation-analysis",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "7. Autocorrelation Analysis",
    "text": "7. Autocorrelation Analysis\n\n7.1 ACF Plots\nAutocorrelation Function (ACF) plots show how time series data is correlated with its lagged values.\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\" |\n         Country == \"United Kingdom\" |\n         Country == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nThe ACF plots reveal important patterns: - China shows a 6-month periodicity - Italy shows a 12-month peak with weak correlation at first month (~0.3) - For statistically significant correlation, values should exceed the blue line (95% confidence level) - For China and Vietnam, most/all lags show statistical significance - For China and Vietnam, correlation decreases then increases again, but with different periodicity (Vietnam: 12 months, China: 6 months) - UK shows significant lag at t-1, then non-significant values, then significance again at 12 months, indicating weak trend but strong annual seasonality - Both UK and Italy show less pronounced seasonal patterns compared to Vietnam and China\n\n\n7.2 PACF Plots\nPartial Autocorrelation Function (PACF) plots show the direct correlation between observations at different lags after removing the effects of intermediate lags.\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\" |\n         Country == \"United Kingdom\" |\n         Country == \"China\") %&gt;%\n  PACF(Arrivals) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nPACF plots help identify what might happen with further decomposition: - For the UK, the first lag is positive while the second lag is negative, suggesting a turning point where arrival patterns change - Statistical significance is indicated when values extend beyond the blue confidence interval - These visualizations help identify both statistically significant patterns and potential turning points in the data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#time-series-decomposition",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#time-series-decomposition",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "8. Time Series Decomposition",
    "text": "8. Time Series Decomposition\nDecomposing time series helps separate the data into trend, seasonal, and remainder components.\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nThe decomposition shows: - The trend component shows the long-term movement - The seasonal component shows regular patterns - The remainder shows what’s left after removing trend and seasonality\nIf the remainder shows no clear pattern, it’s considered “white noise” - indicating a good decomposition. When patterns remain in the remainder, it suggests the decomposition is incomplete, which may indicate that more advanced methods like machine learning might be needed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#forecasting",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#forecasting",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "9. Forecasting",
    "text": "9. Forecasting\n\n9.1 Creating Training and Hold-out Sets\nFor time series forecasting, we can’t use random sampling for validation. Instead, we keep the most recent data as a hold-out set.\n\nvietnam_ts &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;% \n  mutate(Type = if_else(\n    `Month-Year` &gt;= \"2019-01-01\", \n    \"Hold-out\", \"Training\"))\n\n\nvietnam_train &lt;- vietnam_ts %&gt;%\n  filter(`Month-Year` &lt; \"2019-01-01\")\n\n\n\n9.2 Decomposing the Training Data\nIt’s important to analyze training data before forecasting to understand its components.\n\nvietnam_train %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n9.3 Fitting Forecast Models\nWhen fitting a good forecast model, the residuals should follow a normal distribution, indicating that the model has captured the systematic patterns in the data.\nHere we create an automatic ETS (Error, Trend, Seasonal) model:\n\nfit_autoETS &lt;- vietnam_train %&gt;%\n  model(ETS(Arrivals))\n\n\n\n9.4 Visualizing the Forecast Results\n\nfc_autoETS &lt;- fit_autoETS %&gt;%\n  forecast(h = \"12 months\")\n\nvietnam_ts %&gt;%\n  ggplot(aes(x = Month, \n             y = Arrivals)) +\n  autolayer(fc_autoETS, \n            alpha = 0.6) +\n  geom_line(aes(\n    color = Type), \n    alpha = 0.8) + \n  geom_line(aes(\n    y = .mean, \n    colour = \"Forecast\"), \n    data = fc_autoETS) +\n  geom_line(aes(\n    y = .fitted, \n    colour = \"Fitted\"), \n    data = augment(fit_autoETS))\n\n\n\n\n\n\n\n\nWhen visualizing forecast results, it’s most useful to focus on the last few cycles rather than the entire history, as we want to assess the accuracy of predictions compared to the hold-out data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#additional-insights",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#additional-insights",
    "title": "In-class Exercise 7: Visualising, Analysing and Forecasting Time-series Data: tidyverts methods",
    "section": "10. Additional Insights",
    "text": "10. Additional Insights\n\n10.1 Comparing Seasonal Patterns Across Countries\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\" |\n         Country == \"Malaysia\" |\n         Country == \"Germany\") %&gt;% \n  gg_season(Arrivals) +\n  labs(title = \"Seasonal Patterns by Country\",\n       y = \"Visitor Arrivals\")\n\n\n\n\n\n\n\n\nThis visualization shows how visitor arrivals vary by month across different countries, highlighting distinct seasonal tourism patterns.\n\n\n10.2 Exploring Trend vs Seasonality Strength\n\ncountry_features &lt;- tsibble_longer %&gt;%\n  features(Arrivals, feat_stl)\n\nggplot(country_features, aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point() +\n  geom_text(aes(label = Country), check_overlap = TRUE, hjust = -0.1, vjust = -0.1) +\n  labs(x = \"Trend Strength\", y = \"Seasonal Strength\", \n       title = \"Trend vs Seasonal Strength by Country\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis scatter plot helps identify which countries have strong seasonal patterns versus consistent growth trends, providing insight for targeted tourism strategies.\n\n\n10.3 ARIMA Model Fitting and Comparison\n\nfit_arima &lt;- vietnam_train %&gt;%\n  model(\n    arima_auto = ARIMA(Arrivals),\n    ets_auto = ETS(Arrivals)\n  )\n\nfit_arima %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(vietnam_ts, level = NULL) +\n  labs(title = \"ARIMA vs ETS Model Comparison\",\n       y = \"Visitor Arrivals\")\n\n\n\n\n\n\n\n\nComparing different forecasting methods helps identify which approach works best for specific time series patterns.\n\n\n10.4 Detecting Anomalies in Visitor Arrivals\n\naugment(fit_autoETS) %&gt;%\n  mutate(\n    anomaly = abs(.resid) &gt; 2*sd(.resid, na.rm = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = Month, y = Arrivals)) +\n  geom_line() +\n  geom_point(aes(color = anomaly), size = 1) +\n  scale_color_manual(values = c(\"FALSE\" = NA, \"TRUE\" = \"red\")) +\n  labs(title = \"Anomaly Detection in Vietnam Visitor Arrivals\",\n       color = \"Anomaly\")\n\n\n\n\n\n\n\n\nThis visualization helps identify unusual spikes or drops in visitor arrivals that might warrant further investigation.\n\n\n10.5 Visualizing Forecast Uncertainty\n\nfc_autoETS %&gt;%\n  autoplot(vietnam_ts) +\n  labs(title = \"Forecast with Prediction Intervals\",\n       y = \"Visitor Arrivals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnderstanding forecast uncertainty is crucial for planning, as it shows the range of likely outcomes rather than just point estimates."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608",
    "section": "",
    "text": "Welcome to Sindy’s ISSS608 Visual Analytics and Applications."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html",
    "href": "Take-home_Ex/Take-home_Ex2.html",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "",
    "text": "This assignment requires the application of visual analytics techniques to conduct a systematic exploration and analysis of Singapore’s international trade patterns and trends since 2015."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#overview",
    "href": "Take-home_Ex/Take-home_Ex2.html#overview",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "",
    "text": "This assignment requires the application of visual analytics techniques to conduct a systematic exploration and analysis of Singapore’s international trade patterns and trends since 2015."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex2.html#the-task",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "The task",
    "text": "The task\nThis take-home exercise comprises the following requirements:\n\nObtain the “Merchandise Trade by Region/Market” dataset from the Department of Statistics Singapore, DOS website via the Merchandise Trade by Region/Market page.\nConduct a critical evaluation of three data visualizations presented on this page by assessing their respective strengths and limitations\nImplement enhanced versions of the selected visualizations with R packages.\nPerform time-series forecasting methodologies on the trade data to support analytical findings."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex2.html#getting-started",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nFirst, we load packages required:\n\npacman::p_load(tidyverse, readxl, dplyr, magick, gganimate, gifski, scales, ggrepel, lubridate, ggplot2, cowplot, patchwork)\n\n\n\nImport data\nNext, we import the “Merchandise Trade by Region/Market” dataset. The dataset has a hierarchical structure with continents and their respective countries, along with various trade metrics across multiple time periods.\nThe data preparation process handles the dataset’s structure by:\n\nSkipping metadata and header rows\nProcessing the hierarchical relationship between regions and countries\nCleaning country names by removing indentation\nCreating a standardized tabular structure with clear column naming\nCombining domestic exports and re-exports to calculate total exports\n\nWe created four key dataframes, each corresponding to a specific trade flow:\n\nimports: Contains all import data by country and region\ndomestic_exports: Contains Singapore’s direct exports of locally produced goods\nreexports: Contains goods that were imported and then exported with minimal processing\ntotal_exports: Combines domestic exports and re-exports\n\n\n\nCode\nfile_path &lt;- \"data/outputFile.xlsx\"\n\nclean_trade_data &lt;- function(file_path, sheet_name) {\n  raw_data &lt;- read_excel(file_path, sheet = sheet_name, skip = 10)\n  \n  # Find where footnotes begin\n  for(i in 1:nrow(raw_data)) {\n    first_col_value &lt;- raw_data[[1]][i]\n    if(is.na(first_col_value) || grepl(\"Footnotes:\", first_col_value) || first_col_value == \"\") {\n      break\n    }\n  }\n  \n  # Keep only the rows before footnotes\n  clean_data &lt;- raw_data[1:(i-1), ]\n  \n  current_region &lt;- NA\n  result_df &lt;- data.frame()\n  \n  for(j in 1:nrow(clean_data)) {\n    row_value &lt;- clean_data$`Data Series`[j]\n    \n    if(row_value %in% c(\"America\", \"Asia\", \"Europe\", \"Oceania\", \"Africa\")) {\n      current_region &lt;- row_value\n      next  # Skip the region row itself\n    }\n    \n    # Skip the \"Total All Markets\" row\n    if(row_value == \"Total All Markets\") {\n      next\n    }\n    \n    if(grepl(\"^ +\", row_value) || !is.na(current_region)) {\n      country_name &lt;- trimws(row_value)\n      \n      country_row &lt;- clean_data[j, ]\n      country_row$Region &lt;- current_region\n      country_row$`Data Series` &lt;- country_name\n      \n      result_df &lt;- rbind(result_df, country_row)\n    }\n  }\n  \n  names(result_df)[1] &lt;- \"Country\"\n  \n  result_df &lt;- result_df %&gt;%\n    select(Country, Region, everything())\n  \n  return(result_df)\n}\n\nimports &lt;- clean_trade_data(file_path, \"T1\")\ndomestic_exports &lt;- clean_trade_data(file_path, \"T2\")\nreexports &lt;- clean_trade_data(file_path, \"T3\")\n\n\n# Make sure both dataframes have the same countries and regions\nall_countries &lt;- unique(c(domestic_exports$Country, reexports$Country))\nall_regions &lt;- unique(c(domestic_exports$Region, reexports$Region))\n\nstandardize_df &lt;- function(df, all_countries, all_regions) {\n  template &lt;- expand.grid(\n    Country = all_countries,\n    Region = all_regions,\n    stringsAsFactors = FALSE\n  ) %&gt;%\n\n    semi_join(\n      bind_rows(\n        select(domestic_exports, Country, Region),\n        select(reexports, Country, Region)\n      ),\n      by = c(\"Country\", \"Region\")\n    )\n  \n  result &lt;- template %&gt;%\n    left_join(df, by = c(\"Country\", \"Region\"))\n  \n  result &lt;- result %&gt;%\n    mutate(across(where(is.numeric), ~ifelse(is.na(.), 0, .)))\n  \n  return(result)\n}\n\ndomestic_exports &lt;- standardize_df(domestic_exports, all_countries, all_regions)\nreexports &lt;- standardize_df(reexports, all_countries, all_regions)\n\ntotal_exports &lt;- domestic_exports %&gt;%\n  select(Country, Region) %&gt;%\n  bind_cols(\n    domestic_exports %&gt;% \n      select(where(is.numeric)) %&gt;%\n      rename_with(~paste0(., \"_domestic\"), everything()),\n    \n    reexports %&gt;% \n      select(where(is.numeric)) %&gt;%\n      # Rename columns to identify source\n      rename_with(~paste0(., \"_reexport\"), everything())\n  )\n\ndate_cols &lt;- unique(sub(\"_domestic$|_reexport$\", \"\", \n                        names(total_exports)[-(1:2)]))\n\n# For each date, add domestic and re-export values\nfor(date in date_cols) {\n  total_exports[[date]] &lt;- total_exports[[paste0(date, \"_domestic\")]] + \n                           total_exports[[paste0(date, \"_reexport\")]]\n}\n\n# Keep only the total columns plus Country and Region\ntotal_exports &lt;- total_exports %&gt;%\n  select(Country, Region, all_of(date_cols))\n\n## Check dimensions to verify. Should be the same\n# dim(domestic_exports)  # output: 154 267\n# dim(reexports)  # output: 154 267      \n# dim(total_exports)  # output: 154 267   \n\n\nWe can verify that imports is correctly configured by examining the initial rows of the dataframe.\n\nhead(imports)\n\n# A tibble: 6 × 267\n  Country          Region `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Antigua And Bar… Ameri…         0         0           0         0          0  \n2 Argentina        Ameri…         4        12.5       116.        4.1        8.1\n3 Bahamas          Ameri…         0         8.1         0         0          0  \n4 Bermuda          Ameri…         0         0           0         0          0  \n5 Brazil           Ameri…       870.      587.        942.      640.       787. \n6 Canada           Ameri…       268.      213.        222.      324.       236. \n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#visualisation-1-merchandise-trade-performance-with-major-trading-partners",
    "href": "Take-home_Ex/Take-home_Ex2.html#visualisation-1-merchandise-trade-performance-with-major-trading-partners",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Visualisation 1: MERCHANDISE TRADE PERFORMANCE WITH MAJOR TRADING PARTNERS",
    "text": "Visualisation 1: MERCHANDISE TRADE PERFORMANCE WITH MAJOR TRADING PARTNERS\n\nOriginal design\nThis visualization is intended to display Singapore’s trade relationships with its major trading partners, showing imports, exports, total trade volume, and trade balance simultaneously in a static bubble chart format. The chart uses a scatter plot framework with imports on the x-axis, exports on the y-axis, and bubble size representing total trade volume.\nThe original design is shown below.\n\n\n\nMERCHANDISE TRADE PERFORMANCE WITH MAJOR TRADING PARTNERS, 2024\n\n\n\n\nCritique\n\nClarity\nWhy it is clear:\n\nIntuitive coordinate mapping: The use of a diagonal reference line creates an immediate visual classifier for trade surplus vs deficit. This diagonal approach leverages our spatial reasoning abilities to quickly identify balanced trade positions.\nMulti-dimensional encoding: Successfully represents four data dimensions (imports, exports, total trade volume, and trade balance) in a single visualization, allowing viewers to grasp complex relationships at once.\nDirect labeling approach: Each bubble contains both qualitative (country name) and quantitative (exact trade value) information, eliminating the need to reference a separate legend for primary data points.\n\nWhy it can be confusing:\n\nAxis labeling position: The imports and exports labels positioned in the bottom corners could be potentially confusing or misleading, as readers may be uncertain which axis corresponds to which label. The axes would be more clearly defined if labeled directly on the sides.\nRegional vs country-level comparison: Presenting the EU as a single entity while showing individual countries creates an inconsistent unit of analysis. This approach obscures the distribution of countries within the EU and their individual trade positions relative to Singapore.\nStatic limitation: The single time-point snapshot fails to reveal trends or patterns in trading relationships that have evolved over time, limiting context for analysis.\n\n\n\nAesthetics\nWhy it is beautiful:\n\nVisual harmony: The consistent use of circular elements creates a visual rhythm across the chart, with the varying sizes creating a natural visual hierarchy that draws attention to major trading partners. -Bounded compositional structure: The diagonal line creates a clear compositional structure that organizes the visual space effectively, using the principle of visual balance across the chart.\n\nWhy it can be ugly:\n\nWhile the visualization is generally attractive, the arbitrary color scheme for countries misses an opportunity to add meaningful encoding (such as by geographic region), which would enhance both the aesthetic appeal and analytical value of the chart.\n\n\n\n\nAnimated bubble chart of Singapore’s trade with major partners\nAnimated bubble chart of Singapore’s trade with major partners This visualization shows an animated bubble chart displaying Singapore’s merchandise trade performance with its top 20 trading partners over time. The animation provides several key insights that a static visualization cannot:\n\nTemporal evolution: We can observe how trading relationships have evolved over the years, with some partners becoming more significant while others decline in importance.\nRegional patterns: By color-coding bubbles by region, we can easily identify which regions have become increasingly important to Singapore’s trade network.\nTrade balance dynamics: The animation reveals how the trade balance (surplus or deficit) with key partners has shifted over time, with bubbles moving across the diagonal line that represents balanced trade.\nComparative size changes: The changing size of bubbles represents the growth or decline in total trade volume with each partner, offering a visual representation of Singapore’s changing trade priorities.\nMarket concentration or diversification: We can observe whether Singapore’s trade has become more concentrated among fewer partners or more diversified across many partners over the years.\n\n\n\nData preparation\nWe’ll work with the existing dataframes (imports and total_exports) that were already created in the data preparation step.\n\n# Get all date columns (excluding Country and Region columns)\ndate_cols &lt;- colnames(imports)[-(1:2)]\n\ndate_objects &lt;- as.Date(paste0(\"01 \", date_cols), format = \"%d %Y %b\")\nif(any(is.na(date_objects))) {\n  date_objects &lt;- as.Date(paste0(\"01 \", date_cols), format = \"%d %b %Y\")\n}\n\n# Sort date columns chronologically\ndate_cols_sorted &lt;- date_cols[order(date_objects)]\n\nlatest_period &lt;- tail(date_cols_sorted, 1)\n\n# Now identify top 20 countries by total trade volume in the latest period\ntop_countries &lt;- imports %&gt;%\n  left_join(total_exports %&gt;% select(Country, Region, all_of(latest_period)), \n            by = c(\"Country\", \"Region\"), \n            suffix = c(\"_imports\", \"_exports\")) %&gt;%\n  mutate(total_trade = .data[[paste0(latest_period, \"_imports\")]] + \n                      .data[[paste0(latest_period, \"_exports\")]]) %&gt;%\n  arrange(desc(total_trade)) %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(Country, Region)\n\n# Print top countries for reference\ntop_countries\n\n# A tibble: 20 × 2\n   Country              Region \n   &lt;chr&gt;                &lt;chr&gt;  \n 1 Taiwan               Asia   \n 2 Malaysia             Asia   \n 3 China                Asia   \n 4 United States        America\n 5 Hong Kong            Asia   \n 6 Korea, Rep Of        Asia   \n 7 Indonesia            Asia   \n 8 Japan                Asia   \n 9 Viet Nam             Asia   \n10 Thailand             Asia   \n11 India                Asia   \n12 United Arab Emirates Asia   \n13 Australia            Oceania\n14 Philippines          Asia   \n15 Germany              Europe \n16 United Kingdom       Europe \n17 France               Europe \n18 Switzerland          Europe \n19 Netherlands          Europe \n20 Brazil               America\n\n# Create a long format dataset for trade data over time for the top 20 countries\ntrade_data_long &lt;- tibble()\n\nfor(date_col in date_cols_sorted) {\n  period_imports &lt;- imports %&gt;%\n    semi_join(top_countries, by = c(\"Country\", \"Region\")) %&gt;%\n    select(Country, Region, all_of(date_col)) %&gt;%\n    rename(Imports = all_of(date_col))\n  \n  period_exports &lt;- total_exports %&gt;%\n    semi_join(top_countries, by = c(\"Country\", \"Region\")) %&gt;%\n    select(Country, Region, all_of(date_col)) %&gt;%\n    rename(Exports = all_of(date_col))\n  \n  # Join imports and exports\n  period_data &lt;- period_imports %&gt;%\n    left_join(period_exports, by = c(\"Country\", \"Region\")) %&gt;%\n    mutate(\n      Period = date_col,\n      Date = as.Date(paste0(\"01 \", date_col), format = \"%d %Y %b\", tz = \"UTC\"),\n      Date = if(all(is.na(Date))) as.Date(paste0(\"01 \", date_col), format = \"%d %b %Y\", tz = \"UTC\") else Date,\n      TotalTrade = Imports + Exports,\n      TradeBalance = Exports - Imports,\n      NetExporter = Exports &gt; Imports,\n      Year = year(Date),\n      Month = month(Date)\n    )\n  \n  trade_data_long &lt;- bind_rows(trade_data_long, period_data)\n}\n\nif(any(is.na(trade_data_long$Date))) {\n  periods &lt;- unique(trade_data_long$Period)\n  period_dates &lt;- seq.Date(from = as.Date(\"2003-01-01\"), \n                           by = \"month\", \n                           length.out = length(periods))\n  period_map &lt;- tibble(\n    Period = periods,\n    MappedDate = period_dates\n  )\n  \n  trade_data_long &lt;- trade_data_long %&gt;%\n    left_join(period_map, by = \"Period\") %&gt;%\n    mutate(Date = coalesce(Date, MappedDate)) %&gt;%\n    select(-MappedDate)\n}\n\ntrade_data_long &lt;- trade_data_long %&gt;%\n  mutate(PeriodLabel = format(Date, \"%b %Y\"))\n\nglimpse(trade_data_long)\n\nRows: 5,300\nColumns: 12\n$ Country      &lt;chr&gt; \"Brazil\", \"United States\", \"China\", \"Hong Kong\", \"India\",…\n$ Region       &lt;chr&gt; \"America\", \"America\", \"Asia\", \"Asia\", \"Asia\", \"Asia\", \"As…\n$ Imports      &lt;dbl&gt; 77.6, 2129.9, 1539.1, 427.0, 172.8, 1255.1, 1902.4, 676.3…\n$ Exports      &lt;dbl&gt; 27.1, 2777.5, 1258.9, 2277.8, 435.0, 2356.8, 1426.5, 1012…\n$ Period       &lt;chr&gt; \"2003 Jan\", \"2003 Jan\", \"2003 Jan\", \"2003 Jan\", \"2003 Jan…\n$ Date         &lt;date&gt; 2003-01-01, 2003-01-01, 2003-01-01, 2003-01-01, 2003-01-…\n$ TotalTrade   &lt;dbl&gt; 104.7, 4907.4, 2798.0, 2704.8, 607.8, 3611.9, 3328.9, 168…\n$ TradeBalance &lt;dbl&gt; -50.5, 647.6, -280.2, 1850.8, 262.2, 1101.7, -475.9, 336.…\n$ NetExporter  &lt;lgl&gt; FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, …\n$ Year         &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 200…\n$ Month        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ PeriodLabel  &lt;chr&gt; \"Jan 2003\", \"Jan 2003\", \"Jan 2003\", \"Jan 2003\", \"Jan 2003…\n\n\n\n\nCreating the animated bubble chart\nNow, we will create an animated bubble chart showing Singapore’s trade patterns over time. The animation showcases:\n\nRegional color-coding to identify geographic trade patterns\nBubble size representing total trade volume\nPosition relative to the diagonal line showing trade balance (surplus/deficit)\nSpecial highlighting for top 5 trading partners\nClear annotations explaining the trade balance zones\n\n\n# Define color scheme for regions\nregion_colors &lt;- c(\n  \"Asia\" = \"#1E88E5\",       # Blue\n  \"America\" = \"#D81B60\",    # Red\n  \"Europe\" = \"#8E24AA\",     # Purple\n  \"Oceania\" = \"#43A047\",    # Green\n  \"Africa\" = \"#F57C00\"      # Orange\n)\n\ntop_partners &lt;- trade_data_long %&gt;%\n  filter(Period == latest_period) %&gt;%\n  arrange(desc(TotalTrade)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  pull(Country)\n\n# Add a new column to identify top partners\ntrade_data_long &lt;- trade_data_long %&gt;%\n  mutate(\n    TopPartner = if_else(Country %in% top_partners, Country, \"Other\"),\n    TopPartnerFactor = factor(TopPartner, levels = c(top_partners, \"Other\"))\n  )\n\npartner_colors &lt;- c(\n  setNames(RColorBrewer::brewer.pal(5, \"Set1\"), top_partners),\n  \"Other\" = \"gray70\"\n)\n\n# Create the base plot\np &lt;- ggplot(\n  trade_data_long, \n  aes(x = Imports, y = Exports, \n      size = TotalTrade, \n      color = Region,\n      fill = Region,\n      alpha = 0.7)\n) +\n\n  annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf,\n           fill = \"white\", alpha = 0.2) +\n  geom_polygon(\n    data = data.frame(\n      x = c(0, max(trade_data_long$Imports) * 1.2, 0),\n      y = c(0, max(trade_data_long$Exports) * 1.2, max(trade_data_long$Exports) * 1.2)\n    ),\n    aes(x = x, y = y),\n    fill = \"white\", \n    alpha = 0.2,\n    inherit.aes = FALSE\n  ) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray70\") +\n  geom_point(shape = 21, stroke = 0.5) +\n  geom_point(\n    data = function(x) filter(x, Country %in% top_partners),\n    aes(color = Country), \n    shape = 21, \n    stroke = 1.5,\n    alpha = 0.9\n  ) +\n  geom_text_repel(\n    aes(label = Country),\n    size = 3,\n    force = 3,\n    max.iter = 3000,\n    segment.color = \"gray50\",\n    segment.alpha = 0.6,\n    box.padding = 0.5,\n    point.padding = 0.3,\n    max.overlaps = 100,\n    color = \"black\"\n  ) +\n  scale_x_continuous(labels = label_number(suffix = \" Bil\"), \n                     expand = expansion(mult = c(0.1, 0.1))) +\n  scale_y_continuous(labels = label_number(suffix = \" Bil\"), \n                     expand = expansion(mult = c(0.1, 0.1))) +\n  scale_size_continuous(range = c(3, 20), guide = \"none\") + \n  scale_color_manual(values = c(region_colors, partner_colors)) +\n  scale_fill_manual(values = region_colors) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"vertical\",\n    legend.justification = \"left\",    \n    legend.box.just = \"left\",     \n    legend.margin = margin(t = 5, r = 5, b = 5, l = 5),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, face = \"italic\"),\n    axis.title = element_text(size = 11, face = \"bold\")\n  ) +\n  # Set labels\n  labs(\n    title = \"Singapore's Merchandise Trade Performance\",\n    subtitle = \"Period: {closest_state}\",\n    x = \"Imports (S$ Bil)\",\n    y = \"Exports (S$ Bil)\",\n    color = \"Region / Top Partners\",\n    fill = \"Region\") +\n\n  annotate(\"text\", x = max(trade_data_long$Imports) * 0.25, y = max(trade_data_long$Exports) * 0.85, \n           label = \"Singapore's exports exceed imports\", \n           size = 3, color = \"darkblue\", hjust = 0) +\n  annotate(\"text\", x = max(trade_data_long$Imports) * 0.75, y = max(trade_data_long$Exports) * 0.25, \n           label = \"Singapore's imports exceed exports\", \n           size = 3, color = \"darkgreen\", hjust = 1)\n\nanimated_chart &lt;- p + \n  transition_states(\n    states = PeriodLabel,\n    transition_length = 3,\n    state_length = 15\n  ) +\n  ease_aes('cubic-in-out') +\n  enter_fade() + \n  exit_fade()\n\n# Create the animation object\ntrade_animation &lt;- animate(\n  animated_chart, \n  nframes = 300,\n  fps = 2,      \n  width = 800, \n  height = 800,\n  renderer = gifski_renderer(loop = TRUE),\n  res = 100\n)\n\n# Save the animation to a file\nanim_save(\"trade_animation.gif\", trade_animation)\n\n\n\n\nSingapore Trade Animation"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#visualisation-2-part-1-of-major-trading-partners-for-trade-in-services-2023",
    "href": "Take-home_Ex/Take-home_Ex2.html#visualisation-2-part-1-of-major-trading-partners-for-trade-in-services-2023",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Visualisation 2: Part 1 of MAJOR TRADING PARTNERS FOR TRADE IN SERVICES, 2023",
    "text": "Visualisation 2: Part 1 of MAJOR TRADING PARTNERS FOR TRADE IN SERVICES, 2023\n\nOriginal design\nThis visualization shows Singapore’s exports and imports in 2019 and 2023 for each of its top trading partners. The bars on the left indicate exports for 2019 vs 2023, and the bars on the right indicate imports for 2019 vs 2023. Each row represents a major trading partner, with numeric labels giving the exact trade values.\n\n\n\nMAJOR TRADING PARTNERS FOR TRADE IN SERVICES, 2023\n\n\n\n\nCritique\n\nClarity\nWhy it is clear:\n\nGeographic context: The map format provides immediate geographic context, making it easy to visualize the global distribution of Singapore’s trading partners.\nRegional color-coding: Different continents are clearly distinguished by distinct color schemes, allowing viewers to quickly identify regional patterns in Singapore’s trade partnerships. Flag identification: The use of national flags creates immediate recognition of specific countries without requiring extensive labeling or legends.\nVisual hierarchy: The highlighted regions with dots draw attention to areas of significance in Singapore’s trade network.\n\nWhy it can be confusing:\n\nLimited quantitative information: The visualization lacks actual trade values, making it impossible to understand the relative importance of each partner or region.\nBinary representation: Countries are either included (with a flag) or excluded, with no indication of the varying degrees of trade importance.\nTemporal limitations: Like the first visualization, this is a static snapshot that fails to show trends or patterns over time.\n\n\n\nAesthetics\nWhy it is beautiful:\n\nClean stylized design: The dot-matrix representation creates a modern, stylized appearance that’s visually appealing and reduces the complexity of traditional maps.\nColor harmony: The color palette is visually pleasing and creates a cohesive look while maintaining clear distinction between regions.\nSimplified geography: By abstracting continents to dot patterns, the visualization removes unnecessary geographic details while maintaining recognizable shapes.\n\nWhy it can be ugly:\n\nInformation-to-ink ratio: The visualization uses significant space to convey relatively little quantitative information, prioritizing aesthetic appeal over analytical depth.\nLimited analytical value: Despite its visual appeal, it offers little insight into the actual trade relationships, volumes, or trends.\n\n\n\n\nHorizon graph: visualizing trade patterns over time by region\nThe horizon graph makeover addresses several limitations of the original map visualization by:\n\nTemporal dimension: Unlike the static map, horizon graphs capture trade patterns over time (2003-2025), revealing long-term trends and fluctuations in Singapore’s trade relationships.\nQuantitative comparison: The visualization shows deviations from baseline trade values, allowing for meaningful comparisons between periods of growth and decline.\nRegional patterns: By faceting by region (continent), the horizon graph maintains the geographic grouping from the original but adds quantitative depth.\nDual metrics: The visualization shows both imports and exports simultaneously, enabling comparison between these two critical trade flows.\nEfficient use of space: Horizon graphs are specifically designed to maximize the data-to-ink ratio, allowing for the display of complex time series data in a compact format.\n\n\nprocess_trade_data &lt;- function(df, trade_type) {\n  # Aggregate by continent\n  continent_data &lt;- df %&gt;%\n    group_by(Region) %&gt;%\n    summarize(across(where(is.numeric), sum, na.rm = TRUE)) %&gt;%\n    ungroup()\n  \n  long_data &lt;- continent_data %&gt;%\n    pivot_longer(\n      cols = -Region,\n      names_to = \"Date\",\n      values_to = \"Value\"\n    ) %&gt;%\n    mutate(\n      Date = case_when(\n        grepl(\"^\\\\d{4}\\\\s+\\\\w+$\", Date) ~ as.Date(paste0(\"01 \", Date), format = \"%d %Y %b\"),\n        TRUE ~ as.Date(paste0(\"01 \", Date), format = \"%d %b %Y\")\n      )\n    ) %&gt;%\n    filter(!is.na(Date)) %&gt;%\n    arrange(Region, Date) %&gt;%\n    mutate(TradeType = trade_type) %&gt;%\n    group_by(Region) %&gt;%\n    mutate(Baseline = mean(Value, na.rm = TRUE)) %&gt;%\n    ungroup()\n  \n  return(long_data)\n}\n\ncreate_steps_1_to_3_horizon &lt;- function(all_trade) {\n  max_dev_by_group &lt;- all_trade %&gt;%\n    group_by(Region, TradeType) %&gt;%\n    summarize(\n      MaxDev = max(abs(Value - Baseline), na.rm = TRUE),\n      .groups = \"drop\"\n    )\n  \n  layer_data &lt;- all_trade %&gt;%\n    mutate(Deviation = Value - Baseline) %&gt;%\n    left_join(max_dev_by_group, by = c(\"Region\", \"TradeType\")) %&gt;%\n    mutate(\n      Direction = ifelse(Deviation &gt;= 0, \"Above\", \"Below\")\n    )\n  \n  continent_order &lt;- c(\"Asia\", \"Europe\", \"America\", \"Oceania\", \"Africa\")\n  layer_data &lt;- layer_data %&gt;%\n    mutate(Region = factor(Region, levels = continent_order))\n  \n  create_section_plots &lt;- function(data_subset, trade_type) {\n    subset_data &lt;- data_subset %&gt;% filter(TradeType == trade_type)\n    \n    p1 &lt;- ggplot(subset_data, aes(x = Date, y = Deviation)) +\n      geom_line() +\n      geom_hline(yintercept = 0, linetype = \"dashed\") +\n      facet_grid(Region ~ ., scales = \"free_y\") +\n      theme_minimal() +\n      labs(title = paste(trade_type, \"- Step 1: Standard line graph centered around baseline\"))\n    \n    p2 &lt;- ggplot(subset_data, aes(x = Date, y = abs(Deviation), fill = Direction)) +\n      geom_area(alpha = 0.7) + # Added translucency\n      scale_fill_manual(values = c(\n        \"Above\" = \"#3498DB\", \"Below\" = \"#E67E22\"\n      )) +\n      facet_grid(Region ~ ., scales = \"free_y\") +\n      theme_minimal() +\n      labs(title = paste(trade_type, \"- Step 2: Color (blue positive, orange negative) and layering\"))\n    \n    p3 &lt;- ggplot(subset_data, aes(x = Date, y = abs(Deviation), fill = Direction)) +\n      geom_area(alpha = 0.7) + # Added translucency\n      scale_fill_manual(values = c(\n        \"Above\" = \"#3498DB\", \"Below\" = \"#E67E22\"\n      )) +\n      facet_grid(Region ~ ., scales = \"free_y\") +\n      theme_minimal() +\n      labs(title = paste(trade_type, \"- Step 3: Mirroring around the baseline (negatives folded up)\"))\n    \n    return(list(p1 = p1, p2 = p2, p3 = p3))\n  }\n  \n  # Generate plots for both trade types\n  exports_plots &lt;- create_section_plots(layer_data, \"Exports\")\n  imports_plots &lt;- create_section_plots(layer_data, \"Imports\")\n  \n  combined_steps &lt;- plot_grid(\n    exports_plots$p1, imports_plots$p1,\n    exports_plots$p2, imports_plots$p2,\n    exports_plots$p3, imports_plots$p3,\n    ncol = 2,\n    align = \"hv\"\n  )\n  \n  final_step3 &lt;- plot_grid(\n    exports_plots$p3 + labs(title = \"Exports\") + theme(legend.position = \"right\"),\n    imports_plots$p3 + labs(title = \"Imports\") + theme(legend.position = \"right\"),\n    ncol = 1,\n    align = \"v\"\n  )\n  \n  final_with_title &lt;- plot_grid(\n    ggdraw() + draw_label(\"Singapore's Trade Patterns by Continent (2003-2025)\", fontface = \"bold\", size = 16),\n    final_step3,\n    ggdraw() + draw_label(\"/n/nBaseline = Average monthly trade value for each continent over the entire period (2003-2025)\", size = 8),\n    ncol = 1,\n    rel_heights = c(0.1, 1, 0.1)\n  )\n\n  return(list(steps = combined_steps, final_step3 = final_with_title))\n}\n\nimports_processed &lt;- process_trade_data(imports, \"Imports\")\nexports_processed &lt;- process_trade_data(total_exports, \"Exports\")\nall_trade &lt;- bind_rows(imports_processed, exports_processed)\n\nhorizon_plots &lt;- create_steps_1_to_3_horizon(all_trade)\nhorizon_plots$final_step3"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#visualisation-3-part-2-of-major-trading-partners-for-trade-in-services-2023",
    "href": "Take-home_Ex/Take-home_Ex2.html#visualisation-3-part-2-of-major-trading-partners-for-trade-in-services-2023",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Visualisation 3: Part 2 of MAJOR TRADING PARTNERS FOR TRADE IN SERVICES, 2023",
    "text": "Visualisation 3: Part 2 of MAJOR TRADING PARTNERS FOR TRADE IN SERVICES, 2023\n\nOriginal design\nThis visualization shows Singapore’s exports and imports in 2019 and 2023 for each of its top trading partners. The bars on the left indicate exports for 2019 vs 2023, and the bars on the right indicate imports for 2019 vs 2023. Each row represents a major trading partner, with numeric labels giving the exact trade values.\n Intended message\n\nIdentify major trading partners – United States, EU‐27, Mainland China, and so on.\nCompare changes in trade – How much exports and imports have grown (or shrunk) for each partner since 2019.\nHighlight relative magnitudes – For instance, the United States shows a large jump from S$50.7 Bil to S$108 Bil in imports, while EU‐27’s imports grew more modestly from S$40.4 Bil to S$42.2 Bil.\n\n\n\nCritique\n\nClarity\nWhy it is clear:\n\nStraightforward grouping: Each trading partner is in its own row, with 2019 and 2023 clearly labeled under “Exports” (green bars) and “Imports” (blue bars).\nImmediate numeric readout: Viewers see the numbers for both 2019 and 2023 at a glance, without needing to reference additional legends.\n\nWhy it can be confusing:\n\nSeparate bars for exports vs. imports: Because the bars are on opposite sides of the figure, it can be challenging to compare total trade or observe which side grew faster.\nNot intuitive to compare and rank the relative difference of imports and exports between the countries.\n\n\n\nAesthetics\nWhy it is beautiful:\n\nOrganized layout: Partners are neatly stacked from top to bottom with icons and flags, giving a strong visual identity.\nColor‐coding for years: Different color intensities help distinguish 2019 from 2023, making the chart visually appealing and fairly easy to decode.\n\nWhy it can be ugly:\n\nLimited temporal perspective: We only see two time points and cannot discern any intermediate trends or fluctuations (for instance, whether trade dipped in 2020 and rebounded afterward).\n\n\n\n\nSlope charts: visualizing year-by-year trade evolution\nThe enhanced slope chart redesign addresses several limitations of the original bar chart by:\n\nContinuous timeline: Rather than showing just two years, the slope charts reveal the continuous evolution of trade from 2020 to 2025, capturing year-over-year changes.\nRevealing patterns: The slopes’ directions and steepness immediately reveal growth or decline patterns in trade relationships, making trends more apparent than in static bar comparisons.\nCountry-specific trajectories: By using color-coding and connecting lines, viewers can follow each country’s specific trade trajectory across multiple years.\nMultiple time windows: Breaking the visualization into year-pair panels (2020→2021, 2021→2022, etc.) allows for more detailed analysis of how trade relationships evolved during specific time periods. Parallel comparison: Stacking imports and exports charts vertically maintains the original’s separation while facilitating direct comparison of patterns between the two trade flows.\nValue annotations: Like the original, the enhanced version retains direct value labeling at each time point, ensuring precise reading of the data.\n\n\ntop_10_map &lt;- function(country, region) {\n  case_when(\n    country == \"United States\" ~ \"United States\",\n    region == \"Europe\" ~ \"EU-27\",\n    country %in% c(\"Mainland China\", \"China\") ~ \"Mainland China\",\n    country == \"Japan\" ~ \"Japan\",\n    country %in% c(\"Brunei\", \"Cambodia\", \"Indonesia\", \"Laos\", \"Malaysia\", \n                   \"Myanmar\", \"Philippines\", \"Thailand\", \"Viet Nam\") ~ \"ASEAN\",\n    country == \"Australia\" ~ \"Australia\",\n    country == \"Hong Kong\" ~ \"Hong Kong\",\n    country == \"United Kingdom\" ~ \"United Kingdom\",\n    country == \"Switzerland\" ~ \"Switzerland\",\n    country == \"India\" ~ \"India\",\n    TRUE ~ \"Others\"\n  )\n}\n\njan_cols &lt;- c(\"2020 Jan\",\"2021 Jan\",\"2022 Jan\",\"2023 Jan\",\"2024 Jan\",\"2025 Jan\")\n\nimports_top10 &lt;- imports %&gt;%\n  mutate(PartnerGroup = top_10_map(Country, Region)) %&gt;%\n  group_by(PartnerGroup) %&gt;%\n  summarize(across(all_of(jan_cols), sum, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  filter(PartnerGroup != \"Others\")\n\nexports_top10 &lt;- total_exports %&gt;%\n  mutate(PartnerGroup = top_10_map(Country, Region)) %&gt;%\n  group_by(PartnerGroup) %&gt;%\n  summarize(across(all_of(jan_cols), sum, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  filter(PartnerGroup != \"Others\")\n\n\ncreate_slopegraph &lt;- function(\n  data,\n  col_left, \n  col_right, \n  title_label,\n  plot_type = c(\"Imports\",\"Exports\"),  # new argument\n  y_limits = NULL,   \n  show_y_axis = TRUE,\n  show_left_labels = TRUE\n) {\n  plot_type &lt;- match.arg(plot_type)\n  \n  df_long &lt;- data %&gt;%\n    select(PartnerGroup, all_of(col_left), all_of(col_right)) %&gt;%\n    pivot_longer(\n      cols = c(col_left, col_right),\n      names_to = \"Date\",\n      values_to = \"Value\"\n    ) %&gt;%\n    mutate(Date = factor(Date, levels = c(col_left, col_right)))\n  \n  p &lt;- ggplot(df_long, aes(x = Date, y = Value, group = PartnerGroup)) +\n    geom_line(aes(color = PartnerGroup), size = 1) +\n    geom_point(aes(color = PartnerGroup), size = 2)\n  \n  if (show_left_labels) {\n    p &lt;- p +\n      geom_text(\n        data = filter(df_long, Date == !!col_left),\n        aes(label = PartnerGroup),\n        hjust = 1.05, size = 3\n      )\n  }\n  \n  p &lt;- p +\n    geom_text(\n      data = filter(df_long, Date == !!col_right),\n      aes(label = round(Value,1)),\n      hjust = -0.05, size = 3, color = \"black\"\n    ) +\n    labs(\n      title = title_label,\n      x = NULL,\n      # Show y-axis only if show_y_axis=TRUE:\n      y = if (show_y_axis) paste0(plot_type, \" S$ (Millions)\") else NULL\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\",\n      plot.title = element_text(size = 11, face = \"bold\")\n    ) +\n    scale_x_discrete(expand = expansion(mult = c(0.2, 0.2)))\n  \n  if (!is.null(y_limits)) {\n    p &lt;- p + scale_y_continuous(limits = y_limits, expand = c(0,0))\n  }\n  \n  if (!show_y_axis) {\n    p &lt;- p + theme(\n      axis.title.y = element_blank(),\n      axis.text.y  = element_blank(),\n      axis.ticks.y = element_blank()\n    )\n  }\n  \n  return(p)\n}\n\nall_values_imports &lt;- imports_top10 %&gt;%\n  select(any_of(jan_cols)) %&gt;%\n  as.matrix() %&gt;%\n  as.numeric() %&gt;%\n  na.omit()\n\ny_min_imp &lt;- min(all_values_imports)\ny_max_imp &lt;- max(all_values_imports)\ny_limits_imp &lt;- c(y_min_imp, y_max_imp)\n\np1 &lt;- create_slopegraph(\n  data = imports_top10, \n  col_left  = \"2020 Jan\", \n  col_right = \"2021 Jan\",\n  title_label   = \"Jan 2020 → Jan 2021\",\n  plot_type     = \"Imports\",    # &lt;--- new\n  y_limits      = y_limits_imp,\n  show_y_axis   = TRUE,\n  show_left_labels = TRUE\n)\np2 &lt;- create_slopegraph(\n  imports_top10, \"2021 Jan\", \"2022 Jan\",\n  title_label   = \"Jan 2021 → Jan 2022\",\n  plot_type     = \"Imports\",\n  y_limits      = y_limits_imp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\np3 &lt;- create_slopegraph(\n  imports_top10, \"2022 Jan\", \"2023 Jan\",\n  title_label   = \"Jan 2022 → Jan 2023\",\n  plot_type     = \"Imports\",\n  y_limits      = y_limits_imp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\np4 &lt;- create_slopegraph(\n  imports_top10, \"2023 Jan\", \"2024 Jan\",\n  title_label   = \"Jan 2023 → Jan 2024\",\n  plot_type     = \"Imports\",\n  y_limits      = y_limits_imp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\np5 &lt;- create_slopegraph(\n  imports_top10, \"2024 Jan\", \"2025 Jan\",\n  title_label   = \"Jan 2024 → Jan 2025\",\n  plot_type     = \"Imports\",\n  y_limits      = y_limits_imp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\n\nimports_slope_combined &lt;- p1 | p2 | p3 | p4 | p5\n\n\nall_values_exports &lt;- exports_top10 %&gt;%\n  select(any_of(jan_cols)) %&gt;%\n  as.matrix() %&gt;%\n  as.numeric() %&gt;%\n  na.omit()\n\ny_min_exp &lt;- min(all_values_exports)\ny_max_exp &lt;- max(all_values_exports)\ny_limits_exp &lt;- c(y_min_exp, y_max_exp)\n\np6 &lt;- create_slopegraph(\n  exports_top10, \"2020 Jan\", \"2021 Jan\",\n  title_label   = \"Jan 2020 → Jan 2021\",\n  plot_type     = \"Exports\",\n  y_limits      = y_limits_exp,\n  show_y_axis   = TRUE,\n  show_left_labels = TRUE\n)\np7 &lt;- create_slopegraph(\n  exports_top10, \"2021 Jan\", \"2022 Jan\",\n  title_label   = \"Jan 2021 → Jan 2022\",\n  plot_type     = \"Exports\",\n  y_limits      = y_limits_exp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\np8 &lt;- create_slopegraph(\n  exports_top10, \"2022 Jan\", \"2023 Jan\",\n  title_label   = \"Jan 2022 → Jan 2023\",\n  plot_type     = \"Exports\",\n  y_limits      = y_limits_exp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\np9 &lt;- create_slopegraph(\n  exports_top10, \"2023 Jan\", \"2024 Jan\",\n  title_label   = \"Jan 2023 → Jan 2024\",\n  plot_type     = \"Exports\",\n  y_limits      = y_limits_exp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\np10 &lt;- create_slopegraph(\n  exports_top10, \"2024 Jan\", \"2025 Jan\",\n  title_label   = \"Jan 2024 → Jan 2025\",\n  plot_type     = \"Exports\",\n  y_limits      = y_limits_exp,\n  show_y_axis   = FALSE,\n  show_left_labels = FALSE\n)\n\nexports_slope_combined &lt;- p6 | p7 | p8 | p9 | p10\n\n\nfinal_figure &lt;- imports_slope_combined /\n                exports_slope_combined\n\n\nfinal_figure\n\n\n\n\n\n\n\n\nThe slope chart format is particularly effective for this data as it:\n\nHighlights rate of change through the slope angle\nMakes ranking changes obvious when lines cross\nShows acceleration or deceleration in trade through changing slope angles\nEnables easy identification of outliers or unusual patterns\nProvides a cleaner visualization for multiple entities compared to multiple overlapping line charts\n\nThis enhanced visualization allows analysts to identify not just which partners Singapore trades with most, but how these relationships have evolved year by year, revealing patterns that might indicate changing economic priorities, effects of global events, or emerging trade opportunities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#time-series-forecasting",
    "href": "Take-home_Ex/Take-home_Ex2.html#time-series-forecasting",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Time-series forecasting",
    "text": "Time-series forecasting\nTime series forecasting is a critical component of trade analysis, allowing us to project future trade patterns based on historical data. By forecasting Singapore’s trade relationships, we can:\n\nIdentify emerging trade partners and markets with growth potential\nAnticipate possible shifts in trade balance with key partners\nProvide data-driven insights for strategic trade policy decisions\nQuantify the stability or volatility of specific trade relationships\n\nIn this section, we’ll apply advanced forecasting methodologies to Singapore’s merchandise trade data, focusing on predicting future import volumes for Singapore’s top 5 trading partners.\n\nForecasting methodology\nOur forecasting approach compares multiple time series models to identify the most accurate predictors for each trading partner. We’ll implement a systematic workflow:\n\nData preparation and transformation for time series analysis\nModel training on historical data (90% of available data)\nModel validation on a test set (most recent 10% of data)\nModel comparison and selection based on accuracy metrics\nForecasting future trade volumes for the next six months\n\n\npacman::p_load(tidyverse, lubridate, tidymodels, timetk, modeltime, purrr)\n\n\nData preparation for forecasting\nFirst, we identify Singapore’s top 5 trading partners by import volume in the latest available period. These partners will be the focus of our forecasting models.\n\nlatest_col &lt;- tail(names(imports), 1)\n\ntop5_countries &lt;- imports %&gt;%\n  arrange(desc(.data[[latest_col]])) %&gt;%\n  slice_head(n = 5) %&gt;%\n  pull(Country)\n\ntop5_countries\n\n[1] \"Malaysia\"      \"United States\" \"Japan\"         \"China\"        \n[5] \"Indonesia\"    \n\n\nThe top 5 trading partners for Singapore’s imports are Malaysia, United States, Japan, China (Mainland), and Indonesia. These five countries represent a significant portion of Singapore’s total import volume and provide diverse geographic and economic profiles for our forecasting analysis.\n\n\n\nForecasting workflow implementation\nWe’ll now implement a comprehensive forecasting workflow function that handles all steps of the modeling process for a specified country:\n\nforecast_one_country &lt;- function(country_name, df_wide, forecast_h = \"6 months\") {\n  \n  # 1) Filter to the specified country\n  df_filtered &lt;- df_wide %&gt;%\n    filter(Country == country_name)\n  \n  # 2) Reshape from wide =&gt; long: [date, value]\n  #    Summation is not needed since we want *this* country's data specifically.\n  df_long &lt;- df_filtered %&gt;%\n    select(where(is.numeric)) %&gt;% \n    pivot_longer(\n      cols      = everything(), \n      names_to  = \"MonthYearCol\", \n      values_to = \"TradeValue\"\n    ) %&gt;%\n    mutate(\n      date = lubridate::parse_date_time(MonthYearCol, orders = c(\"Y b\", \"b Y\"))\n    ) %&gt;%\n    filter(!is.na(date)) %&gt;%\n    arrange(date)\n  \n  # Safety check\n  if (nrow(df_long) &lt; 2) {\n    stop(\"Not enough data points for country: \", country_name)\n  }\n  \n  # 3) Split\n  splits &lt;- initial_time_split(df_long, prop = 0.9)\n  \n  # 4) Create & Fit Models (4 from Chap 20)\n\n  ## (a) ETS\n  model_fit_ets &lt;- exp_smoothing() %&gt;%\n    set_engine(\"ets\") %&gt;%\n    fit(TradeValue ~ date, data = training(splits))\n  \n  ## (b) Auto ARIMA\n  model_fit_arima &lt;- arima_reg() %&gt;%\n    set_engine(\"auto_arima\") %&gt;%\n    fit(TradeValue ~ date, data = training(splits))\n  \n  ## (c) Boosted ARIMA\n  model_fit_arima_boost &lt;- arima_boost(min_n = 2, learn_rate = 0.015) %&gt;%\n    set_engine(\"auto_arima_xgboost\") %&gt;%\n    fit(TradeValue ~ date, data = training(splits))\n  \n  ## (d) Prophet\n  model_fit_prophet &lt;- prophet_reg() %&gt;%\n    set_engine(\"prophet\") %&gt;%\n    fit(TradeValue ~ date, data = training(splits))\n  \n  # 5) modeltime_table\n  models_tbl &lt;- modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boost,\n    model_fit_prophet\n  )\n  \n  # 6) Calibrate on Test\n  calibration_tbl &lt;- models_tbl %&gt;%\n    modeltime_calibrate(testing(splits))\n  \n  # 7) Accuracy\n  accuracy_tbl &lt;- calibration_tbl %&gt;%\n    modeltime_accuracy() %&gt;%\n    arrange(rmse)  # sort by RMSE ascending\n  \n  # 8) Refit on full data & Forecast h months ahead\n  refit_tbl &lt;- calibration_tbl %&gt;%\n    modeltime_refit(data = df_long)\n  \n  forecast_tbl &lt;- refit_tbl %&gt;%\n    modeltime_forecast(\n      h           = forecast_h,\n      actual_data = df_long\n    ) %&gt;%\n    mutate(Country = country_name)\n  \n  # Return a list with relevant outputs\n  list(\n    country       = country_name,\n    data_long     = df_long,\n    splits        = splits,\n    models_tbl    = models_tbl,\n    calibration   = calibration_tbl,\n    accuracy      = accuracy_tbl,\n    refit_tbl     = refit_tbl,\n    forecast_data = forecast_tbl\n  )\n}\n\nThis function implements a complete forecasting workflow for a single country, including:\n\nData filtering and transformation to time series format\nTime-based train/test splitting\nTraining of four different forecasting models:\n\nETS (Exponential Smoothing): Effective for data with clear trend and seasonality patterns\nAuto ARIMA: Handles complex time series with autocorrelation structures\nBoosted ARIMA: Combines statistical time series with machine learning for enhanced accuracy\nProphet: Facebook’s decomposition-based forecasting model for robust trend analysis\n\nModel calibration and accuracy evaluation\nFuture forecasting based on the complete historical data\n\n\n\nApplying the forecasting workflow to top trading partners\nNow we apply our forecasting workflow to each of Singapore’s top 5 trading partners:\n\nresults_list &lt;- map(top5_countries, ~ forecast_one_country(\n  country_name = .x, \n  df_wide      = imports, \n  forecast_h   = \"6 months\"\n))\n\n# Name each list element after the country\nnames(results_list) &lt;- top5_countries\n\nThis creates a structured list containing the complete forecasting workflow results for each country. The list organization allows us to easily access specific components like accuracy metrics or forecast data for any country."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#model-performance-evaluation",
    "href": "Take-home_Ex/Take-home_Ex2.html#model-performance-evaluation",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Model performance evaluation",
    "text": "Model performance evaluation\n\nAccuracy metrics comparison\nFirst, we compare the forecasting accuracy across all models and countries:\n\nall_accuracy &lt;- map_dfr(\n  .x = results_list,\n  .f = ~ .x$accuracy %&gt;%\n    mutate(Country = .x$country),\n  .id = \"list_element\"\n)\n\nall_accuracy\n\n# A tibble: 20 × 11\n   list_element  .model_id .model_desc       .type   mae  mape  mase smape  rmse\n   &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;             &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Malaysia              4 PROPHET           Test  1377. 25.3  2.48  22.1  1476.\n 2 Malaysia              2 ARIMA(0,1,1)(0,0… Test  1547. 28.5  2.78  24.5  1636.\n 3 Malaysia              3 ARIMA(0,1,1)(0,0… Test  1547. 28.5  2.78  24.5  1636.\n 4 Malaysia              1 ETS(M,A,A)        Test  1552. 28.6  2.79  24.5  1646.\n 5 United States         1 ETS(M,N,A)        Test   559.  9.88 0.784  9.42  648.\n 6 United States         4 PROPHET           Test   632.  9.91 0.886 10.7   797.\n 7 United States         3 ARIMA(0,1,2)(1,0… Test   702. 12.8  0.984 11.7   858.\n 8 United States         2 ARIMA(0,1,2)(2,0… Test   758. 13.8  1.06  12.5   906.\n 9 Japan                 4 PROPHET           Test   241. 10.2  0.984  9.65  285.\n10 Japan                 1 ETS(M,AD,M)       Test   528. 22.7  2.16  19.8   592.\n11 Japan                 3 ARIMA(3,1,0)(2,0… Test   945. 39.9  3.85  32.6   993.\n12 Japan                 2 ARIMA(2,1,0)(2,0… Test   983. 41.5  4.01  33.7  1029.\n13 China                 4 PROPHET           Test   705. 11.1  1.02  10.5   821.\n14 China                 1 ETS(M,A,M)        Test  1550. 24.6  2.25  21.4  1696.\n15 China                 3 ARIMA(1,1,1)(2,0… Test  2089. 33.3  3.03  27.7  2279.\n16 China                 2 ARIMA(0,1,2)(2,0… Test  2142. 34.1  3.11  28.3  2333.\n17 Indonesia             4 PROPHET           Test   175.  9.87 1.13   9.96  200.\n18 Indonesia             1 ETS(M,N,N)        Test   266. 15.8  1.72  14.2   306.\n19 Indonesia             2 ARIMA(0,1,1)      Test   267. 15.9  1.73  14.3   307.\n20 Indonesia             3 ARIMA(0,1,1)(1,0… Test   301. 17.8  1.94  15.9   338.\n# ℹ 2 more variables: rsq &lt;dbl&gt;, Country &lt;chr&gt;\n\n\nThe accuracy table shows multiple performance metrics for each model and country combination:\n\nMAE (Mean Absolute Error): Average absolute prediction error\nMAPE (Mean Absolute Percentage Error): Percentage error, useful for comparing across countries\nRMSE (Root Mean Squared Error): Penalizes large errors more heavily, good for identifying models with occasional large misses\nMASE (Mean Absolute Scaled Error): Scale-independent measure comparing to a naive forecast\n\nLower values indicate better forecasting performance across all metrics.\n\n\nModel performance visualization\nLet’s examine how each model performs on the test data for each of our top trading partners:\n\nMalaysiaUSJapanChinaIndonesia\n\n\n\nresults_list[[1]]$calibration %&gt;%\n  modeltime_forecast(\n    new_data = testing(results_list[[1]]$splits),\n    actual_data = results_list[[1]]$data_long\n  ) %&gt;%\n  plot_modeltime_forecast(\n    .facet_ncol = 1,\n    .interactive = TRUE,\n    .title = paste(\"Test Set Performance for\", results_list[[1]]$country)\n  )\n\n\n\n\n\n\n\n\nresults_list[[2]]$calibration %&gt;%\n  modeltime_forecast(\n    new_data = testing(results_list[[2]]$splits),\n    actual_data = results_list[[2]]$data_long\n  ) %&gt;%\n  plot_modeltime_forecast(\n    .facet_ncol = 1,\n    .interactive = TRUE,\n    .title = paste(\"Test Set Performance for\", results_list[[2]]$country)\n  )\n\n\n\n\n\n\n\n\nresults_list[[3]]$calibration %&gt;%\n  modeltime_forecast(\n    new_data = testing(results_list[[3]]$splits),\n    actual_data = results_list[[3]]$data_long\n  ) %&gt;%\n  plot_modeltime_forecast(\n    .facet_ncol = 1,\n    .interactive = TRUE,\n    .title = paste(\"Test Set Performance for\", results_list[[3]]$country)\n  )\n\n\n\n\n\n\n\n\nresults_list[[4]]$calibration %&gt;%\n  modeltime_forecast(\n    new_data = testing(results_list[[4]]$splits),\n    actual_data = results_list[[4]]$data_long\n  ) %&gt;%\n  plot_modeltime_forecast(\n    .facet_ncol = 1,\n    .interactive = TRUE,\n    .title = paste(\"Test Set Performance for\", results_list[[4]]$country)\n  )\n\n\n\n\n\n\n\n\nresults_list[[5]]$calibration %&gt;%\n  modeltime_forecast(\n    new_data = testing(results_list[[5]]$splits),\n    actual_data = results_list[[5]]$data_long\n  ) %&gt;%\n  plot_modeltime_forecast(\n    .facet_ncol = 1,\n    .interactive = TRUE,\n    .title = paste(\"Test Set Performance for\", results_list[[5]]$country)\n  )\n\n\n\n\n\n\n\n\n\n\nResults analysis\nEach country exhibits distinct patterns in their trade relationship with Singapore, necessitating different forecasting approaches rather than a one-size-fits-all model.\n\nMalaysia\nAll forecasting models significantly overestimate the recent peak values, as the sudden drop in imports during the test period had no precedent in the training dataset.\n\n\nUnited States\nThe ETS model (red line) aligns most closely with actual values. It also features narrower confidence intervals compared to other models, indicating higher prediction certainty.\n\n\nJapan\nThe Auto ARIMA and boosted ARIMA models (yellow and green lines) capture the overall trend effectively but consistently overpredict. The Prophet model (blue line) provides estimates closest to actual values.\n\n\nChina\nDuring the test period, the Prophet model (blue line) most accurately tracks actual import values. All four models identify an upward trajectory in imports.\n\n\nIndonesia\nIndonesia demonstrates the most volatile trading pattern among the five countries, with substantial fluctuations throughout the entire period. The Prophet model (blue line) outperforms other forecasting approaches for this challenging dataset.\n\n\n\nFuture trade projections\nFinally, we generate and visualize six-month forecasts for all five trading partners:\n\nall_forecasts &lt;- map_dfr(results_list, ~ .x$forecast_data)\n\nall_forecasts %&gt;%\n  filter(.index &gt;= as.POSIXct(\"2023-01-01\")) %&gt;%\n  group_by(Country) %&gt;%\n  plot_modeltime_forecast(\n    .facet_ncol   = 1,\n    .interactive  = TRUE,\n    .legend_max_width = 25\n  )\n\n\n\n\n\nThe combined forecast plot reveals distinct patterns for each country from January 2023 to July 2025, with the last six months showing the future projections."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2.html#key-findings-from-forecasting-analysis",
    "href": "Take-home_Ex/Take-home_Ex2.html#key-findings-from-forecasting-analysis",
    "title": "Take-home Exercise 2: Be Tradewise or Otherwise",
    "section": "Key findings from forecasting analysis",
    "text": "Key findings from forecasting analysis\nBased on our time series forecasting and the actual visualizations, we can draw several insights about Singapore’s trade relationships:\n\nHistorical Growth Patterns: The long-term view reveals dramatically different growth trajectories - China shows the most substantial growth (7x increase since 2003), while Indonesia’s trade has remained relatively flat after initial growth in the 2000s.\nRegional vs. Global Partners: The forecasts suggest that while both regional partners (Malaysia, China) and global partners (United States) remain important, the strongest recent growth has been with the United States and China, indicating Singapore’s balanced approach to trade diversification.\nModel Performance Variation: Different forecasting models perform better for different countries - Prophet generally perform better overall, while ARIMA variants models capture the trends better.\n\nThese forecasting insights complement our earlier visualizations by adding a predictive dimension that extends into the future, helping to identify not just past patterns but also the most likely near-term directions for Singapore’s key trade relationships."
  }
]